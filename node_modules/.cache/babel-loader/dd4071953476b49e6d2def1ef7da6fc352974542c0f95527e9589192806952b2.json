{"ast":null,"code":"import _regeneratorRuntime from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/regeneratorRuntime.js\";\nimport _asyncToGenerator from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\nimport _classCallCheck from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";\nimport _createClass from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createClass.js\";\nimport _inherits from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/inherits.js\";\nimport _createSuper from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createSuper.js\";\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n// inspired by https://github.com/maxogden/filereader-stream\nimport { env, util } from '@tensorflow/tfjs-core';\nimport { ByteChunkIterator } from './byte_chunk_iterator';\n/**\n * Provide a stream of chunks from a File, Blob, or Uint8Array.\n * @param file The source File, Blob or Uint8Array.\n * @param options Optional settings controlling file reading.\n * @returns a lazy Iterator of Uint8Arrays containing sequential chunks of the\n *   input File, Blob or Uint8Array.\n */\nexport var FileChunkIterator = /*#__PURE__*/function (_ByteChunkIterator) {\n  _inherits(FileChunkIterator, _ByteChunkIterator);\n  var _super = _createSuper(FileChunkIterator);\n  function FileChunkIterator(file) {\n    var _this;\n    var options = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n    _classCallCheck(this, FileChunkIterator);\n    _this = _super.call(this);\n    _this.file = file;\n    _this.options = options;\n    util.assert(file instanceof Uint8Array || (env().get('IS_BROWSER') ? file instanceof File || file instanceof Blob : false), function () {\n      return 'FileChunkIterator only supports File, Blob and Uint8Array ' + 'right now.';\n    });\n    _this.offset = options.offset || 0;\n    // default 1MB chunk has tolerable perf on large files\n    _this.chunkSize = options.chunkSize || 1024 * 1024;\n    return _this;\n  }\n  _createClass(FileChunkIterator, [{\n    key: \"summary\",\n    value: function summary() {\n      return \"FileChunks \".concat(this.file);\n    }\n  }, {\n    key: \"next\",\n    value: function () {\n      var _next = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee() {\n        var _this2 = this;\n        var chunk;\n        return _regeneratorRuntime().wrap(function _callee$(_context) {\n          while (1) switch (_context.prev = _context.next) {\n            case 0:\n              if (!(this.offset >= (this.file instanceof Uint8Array ? this.file.byteLength : this.file.size))) {\n                _context.next = 2;\n                break;\n              }\n              return _context.abrupt(\"return\", {\n                value: null,\n                done: true\n              });\n            case 2:\n              chunk = new Promise(function (resolve, reject) {\n                var end = _this2.offset + _this2.chunkSize;\n                if (_this2.file instanceof Uint8Array) {\n                  // Note if end > this.uint8Array.byteLength, we just get a small last\n                  // chunk.\n                  resolve(new Uint8Array(_this2.file.slice(_this2.offset, end)));\n                } else {\n                  // This branch assumes that this.file type is File or Blob, which\n                  // means it is in the browser environment.\n                  // TODO(soergel): is this a performance issue?\n                  var fileReader = new FileReader();\n                  fileReader.onload = function (event) {\n                    var data = fileReader.result;\n                    // Not sure we can trust the return type of\n                    // FileReader.readAsArrayBuffer See e.g.\n                    // https://github.com/node-file-api/FileReader/issues/2\n                    if (data instanceof ArrayBuffer) {\n                      data = new Uint8Array(data);\n                    }\n                    if (!(data instanceof Uint8Array)) {\n                      return reject(new TypeError('FileReader returned unknown type.'));\n                    }\n                    resolve(data);\n                  };\n                  fileReader.onabort = function (event) {\n                    return reject(new Error('Aborted'));\n                  };\n                  fileReader.onerror = function (event) {\n                    return reject(new Error(event.type));\n                  };\n                  // TODO(soergel): better handle onabort, onerror\n                  // Note if end > this.file.size, we just get a small last chunk.\n                  var slice = _this2.file.slice(_this2.offset, end);\n                  // We can't use readAsText here (even if we know the file is text)\n                  // because the slice boundary may fall within a multi-byte character.\n                  fileReader.readAsArrayBuffer(slice);\n                }\n                _this2.offset = end;\n              });\n              _context.next = 5;\n              return chunk;\n            case 5:\n              _context.t0 = _context.sent;\n              return _context.abrupt(\"return\", {\n                value: _context.t0,\n                done: false\n              });\n            case 7:\n            case \"end\":\n              return _context.stop();\n          }\n        }, _callee, this);\n      }));\n      function next() {\n        return _next.apply(this, arguments);\n      }\n      return next;\n    }()\n  }]);\n  return FileChunkIterator;\n}(ByteChunkIterator);","map":{"version":3,"mappings":";;;;;;AAAA;;;;;;;;;;;;;;;;;AAkBA;AACA,SAAQA,GAAG,EAAEC,IAAI,QAAO,uBAAuB;AAE/C,SAAQC,iBAAiB,QAAO,uBAAuB;AASvD;;;;;;;AAOA,WAAaC,iBAAkB;EAAA;EAAA;EAI7B,2BACcC,IAAiB,EACqB;IAAA;IAAA,IAAtCC,8EAAoC,EAAE;IAAA;IAClD;IAFY,UAAI,GAAJD,IAAI;IACJ,aAAO,GAAPC,OAAO;IAEnBJ,IAAI,CAACK,MAAM,CACNF,IAAI,YAAYG,UAAU,KACtBP,GAAG,EAAE,CAACQ,GAAG,CAAC,YAAY,CAAC,GAClBJ,IAAI,YAAYK,IAAI,IAAIL,IAAI,YAAYM,IAAI,GAC7C,KAAK,CAAC,EACf;MAAA,OAAM,4DAA4D,GAC9D,YAAY;IAAA,EAAC;IACrB,MAAKC,MAAM,GAAGN,OAAO,CAACM,MAAM,IAAI,CAAC;IACjC;IACA,MAAKC,SAAS,GAAGP,OAAO,CAACO,SAAS,IAAI,IAAI,GAAG,IAAI;IAAC;EACpD;EAAC;IAAA;IAAA,OAED,mBAAO;MACL,4BAAqB,IAAI,CAACR,IAAI;IAChC;EAAC;IAAA;IAAA;MAAA,uEAED;QAAA;QAAA;QAAA;UAAA;YAAA;cAAA,MACM,IAAI,CAACO,MAAM,KAAM,IAAI,CAACP,IAAI,YAAYG,UAAU,GAC5B,IAAI,CAACH,IAAI,CAACS,UAAU,GACpB,IAAI,CAACT,IAAI,CAACU,IAAI,CAAC;gBAAA;gBAAA;cAAA;cAAA,iCAC9B;gBAACC,KAAK,EAAE,IAAI;gBAAEC,IAAI,EAAE;cAAI,CAAC;YAAA;cAE5BC,KAAK,GAAG,IAAIC,OAAO,CAAa,UAACC,OAAO,EAAEC,MAAM,EAAI;gBACxD,IAAMC,GAAG,GAAG,MAAI,CAACV,MAAM,GAAG,MAAI,CAACC,SAAS;gBACxC,IAAI,MAAI,CAACR,IAAI,YAAYG,UAAU,EAAE;kBACnC;kBACA;kBACAY,OAAO,CAAC,IAAIZ,UAAU,CAAC,MAAI,CAACH,IAAI,CAACkB,KAAK,CAAC,MAAI,CAACX,MAAM,EAAEU,GAAG,CAAC,CAAC,CAAC;iBAC3D,MAAM;kBACL;kBACA;kBAEA;kBACA,IAAME,UAAU,GAAG,IAAIC,UAAU,EAAE;kBACnCD,UAAU,CAACE,MAAM,GAAG,UAACC,KAAK,EAAI;oBAC5B,IAAIC,IAAI,GAAkCJ,UAAU,CAACK,MAAM;oBAC3D;oBACA;oBACA;oBACA,IAAID,IAAI,YAAYE,WAAW,EAAE;sBAC/BF,IAAI,GAAG,IAAIpB,UAAU,CAACoB,IAAI,CAAC;;oBAE7B,IAAI,EAAEA,IAAI,YAAYpB,UAAU,CAAC,EAAE;sBACjC,OAAOa,MAAM,CAAC,IAAIU,SAAS,CAAC,mCAAmC,CAAC,CAAC;;oBAEnEX,OAAO,CAACQ,IAAI,CAAC;kBACf,CAAC;kBACDJ,UAAU,CAACQ,OAAO,GAAG,UAACL,KAAK,EAAI;oBAC7B,OAAON,MAAM,CAAC,IAAIY,KAAK,CAAC,SAAS,CAAC,CAAC;kBACrC,CAAC;kBACDT,UAAU,CAACU,OAAO,GAAG,UAACP,KAAK,EAAI;oBAC7B,OAAON,MAAM,CAAC,IAAIY,KAAK,CAACN,KAAK,CAACQ,IAAI,CAAC,CAAC;kBACtC,CAAC;kBACD;kBACA;kBACA,IAAMZ,KAAK,GAAG,MAAI,CAAClB,IAAI,CAACkB,KAAK,CAAC,MAAI,CAACX,MAAM,EAAEU,GAAG,CAAC;kBAC/C;kBACA;kBACAE,UAAU,CAACY,iBAAiB,CAACb,KAAK,CAAC;;gBAErC,MAAI,CAACX,MAAM,GAAGU,GAAG;cACnB,CAAC,CAAC;cAAA;cAAA,OACoBJ,KAAK;YAAA;cAAA;cAAA;gBAAnBF,KAAK;gBAAiBC,IAAI,EAAE;cAAK;YAAA;YAAA;cAAA;UAAA;QAAA;MAAA,CAC1C;MAAA;QAAA;MAAA;MAAA;IAAA;EAAA;EAAA;AAAA,EAvEoCd,iBAAiB","names":["env","util","ByteChunkIterator","FileChunkIterator","file","options","assert","Uint8Array","get","File","Blob","offset","chunkSize","byteLength","size","value","done","chunk","Promise","resolve","reject","end","slice","fileReader","FileReader","onload","event","data","result","ArrayBuffer","TypeError","onabort","Error","onerror","type","readAsArrayBuffer"],"sources":["E:\\react-detect-toxicity-in-a-chat-app-youtube-2\\Toxic-Word-Checker\\node_modules\\@tensorflow\\tfjs-data\\src\\iterators\\file_chunk_iterator.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\n\n// inspired by https://github.com/maxogden/filereader-stream\nimport {env, util} from '@tensorflow/tfjs-core';\nimport {FileElement} from '../types';\nimport {ByteChunkIterator} from './byte_chunk_iterator';\n\nexport interface FileChunkIteratorOptions {\n  /** The byte offset at which to begin reading the File or Blob. Default 0. */\n  offset?: number;\n  /** The number of bytes to read at a time. Default 1MB. */\n  chunkSize?: number;\n}\n\n/**\n * Provide a stream of chunks from a File, Blob, or Uint8Array.\n * @param file The source File, Blob or Uint8Array.\n * @param options Optional settings controlling file reading.\n * @returns a lazy Iterator of Uint8Arrays containing sequential chunks of the\n *   input File, Blob or Uint8Array.\n */\nexport class FileChunkIterator extends ByteChunkIterator {\n  offset: number;\n  chunkSize: number;\n\n  constructor(\n      protected file: FileElement,\n      protected options: FileChunkIteratorOptions = {}) {\n    super();\n    util.assert(\n        (file instanceof Uint8Array) ||\n            (env().get('IS_BROWSER') ?\n                 (file instanceof File || file instanceof Blob) :\n                 false),\n        () => 'FileChunkIterator only supports File, Blob and Uint8Array ' +\n            'right now.');\n    this.offset = options.offset || 0;\n    // default 1MB chunk has tolerable perf on large files\n    this.chunkSize = options.chunkSize || 1024 * 1024;\n  }\n\n  summary() {\n    return `FileChunks ${this.file}`;\n  }\n\n  async next(): Promise<IteratorResult<Uint8Array>> {\n    if (this.offset >= ((this.file instanceof Uint8Array) ?\n                            this.file.byteLength :\n                            this.file.size)) {\n      return {value: null, done: true};\n    }\n    const chunk = new Promise<Uint8Array>((resolve, reject) => {\n      const end = this.offset + this.chunkSize;\n      if (this.file instanceof Uint8Array) {\n        // Note if end > this.uint8Array.byteLength, we just get a small last\n        // chunk.\n        resolve(new Uint8Array(this.file.slice(this.offset, end)));\n      } else {\n        // This branch assumes that this.file type is File or Blob, which\n        // means it is in the browser environment.\n\n        // TODO(soergel): is this a performance issue?\n        const fileReader = new FileReader();\n        fileReader.onload = (event) => {\n          let data: string|ArrayBuffer|Uint8Array = fileReader.result;\n          // Not sure we can trust the return type of\n          // FileReader.readAsArrayBuffer See e.g.\n          // https://github.com/node-file-api/FileReader/issues/2\n          if (data instanceof ArrayBuffer) {\n            data = new Uint8Array(data);\n          }\n          if (!(data instanceof Uint8Array)) {\n            return reject(new TypeError('FileReader returned unknown type.'));\n          }\n          resolve(data);\n        };\n        fileReader.onabort = (event) => {\n          return reject(new Error('Aborted'));\n        };\n        fileReader.onerror = (event) => {\n          return reject(new Error(event.type));\n        };\n        // TODO(soergel): better handle onabort, onerror\n        // Note if end > this.file.size, we just get a small last chunk.\n        const slice = this.file.slice(this.offset, end);\n        // We can't use readAsText here (even if we know the file is text)\n        // because the slice boundary may fall within a multi-byte character.\n        fileReader.readAsArrayBuffer(slice);\n      }\n      this.offset = end;\n    });\n    return {value: (await chunk), done: false};\n  }\n}\n"]},"metadata":{},"sourceType":"module","externalDependencies":[]}