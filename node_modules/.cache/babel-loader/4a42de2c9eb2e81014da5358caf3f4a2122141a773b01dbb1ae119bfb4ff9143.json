{"ast":null,"code":"import _regeneratorRuntime from\"E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/regeneratorRuntime.js\";import _toConsumableArray from\"E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/toConsumableArray.js\";import _asyncToGenerator from\"E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";import _slicedToArray from\"E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/slicedToArray.js\";import\"./styles.css\";import{useEffect,useState,useRef}from'react';import*as tf from'@tensorflow/tfjs';import{load}from'@tensorflow-models/toxicity';// Set an id counter, so that we can automatically identify each message with a unique number:\nimport{jsx as _jsx}from\"react/jsx-runtime\";import{jsxs as _jsxs}from\"react/jsx-runtime\";import{Fragment as _Fragment}from\"react/jsx-runtime\";var id=3;// Initialize our messages list with some sample messages:\nvar initialMessages=[{id:1,msg:\"Hello!\"},{id:2,msg:\"What's up?\"},{id:3,msg:\"Hello!\"}];export default function App(){var _useState=useState(initialMessages),_useState2=_slicedToArray(_useState,2),messages=_useState2[0],setMessages=_useState2[1];// The list of messages\nvar _useState3=useState({isToxic:false,labels:[]}),_useState4=_slicedToArray(_useState3,2),toxicity=_useState4[0],setToxicity=_useState4[1];// The state of toxicity for the message we just typed \nvar _useState5=useState(false),_useState6=_slicedToArray(_useState5,2),isClassifying=_useState6[0],setIsClassifying=_useState6[1];// A simple state variable to reflect the classifying process status\nvar _useState7=useState(false),_useState8=_slicedToArray(_useState7,2),hasLoaded=_useState8[0],setHasLoaded=_useState8[1];// Has the Toxicity model been loaded?\nvar hasMessages=messages.length>0;// Should we render the ul that will hold the messages if no messages are available?\nvar model=useRef(null);// Retain a value throughout the Component's render cycles WITHOUT triggering a render, as opposed to a useState variable\n// Handle form submission: check the toxicity of the message and update accordingly:\nvar sendMessage=/*#__PURE__*/function(){var _ref=_asyncToGenerator(/*#__PURE__*/_regeneratorRuntime().mark(function _callee(event){var form,msg,predictions,isToxic,labels;return _regeneratorRuntime().wrap(function _callee$(_context){while(1)switch(_context.prev=_context.next){case 0:// console.log(\"sendMessage()\");\nevent.preventDefault();// Prevent default HTML form behaviour that will trigger an HTTP request and a page reload\nform=event.target;msg=form.message.value;// Run the classifier on every message\nsetIsClassifying(true);_context.next=6;return model.current.classify([msg]);case 6:predictions=_context.sent;setIsClassifying(false);// Is the message toxic?\nisToxic=predictions[6].results[0].match;if(isToxic){labels=[];// Loop through the toxicity labels and create a list of them along with the corresponding percentagess (level of confidence):\npredictions.forEach(function(p){if(p.results[0].match){labels.push({label:p.label,prob:Math.round(p.results[0].probabilities[1]*100)+\"%\"});}});// console.log(labels);\nsetToxicity({isToxic:true,labels:labels});}else{setMessages([].concat(_toConsumableArray(messages),[{id:++id,msg:msg}]));setToxicity({isToxic:false,labels:[]});form.reset();}case 10:case\"end\":return _context.stop();}},_callee);}));return function sendMessage(_x){return _ref.apply(this,arguments);};}();// Load the model just one, when the component mounts (hence the empty dependency array [])\nuseEffect(function(){function loadModel(){return _loadModel.apply(this,arguments);}function _loadModel(){_loadModel=_asyncToGenerator(/*#__PURE__*/_regeneratorRuntime().mark(function _callee2(){var threshold;return _regeneratorRuntime().wrap(function _callee2$(_context2){while(1)switch(_context2.prev=_context2.next){case 0:threshold=0.9;// Set a state that indicates the model is being loaded...\n_context2.next=3;return load(threshold);case 3:model.current=_context2.sent;setHasLoaded(true);// Set the state to false to let the user know that they can check the text\nconsole.log(\"Model loaded\");case 6:case\"end\":return _context2.stop();}},_callee2);}));return _loadModel.apply(this,arguments);}loadModel();},[]);return/*#__PURE__*/_jsxs(_Fragment,{children:[/*#__PURE__*/_jsx(\"h1\",{children:\"TensorFlow.JS + Toxicity Classifier\"}),hasMessages&&/*#__PURE__*/_jsx(\"ul\",{children:messages.map(function(message){return/*#__PURE__*/_jsx(\"li\",{children:message.msg},message.id);})}),hasLoaded&&/*#__PURE__*/_jsxs(\"form\",{onSubmit:sendMessage,children:[/*#__PURE__*/_jsx(\"input\",{type:\"text\",name:\"message\",placeholder:\"Enter message\"}),/*#__PURE__*/_jsx(\"button\",{children:isClassifying?\" Ô∏è‚ÄçÔ∏è‚Äçüïµ \":\"Send\"}),/*#__PURE__*/_jsx(\"span\",{children:toxicity.isToxic&&\" ü§ê\"}),toxicity.labels.map(function(l){return\" \".concat(l.label,\" \").concat(l.prob);})]})]});}","map":{"version":3,"names":["useEffect","useState","useRef","tf","load","id","initialMessages","msg","App","messages","setMessages","isToxic","labels","toxicity","setToxicity","isClassifying","setIsClassifying","hasLoaded","setHasLoaded","hasMessages","length","model","sendMessage","event","preventDefault","form","target","message","value","current","classify","predictions","results","match","forEach","p","push","label","prob","Math","round","probabilities","reset","loadModel","threshold","console","log","map","l"],"sources":["E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/src/App.jsx"],"sourcesContent":["import \"./styles.css\";\nimport { useEffect, useState, useRef } from 'react';\nimport * as tf from '@tensorflow/tfjs';\nimport { load } from '@tensorflow-models/toxicity';\n\n// Set an id counter, so that we can automatically identify each message with a unique number:\nlet id = 3;\n// Initialize our messages list with some sample messages:\nconst initialMessages = [\n  { id: 1, msg: \"Hello!\" },\n  { id: 2, msg: \"What's up?\"},\n  { id: 3, msg: \"Hello!\"}\n]\n\nexport default function App() {\n  const [ messages, setMessages ] = useState(initialMessages); // The list of messages\n  const [ toxicity, setToxicity ] = useState({ isToxic: false, labels: [] }); // The state of toxicity for the message we just typed \n  const [ isClassifying, setIsClassifying ] = useState(false); // A simple state variable to reflect the classifying process status\n  const [ hasLoaded, setHasLoaded ] = useState(false); // Has the Toxicity model been loaded?\n  const hasMessages = messages.length > 0;  // Should we render the ul that will hold the messages if no messages are available?\n  const model = useRef(null); // Retain a value throughout the Component's render cycles WITHOUT triggering a render, as opposed to a useState variable\n\n  // Handle form submission: check the toxicity of the message and update accordingly:\n  const sendMessage = async event =>{\n    // console.log(\"sendMessage()\");\n    event.preventDefault(); // Prevent default HTML form behaviour that will trigger an HTTP request and a page reload\n    const form = event.target;\n    const msg = form.message.value;\n\n    // Run the classifier on every message\n    setIsClassifying(true);\n    const predictions = await model.current.classify([msg]);\n    setIsClassifying(false);\n\n    // Is the message toxic?\n    const isToxic = predictions[6].results[0].match;\n\n    if ( isToxic ){\n      const labels = [];\n      // Loop through the toxicity labels and create a list of them along with the corresponding percentagess (level of confidence):\n      predictions.forEach( p =>{\n        if ( p.results[0].match ){\n          labels.push({ \n            label: p.label, \n            prob: Math.round(p.results[0].probabilities[1] * 100) + \"%\" });\n        }\n      })\n      // console.log(labels);\n      setToxicity({ isToxic: true, labels });\n    } else {\n      setMessages([...messages, { id: ++id, msg: msg }]);\n      setToxicity({ isToxic: false, labels: [] });\n      form.reset();\n    }\n  }\n  // Load the model just one, when the component mounts (hence the empty dependency array [])\n  useEffect(()=>{\n\n    async function loadModel(){\n      const threshold = 0.9;\n      // Set a state that indicates the model is being loaded...\n      model.current = await load(threshold);\n      setHasLoaded(true);\n      // Set the state to false to let the user know that they can check the text\n      console.log(\"Model loaded\");\n    } \n    loadModel();\n\n\n  },[]);\n\n  return (\n    <>\n      <h1>TensorFlow.JS + Toxicity Classifier</h1>\n      { hasMessages && \n        <ul>\n          { messages.map( message =>{\n            return <li key={message.id}>{ message.msg }</li>\n          })}\n        </ul>\n      }{\n        hasLoaded && (\n          <form onSubmit={sendMessage}>\n            <input type=\"text\" name=\"message\" placeholder=\"Enter message\" />\n            <button>{ isClassifying ? \" Ô∏è‚ÄçÔ∏è‚Äçüïµ \" : \"Send\" }</button>\n            <span>{ toxicity.isToxic && \" ü§ê\" }</span>\n            { toxicity.labels.map( l => ` ${l.label} ${l.prob}` )}\n          </form>\n        )\n      }\n    </>\n  );\n}\n"],"mappings":"4wBAAA,MAAO,cAAc,CACrB,OAASA,SAAS,CAAEC,QAAQ,CAAEC,MAAM,KAAQ,OAAO,CACnD,MAAO,GAAKC,GAAE,KAAM,kBAAkB,CACtC,OAASC,IAAI,KAAQ,6BAA6B,CAElD;AAAA,6IACA,GAAIC,GAAE,CAAG,CAAC,CACV;AACA,GAAMC,gBAAe,CAAG,CACtB,CAAED,EAAE,CAAE,CAAC,CAAEE,GAAG,CAAE,QAAS,CAAC,CACxB,CAAEF,EAAE,CAAE,CAAC,CAAEE,GAAG,CAAE,YAAY,CAAC,CAC3B,CAAEF,EAAE,CAAE,CAAC,CAAEE,GAAG,CAAE,QAAQ,CAAC,CACxB,CAED,cAAe,SAASC,IAAG,EAAG,CAC5B,cAAkCP,QAAQ,CAACK,eAAe,CAAC,wCAAnDG,QAAQ,eAAEC,WAAW,eAAgC;AAC7D,eAAkCT,QAAQ,CAAC,CAAEU,OAAO,CAAE,KAAK,CAAEC,MAAM,CAAE,EAAG,CAAC,CAAC,yCAAlEC,QAAQ,eAAEC,WAAW,eAA+C;AAC5E,eAA4Cb,QAAQ,CAAC,KAAK,CAAC,yCAAnDc,aAAa,eAAEC,gBAAgB,eAAsB;AAC7D,eAAoCf,QAAQ,CAAC,KAAK,CAAC,yCAA3CgB,SAAS,eAAEC,YAAY,eAAsB;AACrD,GAAMC,YAAW,CAAGV,QAAQ,CAACW,MAAM,CAAG,CAAC,CAAG;AAC1C,GAAMC,MAAK,CAAGnB,MAAM,CAAC,IAAI,CAAC,CAAE;AAE5B;AACA,GAAMoB,YAAW,4FAAG,iBAAMC,KAAK,2JAC7B;AACAA,KAAK,CAACC,cAAc,EAAE,CAAE;AAClBC,IAAI,CAAGF,KAAK,CAACG,MAAM,CACnBnB,GAAG,CAAGkB,IAAI,CAACE,OAAO,CAACC,KAAK,CAE9B;AACAZ,gBAAgB,CAAC,IAAI,CAAC,CAAC,sBACGK,MAAK,CAACQ,OAAO,CAACC,QAAQ,CAAC,CAACvB,GAAG,CAAC,CAAC,QAAjDwB,WAAW,eACjBf,gBAAgB,CAAC,KAAK,CAAC,CAEvB;AACML,OAAO,CAAGoB,WAAW,CAAC,CAAC,CAAC,CAACC,OAAO,CAAC,CAAC,CAAC,CAACC,KAAK,CAE/C,GAAKtB,OAAO,CAAE,CACNC,MAAM,CAAG,EAAE,CACjB;AACAmB,WAAW,CAACG,OAAO,CAAE,SAAAC,CAAC,CAAG,CACvB,GAAKA,CAAC,CAACH,OAAO,CAAC,CAAC,CAAC,CAACC,KAAK,CAAE,CACvBrB,MAAM,CAACwB,IAAI,CAAC,CACVC,KAAK,CAAEF,CAAC,CAACE,KAAK,CACdC,IAAI,CAAEC,IAAI,CAACC,KAAK,CAACL,CAAC,CAACH,OAAO,CAAC,CAAC,CAAC,CAACS,aAAa,CAAC,CAAC,CAAC,CAAG,GAAG,CAAC,CAAG,GAAI,CAAC,CAAC,CAClE,CACF,CAAC,CAAC,CACF;AACA3B,WAAW,CAAC,CAAEH,OAAO,CAAE,IAAI,CAAEC,MAAM,CAANA,MAAO,CAAC,CAAC,CACxC,CAAC,IAAM,CACLF,WAAW,8BAAKD,QAAQ,GAAE,CAAEJ,EAAE,CAAE,EAAEA,EAAE,CAAEE,GAAG,CAAEA,GAAI,CAAC,GAAE,CAClDO,WAAW,CAAC,CAAEH,OAAO,CAAE,KAAK,CAAEC,MAAM,CAAE,EAAG,CAAC,CAAC,CAC3Ca,IAAI,CAACiB,KAAK,EAAE,CACd,CAAC,sDACF,kBA/BKpB,YAAW,4CA+BhB,CACD;AACAtB,SAAS,CAAC,UAAI,SAEG2C,UAAS,uIAAxB,uJACQC,SAAS,CAAG,GAAG,CACrB;AAAA,uBACsBxC,KAAI,CAACwC,SAAS,CAAC,QAArCvB,KAAK,CAACQ,OAAO,gBACbX,YAAY,CAAC,IAAI,CAAC,CAClB;AACA2B,OAAO,CAACC,GAAG,CAAC,cAAc,CAAC,CAAC,uDAC7B,4CACDH,SAAS,EAAE,CAGb,CAAC,CAAC,EAAE,CAAC,CAEL,mBACE,wCACE,oBAAI,qCAAmC,EAAK,CAC1CxB,WAAW,eACX,oBACIV,QAAQ,CAACsC,GAAG,CAAE,SAAApB,OAAO,CAAG,CACxB,mBAAO,oBAAuBA,OAAO,CAACpB,GAAG,EAAzBoB,OAAO,CAACtB,EAAE,CAAsB,CAClD,CAAC,CAAC,EACC,CAELY,SAAS,eACP,cAAM,QAAQ,CAAEK,WAAY,wBAC1B,cAAO,IAAI,CAAC,MAAM,CAAC,IAAI,CAAC,SAAS,CAAC,WAAW,CAAC,eAAe,EAAG,cAChE,wBAAUP,aAAa,CAAG,UAAU,CAAG,MAAM,EAAW,cACxD,sBAAQF,QAAQ,CAACF,OAAO,EAAI,KAAK,EAAS,CACxCE,QAAQ,CAACD,MAAM,CAACmC,GAAG,CAAE,SAAAC,CAAC,mBAAQA,CAAC,CAACX,KAAK,aAAIW,CAAC,CAACV,IAAI,GAAE,CAAE,GAExD,GAEF,CAEP"},"metadata":{},"sourceType":"module","externalDependencies":[]}