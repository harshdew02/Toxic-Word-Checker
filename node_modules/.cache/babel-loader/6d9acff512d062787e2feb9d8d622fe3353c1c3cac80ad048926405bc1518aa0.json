{"ast":null,"code":"import _toConsumableArray from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/toConsumableArray.js\";\nimport _classCallCheck from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";\nimport _createClass from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createClass.js\";\nimport _get from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/get.js\";\nimport _getPrototypeOf from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/getPrototypeOf.js\";\nimport _inherits from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/inherits.js\";\nimport _createSuper from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createSuper.js\";\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Layers that augment the functionality of a base layer.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { InputSpec, Layer, SymbolicTensor } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { VALID_BIDIRECTIONAL_MERGE_MODES } from '../keras_format/common';\nimport * as generic_utils from '../utils/generic_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nimport { rnn, standardizeArgs } from './recurrent';\nimport { deserialize } from './serialization';\n/**\n * Abstract wrapper base class.\n *\n * Wrappers take another layer and augment it in various ways.\n * Do not use this class as a layer, it is only an abstract base class.\n * Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n */\nexport var Wrapper = /*#__PURE__*/function (_Layer) {\n  _inherits(Wrapper, _Layer);\n  var _super = _createSuper(Wrapper);\n  function Wrapper(args) {\n    var _this;\n    _classCallCheck(this, Wrapper);\n    // Porting Note: In PyKeras, `self.layer` is set prior to the calling\n    //   `super()`. But we can't do that here due to TypeScript's restriction.\n    //   See: https://github.com/Microsoft/TypeScript/issues/8277\n    //   As a result, we have to add checks in `get trainable()` and\n    //   `set trainable()` below in order to prevent using `this.layer` when\n    //   its value is `undefined`. The super constructor does use the getter\n    //   and the setter of `this.layer`.\n    _this = _super.call(this, args);\n    _this.layer = args.layer;\n    return _this;\n  }\n  _createClass(Wrapper, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      this.built = true;\n    }\n    // TODO(cais): Implement activityRegularizer getter.\n  }, {\n    key: \"trainable\",\n    get: function get() {\n      // Porting Note: the check of `this.layer` here is necessary due to the\n      //   way the `constructor` of this class is written (see Porting Note\n      //   above).\n      if (this.layer != null) {\n        return this.layer.trainable;\n      } else {\n        return false;\n      }\n    },\n    set: function set(value) {\n      // Porting Note: the check of `this.layer` here is necessary due to the\n      //   way the `constructor` of this class is written (see Porting Note\n      //   above).\n      if (this.layer != null) {\n        this.layer.trainable = value;\n      }\n    }\n  }, {\n    key: \"trainableWeights\",\n    get: function get() {\n      return this.layer.trainableWeights;\n    }\n    // TODO(cais): Implement setter for trainableWeights.\n  }, {\n    key: \"nonTrainableWeights\",\n    get: function get() {\n      return this.layer.nonTrainableWeights;\n    }\n    // TODO(cais): Implement setter for nonTrainableWeights.\n  }, {\n    key: \"updates\",\n    get: function get() {\n      // tslint:disable-next-line:no-any\n      return this.layer._updates;\n    }\n    // TODO(cais): Implement getUpdatesFor().\n  }, {\n    key: \"losses\",\n    get: function get() {\n      return this.layer.losses;\n    }\n    // TODO(cais): Implement getLossesFor().\n  }, {\n    key: \"getWeights\",\n    value: function getWeights() {\n      return this.layer.getWeights();\n    }\n  }, {\n    key: \"setWeights\",\n    value: function setWeights(weights) {\n      this.layer.setWeights(weights);\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        'layer': {\n          'className': this.layer.getClassName(),\n          'config': this.layer.getConfig()\n        }\n      };\n      var baseConfig = _get(_getPrototypeOf(Wrapper.prototype), \"getConfig\", this).call(this);\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }, {\n    key: \"setFastWeightInitDuringBuild\",\n    value: function setFastWeightInitDuringBuild(value) {\n      _get(_getPrototypeOf(Wrapper.prototype), \"setFastWeightInitDuringBuild\", this).call(this, value);\n      if (this.layer != null) {\n        this.layer.setFastWeightInitDuringBuild(value);\n      }\n    }\n    /** @nocollapse */\n  }], [{\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      var customObjects = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n      var layerConfig = config['layer'];\n      var layer = deserialize(layerConfig, customObjects);\n      delete config['layer'];\n      var newConfig = {\n        layer: layer\n      };\n      Object.assign(newConfig, config);\n      return new cls(newConfig);\n    }\n  }]);\n  return Wrapper;\n}(Layer);\nexport var TimeDistributed = /*#__PURE__*/function (_Wrapper) {\n  _inherits(TimeDistributed, _Wrapper);\n  var _super2 = _createSuper(TimeDistributed);\n  function TimeDistributed(args) {\n    var _this2;\n    _classCallCheck(this, TimeDistributed);\n    _this2 = _super2.call(this, args);\n    _this2.supportsMasking = true;\n    return _this2;\n  }\n  _createClass(TimeDistributed, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n      if (inputShape.length < 3) {\n        throw new ValueError(\"TimeDistributed layer expects an input shape >= 3D, but received \" + \"input shape \".concat(JSON.stringify(inputShape)));\n      }\n      this.inputSpec = [{\n        shape: inputShape\n      }];\n      var childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n      if (!this.layer.built) {\n        this.layer.build(childInputShape);\n        this.layer.built = true;\n      }\n      _get(_getPrototypeOf(TimeDistributed.prototype), \"build\", this).call(this, inputShape);\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n      var childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n      var childOutputShape = this.layer.computeOutputShape(childInputShape);\n      var timesteps = inputShape[1];\n      return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this3 = this;\n      return tidy(function () {\n        // TODO(cais): Add 'training' and 'useLearningPhase' to kwargs.\n        inputs = getExactlyOneTensor(inputs);\n        // Porting Note: In tfjs-layers, `inputs` are always concrete tensor\n        // values. Hence the inputs can't have an undetermined first (batch)\n        // dimension, which is why we always use the K.rnn approach here.\n        var step = function step(inputs, states) {\n          // TODO(cais): Add useLearningPhase.\n          // NOTE(cais): `layer.call` may return a length-1 array of Tensor in\n          //   some cases (e.g., `layer` is a `Sequential` instance), which is\n          //   why `getExactlyOneTensor` is used below.\n          var output = getExactlyOneTensor(_this3.layer.call(inputs, kwargs));\n          return [output, []];\n        };\n        var rnnOutputs = rnn(step, inputs, [], false /* goBackwards */, null /* mask */, null /* constants */, false /* unroll */, true /* needPerStepOutputs */);\n        var y = rnnOutputs[1];\n        // TODO(cais): Add activity regularization.\n        // TODO(cais): Add useLearningPhase.\n        return y;\n      });\n    }\n  }]);\n  return TimeDistributed;\n}(Wrapper);\n/** @nocollapse */\nTimeDistributed.className = 'TimeDistributed';\nserialization.registerClass(TimeDistributed);\nexport function checkBidirectionalMergeMode(value) {\n  generic_utils.checkStringTypeUnionValue(VALID_BIDIRECTIONAL_MERGE_MODES, 'BidirectionalMergeMode', value);\n}\nvar DEFAULT_BIDIRECTIONAL_MERGE_MODE = 'concat';\nexport var Bidirectional = /*#__PURE__*/function (_Wrapper2) {\n  _inherits(Bidirectional, _Wrapper2);\n  var _super3 = _createSuper(Bidirectional);\n  function Bidirectional(args) {\n    var _this4;\n    _classCallCheck(this, Bidirectional);\n    _this4 = _super3.call(this, args);\n    // Note: When creating `this.forwardLayer`, the original Layer object\n    //   (`config.layer`) ought to be cloned. This is why we call\n    //   `getConfig()` followed by `deserialize()`. Without this cloning,\n    //   the layer names saved during serialization will incorrectly contain\n    //   the 'forward_' prefix. In Python Keras, this is done using\n    //   `copy.copy` (shallow copy), which does not have a simple equivalent\n    //   in JavaScript. JavaScript's `Object.assign()` does not copy\n    //   methods.\n    var layerConfig = args.layer.getConfig();\n    var forwDict = {};\n    forwDict['className'] = args.layer.getClassName();\n    forwDict['config'] = layerConfig;\n    _this4.forwardLayer = deserialize(forwDict);\n    layerConfig['goBackwards'] = layerConfig['goBackwards'] === true ? false : true;\n    var backDict = {};\n    backDict['className'] = args.layer.getClassName();\n    backDict['config'] = layerConfig;\n    _this4.backwardLayer = deserialize(backDict);\n    _this4.forwardLayer.name = 'forward_' + _this4.forwardLayer.name;\n    _this4.backwardLayer.name = 'backward_' + _this4.backwardLayer.name;\n    _this4.mergeMode = args.mergeMode === undefined ? DEFAULT_BIDIRECTIONAL_MERGE_MODE : args.mergeMode;\n    checkBidirectionalMergeMode(_this4.mergeMode);\n    if (args.weights) {\n      throw new NotImplementedError('weights support is not implemented for Bidirectional layer yet.');\n    }\n    _this4._stateful = args.layer.stateful;\n    _this4.returnSequences = args.layer.returnSequences;\n    _this4.returnState = args.layer.returnState;\n    _this4.supportsMasking = true;\n    _this4._trainable = true;\n    _this4.inputSpec = args.layer.inputSpec;\n    _this4.numConstants = null;\n    return _this4;\n  }\n  _createClass(Bidirectional, [{\n    key: \"trainable\",\n    get: function get() {\n      return this._trainable;\n    },\n    set: function set(value) {\n      // Porting Note: the check of `this.layer` here is necessary due to the\n      //   way the `constructor` of this class is written (see Porting Note\n      //   above).\n      this._trainable = value;\n      if (this.forwardLayer != null) {\n        this.forwardLayer.trainable = value;\n      }\n      if (this.backwardLayer != null) {\n        this.backwardLayer.trainable = value;\n      }\n    }\n  }, {\n    key: \"getWeights\",\n    value: function getWeights() {\n      return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());\n    }\n  }, {\n    key: \"setWeights\",\n    value: function setWeights(weights) {\n      var numWeights = weights.length;\n      var numeightsOver2 = Math.floor(numWeights / 2);\n      this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n      this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      var layerShapes = this.forwardLayer.computeOutputShape(inputShape);\n      if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n        layerShapes = [layerShapes];\n      }\n      layerShapes = layerShapes;\n      var outputShape;\n      var outputShapes;\n      var stateShape;\n      if (this.returnState) {\n        stateShape = layerShapes.slice(1);\n        outputShape = layerShapes[0];\n      } else {\n        outputShape = layerShapes[0];\n      }\n      outputShape = outputShape;\n      if (this.mergeMode === 'concat') {\n        outputShape[outputShape.length - 1] *= 2;\n        outputShapes = [outputShape];\n      } else if (this.mergeMode == null) {\n        outputShapes = [outputShape, outputShape.slice()];\n      } else {\n        outputShapes = [outputShape];\n      }\n      if (this.returnState) {\n        if (this.mergeMode == null) {\n          return outputShapes.concat(stateShape).concat(stateShape.slice());\n        }\n        return [outputShape].concat(stateShape).concat(stateShape.slice());\n      }\n      return generic_utils.singletonOrArray(outputShapes);\n    }\n  }, {\n    key: \"apply\",\n    value: function apply(inputs, kwargs) {\n      var initialState = kwargs == null ? null : kwargs['initialState'];\n      var constants = kwargs == null ? null : kwargs['constants'];\n      if (kwargs == null) {\n        kwargs = {};\n      }\n      var standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n      inputs = standardized.inputs;\n      initialState = standardized.initialState;\n      constants = standardized.constants;\n      if (Array.isArray(inputs)) {\n        initialState = inputs.slice(1);\n        inputs = inputs[0];\n      }\n      if ((initialState == null || initialState.length === 0) && constants == null) {\n        return _get(_getPrototypeOf(Bidirectional.prototype), \"apply\", this).call(this, inputs, kwargs);\n      }\n      var additionalInputs = [];\n      var additionalSpecs = [];\n      if (initialState != null) {\n        var numStates = initialState.length;\n        if (numStates % 2 > 0) {\n          throw new ValueError('When passing `initialState` to a Bidrectional RNN, ' + 'the state should be an Array containing the states of ' + 'the underlying RNNs.');\n        }\n        kwargs['initialState'] = initialState;\n        additionalInputs.push.apply(additionalInputs, _toConsumableArray(initialState));\n        var stateSpecs = initialState.map(function (state) {\n          return new InputSpec({\n            shape: state.shape\n          });\n        });\n        this.forwardLayer.stateSpec = stateSpecs.slice(0, numStates / 2);\n        this.backwardLayer.stateSpec = stateSpecs.slice(numStates / 2);\n        additionalSpecs.push.apply(additionalSpecs, _toConsumableArray(stateSpecs));\n      }\n      if (constants != null) {\n        throw new NotImplementedError('Support for constants in Bidirectional layers is not ' + 'implemented yet.');\n      }\n      var isSymbolicTensor = additionalInputs[0] instanceof SymbolicTensor;\n      for (var _i = 0, _additionalInputs = additionalInputs; _i < _additionalInputs.length; _i++) {\n        var tensor = _additionalInputs[_i];\n        if (tensor instanceof SymbolicTensor !== isSymbolicTensor) {\n          throw new ValueError('The initial state of a Bidirectional layer cannot be ' + 'specified as a mix of symbolic and non-symbolic tensors');\n        }\n      }\n      if (isSymbolicTensor) {\n        // Compute the full input and specs, including the states.\n        var fullInput = [inputs].concat(additionalInputs);\n        var fullInputSpec = this.inputSpec.concat(additionalSpecs);\n        // Perform the call temporarily and replace inputSpec.\n        // Note: with initial states symbolic calls and non-symbolic calls to\n        // this method differ in how the initial states are passed. For\n        // symbolic calls, the initial states are passed in the first arg, as\n        // an Array of SymbolicTensors; for non-symbolic calls, they are\n        // passed in the second arg as a part of the kwargs. Hence the need to\n        // temporarily modify inputSpec here.\n        // TODO(cais): Make refactoring so that this hacky code below is no\n        // longer needed.\n        var originalInputSpec = this.inputSpec;\n        this.inputSpec = fullInputSpec;\n        var output = _get(_getPrototypeOf(Bidirectional.prototype), \"apply\", this).call(this, fullInput, kwargs);\n        this.inputSpec = originalInputSpec;\n        return output;\n      } else {\n        return _get(_getPrototypeOf(Bidirectional.prototype), \"apply\", this).call(this, inputs, kwargs);\n      }\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this5 = this;\n      return tidy(function () {\n        var initialState = kwargs['initialState'];\n        var y;\n        var yRev;\n        if (initialState == null) {\n          y = _this5.forwardLayer.call(inputs, kwargs);\n          yRev = _this5.backwardLayer.call(inputs, kwargs);\n        } else {\n          var forwardState = initialState.slice(0, initialState.length / 2);\n          var backwardState = initialState.slice(initialState.length / 2);\n          y = _this5.forwardLayer.call(inputs, Object.assign(kwargs, {\n            initialState: forwardState\n          }));\n          yRev = _this5.backwardLayer.call(inputs, Object.assign(kwargs, {\n            initialState: backwardState\n          }));\n        }\n        var states;\n        if (_this5.returnState) {\n          if (Array.isArray(y)) {\n            states = y.slice(1).concat(yRev.slice(1));\n          } else {}\n          y = y[0];\n          yRev = yRev[0];\n        }\n        if (_this5.returnSequences) {\n          yRev = tfc.reverse(yRev, 1);\n        }\n        var output;\n        if (_this5.mergeMode === 'concat') {\n          output = K.concatenate([y, yRev]);\n        } else if (_this5.mergeMode === 'sum') {\n          output = tfc.add(y, yRev);\n        } else if (_this5.mergeMode === 'ave') {\n          output = tfc.mul(.5, tfc.add(y, yRev));\n        } else if (_this5.mergeMode === 'mul') {\n          output = tfc.mul(y, yRev);\n        } else if (_this5.mergeMode == null) {\n          output = [y, yRev];\n        }\n        // TODO(cais): Properly set learning phase.\n        if (_this5.returnState) {\n          if (_this5.mergeMode == null) {\n            return output.concat(states);\n          }\n          return [output].concat(states);\n        }\n        return output;\n      });\n    }\n  }, {\n    key: \"resetStates\",\n    value: function resetStates(states) {\n      this.forwardLayer.resetStates();\n      this.backwardLayer.resetStates();\n    }\n  }, {\n    key: \"build\",\n    value: function build(inputShape) {\n      var _this6 = this;\n      nameScope(this.forwardLayer.name, function () {\n        _this6.forwardLayer.build(inputShape);\n      });\n      nameScope(this.backwardLayer.name, function () {\n        _this6.backwardLayer.build(inputShape);\n      });\n      this.built = true;\n    }\n  }, {\n    key: \"computeMask\",\n    value: function computeMask(inputs, mask) {\n      if (Array.isArray(mask)) {\n        mask = mask[0];\n      }\n      var outputMask;\n      if (this.returnSequences) {\n        if (this.mergeMode == null) {\n          outputMask = [mask, mask];\n        } else {\n          outputMask = mask;\n        }\n      } else {\n        if (this.mergeMode == null) {\n          outputMask = [null, null];\n        } else {\n          outputMask = null;\n        }\n      }\n      if (this.returnState) {\n        var states = this.forwardLayer.states;\n        var stateMask = states.map(function (state) {\n          return null;\n        });\n        if (Array.isArray(outputMask)) {\n          return outputMask.concat(stateMask).concat(stateMask);\n        } else {\n          return [outputMask].concat(stateMask).concat(stateMask);\n        }\n      } else {\n        return outputMask;\n      }\n    }\n  }, {\n    key: \"trainableWeights\",\n    get: function get() {\n      return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);\n    }\n  }, {\n    key: \"nonTrainableWeights\",\n    get: function get() {\n      return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);\n    }\n    // TODO(cais): Implement constraints().\n  }, {\n    key: \"setFastWeightInitDuringBuild\",\n    value: function setFastWeightInitDuringBuild(value) {\n      _get(_getPrototypeOf(Bidirectional.prototype), \"setFastWeightInitDuringBuild\", this).call(this, value);\n      if (this.forwardLayer != null) {\n        this.forwardLayer.setFastWeightInitDuringBuild(value);\n      }\n      if (this.backwardLayer != null) {\n        this.backwardLayer.setFastWeightInitDuringBuild(value);\n      }\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        'mergeMode': this.mergeMode\n      };\n      // TODO(cais): Add logic for `numConstants` once the property is added.\n      var baseConfig = _get(_getPrototypeOf(Bidirectional.prototype), \"getConfig\", this).call(this);\n      Object.assign(config, baseConfig);\n      return config;\n    }\n    /** @nocollapse */\n  }], [{\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      var rnnLayer = deserialize(config['layer']);\n      delete config['layer'];\n      // TODO(cais): Add logic for `numConstants` once the property is added.\n      if (config['numConstants'] != null) {\n        throw new NotImplementedError(\"Deserialization of a Bidirectional layer with numConstants \" + \"present is not supported yet.\");\n      }\n      // tslint:disable-next-line:no-any\n      var newConfig = config;\n      newConfig['layer'] = rnnLayer;\n      return new cls(newConfig);\n    }\n  }]);\n  return Bidirectional;\n}(Wrapper);\n/** @nocollapse */\nBidirectional.className = 'Bidirectional';\nserialization.registerClass(Bidirectional);","map":{"version":3,"mappings":";;;;;;;AAAA;;;;;;;;;AAUA;;;AAIA,OAAO,KAAKA,GAAG,MAAM,uBAAuB;AAC5C,SAAQC,aAAa,EAAUC,IAAI,QAAO,uBAAuB;AACjE,OAAO,KAAKC,CAAC,MAAM,yBAAyB;AAC5C,SAAQC,SAAS,QAAO,WAAW;AACnC,SAAQC,SAAS,EAAEC,KAAK,EAAaC,cAAc,QAAO,oBAAoB;AAC9E,SAAQC,mBAAmB,EAAEC,UAAU,QAAO,WAAW;AACzD,SAAuCC,+BAA+B,QAAO,wBAAwB;AAGrG,OAAO,KAAKC,aAAa,MAAM,wBAAwB;AACvD,SAAQC,kBAAkB,EAAEC,mBAAmB,QAAO,sBAAsB;AAG5E,SAAQC,GAAG,EAAOC,eAAe,QAAO,aAAa;AACrD,SAAQC,WAAW,QAAO,iBAAiB;AAS3C;;;;;;;AAOA,WAAsBC,OAAQ;EAAA;EAAA;EAG5B,iBAAYC,IAAsB;IAAA;IAAA;IAChC;IACA;IACA;IACA;IACA;IACA;IACA;IACA,0BAAMA,IAAI;IACV,MAAKC,KAAK,GAAGD,IAAI,CAACC,KAAK;IAAC;EAC1B;EAAC;IAAA;IAAA,OAEQ,eAAMC,UAAyB;MACtC,IAAI,CAACC,KAAK,GAAG,IAAI;IACnB;IAEA;EAAA;IAAA;IAAA,KAEA,eAAsB;MACpB;MACA;MACA;MACA,IAAI,IAAI,CAACF,KAAK,IAAI,IAAI,EAAE;QACtB,OAAO,IAAI,CAACA,KAAK,CAACG,SAAS;OAC5B,MAAM;QACL,OAAO,KAAK;;IAEhB,CAAC;IAAA,KAED,aAAuBC,KAAc;MACnC;MACA;MACA;MACA,IAAI,IAAI,CAACJ,KAAK,IAAI,IAAI,EAAE;QACtB,IAAI,CAACA,KAAK,CAACG,SAAS,GAAGC,KAAK;;IAEhC;EAAC;IAAA;IAAA,KAED,eAA6B;MAC3B,OAAO,IAAI,CAACJ,KAAK,CAACK,gBAAgB;IACpC;IACA;EAAA;IAAA;IAAA,KAEA,eAAgC;MAC9B,OAAO,IAAI,CAACL,KAAK,CAACM,mBAAmB;IACvC;IACA;EAAA;IAAA;IAAA,KAEA,eAAoB;MAClB;MACA,OAAQ,IAAI,CAACN,KAAa,CAACO,QAAQ;IACrC;IAEA;EAAA;IAAA;IAAA,KAEA,eAAmB;MACjB,OAAO,IAAI,CAACP,KAAK,CAACQ,MAAM;IAC1B;IAEA;EAAA;IAAA;IAAA,OAES,sBAAU;MACjB,OAAO,IAAI,CAACR,KAAK,CAACS,UAAU,EAAE;IAChC;EAAC;IAAA;IAAA,OAEQ,oBAAWC,OAAiB;MACnC,IAAI,CAACV,KAAK,CAACW,UAAU,CAACD,OAAO,CAAC;IAChC;EAAC;IAAA;IAAA,OAEQ,qBAAS;MAChB,IAAME,MAAM,GAA6B;QACvC,OAAO,EAAE;UACP,WAAW,EAAE,IAAI,CAACZ,KAAK,CAACa,YAAY,EAAE;UACtC,QAAQ,EAAE,IAAI,CAACb,KAAK,CAACc,SAAS;;OAEjC;MACD,IAAMC,UAAU,yEAAoB;MACpCC,MAAM,CAACC,MAAM,CAACL,MAAM,EAAEG,UAAU,CAAC;MACjC,OAAOH,MAAM;IACf;EAAC;IAAA;IAAA,OAEQ,sCAA6BR,KAAc;MAClD,0FAAmCA,KAAK;MACxC,IAAI,IAAI,CAACJ,KAAK,IAAI,IAAI,EAAE;QACtB,IAAI,CAACA,KAAK,CAACkB,4BAA4B,CAACd,KAAK,CAAC;;IAElD;IAEA;EAAA;IAAA;IAAA,OACA,oBACIe,GAA6C,EAC7CP,MAAgC,EACc;MAAA,IAA9CQ,oFAAgB,EAA8B;MAChD,IAAMC,WAAW,GAAGT,MAAM,CAAC,OAAO,CAA6B;MAC/D,IAAMZ,KAAK,GAAGH,WAAW,CAACwB,WAAW,EAAED,aAAa,CAAU;MAC9D,OAAOR,MAAM,CAAC,OAAO,CAAC;MACtB,IAAMU,SAAS,GAAG;QAACtB,KAAK,EAALA;MAAK,CAAC;MACzBgB,MAAM,CAACC,MAAM,CAACK,SAAS,EAAEV,MAAM,CAAC;MAChC,OAAO,IAAIO,GAAG,CAACG,SAAS,CAAC;IAC3B;EAAC;EAAA;AAAA,EAtGmCnC,KAAK;AAyG3C,WAAaoC,eAAgB;EAAA;EAAA;EAG3B,yBAAYxB,IAAsB;IAAA;IAAA;IAChC,4BAAMA,IAAI;IACV,OAAKyB,eAAe,GAAG,IAAI;IAAC;EAC9B;EAAC;IAAA;IAAA,OAEQ,eAAMvB,UAAyB;MACtCA,UAAU,GAAGR,kBAAkB,CAACQ,UAAU,CAAC;MAC3C,IAAIA,UAAU,CAACwB,MAAM,GAAG,CAAC,EAAE;QACzB,MAAM,IAAInC,UAAU,CAChB,4FACeoC,IAAI,CAACC,SAAS,CAAC1B,UAAU,CAAC,CAAE,CAAC;;MAElD,IAAI,CAAC2B,SAAS,GAAG,CAAC;QAACC,KAAK,EAAE5B;MAAU,CAAC,CAAC;MACtC,IAAM6B,eAAe,GAAG,CAAC7B,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC8B,MAAM,CAAC9B,UAAU,CAAC+B,KAAK,CAAC,CAAC,CAAC,CAAC;MACnE,IAAI,CAAC,IAAI,CAAChC,KAAK,CAACE,KAAK,EAAE;QACrB,IAAI,CAACF,KAAK,CAACiC,KAAK,CAACH,eAAe,CAAC;QACjC,IAAI,CAAC9B,KAAK,CAACE,KAAK,GAAG,IAAI;;MAEzB,2EAAYD,UAAU;IACxB;EAAC;IAAA;IAAA,OAEQ,4BAAmBA,UAAyB;MACnDA,UAAU,GAAGR,kBAAkB,CAACQ,UAAU,CAAC;MAC3C,IAAM6B,eAAe,GAAG,CAAC7B,UAAU,CAAC,CAAC,CAAC,CAAC,CAAC8B,MAAM,CAAC9B,UAAU,CAAC+B,KAAK,CAAC,CAAC,CAAC,CAAC;MACnE,IAAME,gBAAgB,GAClB,IAAI,CAAClC,KAAK,CAACmC,kBAAkB,CAACL,eAAe,CAAU;MAC3D,IAAMM,SAAS,GAAGnC,UAAU,CAAC,CAAC,CAAC;MAC/B,OAAO,CAACiC,gBAAgB,CAAC,CAAC,CAAC,EAAEE,SAAS,CAAC,CAACL,MAAM,CAACG,gBAAgB,CAACF,KAAK,CAAC,CAAC,CAAC,CAAC;IAC3E;EAAC;IAAA;IAAA,OAEQ,cAAKK,MAAuB,EAAEC,MAAc;MAAA;MACnD,OAAOvD,IAAI,CAAC,YAAK;QACf;QACAsD,MAAM,GAAG3C,mBAAmB,CAAC2C,MAAM,CAAC;QACpC;QACA;QACA;QACA,IAAME,IAAI,GAAoB,SAAxBA,IAAI,CAAqBF,MAAc,EAAEG,MAAgB,EAAI;UACjE;UACA;UACA;UACA;UACA,IAAMC,MAAM,GAAG/C,mBAAmB,CAAC,MAAI,CAACM,KAAK,CAAC0C,IAAI,CAACL,MAAM,EAAEC,MAAM,CAAC,CAAC;UACnE,OAAO,CAACG,MAAM,EAAE,EAAE,CAAC;QACrB,CAAC;QACD,IAAME,UAAU,GACZhD,GAAG,CAAC4C,IAAI,EAAEF,MAAM,EAAE,EAAE,EAAE,KAAK,CAAC,mBAAmB,IAAI,CAAC,YAChD,IAAI,CAAC,iBAAiB,KAAK,CAAC,cAC5B,IAAI,CAAC,yBAAyB;QACtC,IAAMO,CAAC,GAAGD,UAAU,CAAC,CAAC,CAAC;QACvB;QACA;QACA,OAAOC,CAAC;MACV,CAAC,CAAC;IACJ;EAAC;EAAA;AAAA,EAzDkC9C,OAAO;AAC1C;AACOyB,yBAAS,GAAG,iBAAiB;AA2DtCzC,aAAa,CAAC+D,aAAa,CAACtB,eAAe,CAAC;AAE5C,OAAM,SAAUuB,2BAA2B,CAAC1C,KAAc;EACxDZ,aAAa,CAACuD,yBAAyB,CACnCxD,+BAA+B,EAAE,wBAAwB,EAAEa,KAAK,CAAC;AACvE;AAkBA,IAAM4C,gCAAgC,GAA2B,QAAQ;AAEzE,WAAaC,aAAc;EAAA;EAAA;EAWzB,uBAAYlD,IAA4B;IAAA;IAAA;IACtC,4BAAMA,IAAI;IAEV;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA,IAAMsB,WAAW,GAAGtB,IAAI,CAACC,KAAK,CAACc,SAAS,EAAE;IAC1C,IAAMoC,QAAQ,GAA6B,EAAE;IAC7CA,QAAQ,CAAC,WAAW,CAAC,GAAGnD,IAAI,CAACC,KAAK,CAACa,YAAY,EAAE;IACjDqC,QAAQ,CAAC,QAAQ,CAAC,GAAG7B,WAAW;IAChC,OAAK8B,YAAY,GAAGtD,WAAW,CAACqD,QAAQ,CAAQ;IAChD7B,WAAW,CAAC,aAAa,CAAC,GACtBA,WAAW,CAAC,aAAa,CAAC,KAAK,IAAI,GAAG,KAAK,GAAG,IAAI;IACtD,IAAM+B,QAAQ,GAA6B,EAAE;IAC7CA,QAAQ,CAAC,WAAW,CAAC,GAAGrD,IAAI,CAACC,KAAK,CAACa,YAAY,EAAE;IACjDuC,QAAQ,CAAC,QAAQ,CAAC,GAAG/B,WAAW;IAChC,OAAKgC,aAAa,GAAGxD,WAAW,CAACuD,QAAQ,CAAQ;IACjD,OAAKD,YAAY,CAACG,IAAI,GAAG,UAAU,GAAG,OAAKH,YAAY,CAACG,IAAI;IAC5D,OAAKD,aAAa,CAACC,IAAI,GAAG,WAAW,GAAG,OAAKD,aAAa,CAACC,IAAI;IAE/D,OAAKC,SAAS,GAAGxD,IAAI,CAACwD,SAAS,KAAKC,SAAS,GACzCR,gCAAgC,GAChCjD,IAAI,CAACwD,SAAS;IAClBT,2BAA2B,CAAC,OAAKS,SAAS,CAAC;IAC3C,IAAIxD,IAAI,CAACW,OAAO,EAAE;MAChB,MAAM,IAAIrB,mBAAmB,CACzB,iEAAiE,CAAC;;IAExE,OAAKoE,SAAS,GAAG1D,IAAI,CAACC,KAAK,CAAC0D,QAAQ;IACpC,OAAKC,eAAe,GAAG5D,IAAI,CAACC,KAAK,CAAC2D,eAAe;IACjD,OAAKC,WAAW,GAAG7D,IAAI,CAACC,KAAK,CAAC4D,WAAW;IACzC,OAAKpC,eAAe,GAAG,IAAI;IAC3B,OAAKqC,UAAU,GAAG,IAAI;IACtB,OAAKjC,SAAS,GAAG7B,IAAI,CAACC,KAAK,CAAC4B,SAAS;IACrC,OAAKkC,YAAY,GAAG,IAAI;IAAC;EAC3B;EAAC;IAAA;IAAA,KAED,eAAsB;MACpB,OAAO,IAAI,CAACD,UAAU;IACxB,CAAC;IAAA,KAED,aAAuBzD,KAAc;MACnC;MACA;MACA;MACA,IAAI,CAACyD,UAAU,GAAGzD,KAAK;MACvB,IAAI,IAAI,CAAC+C,YAAY,IAAI,IAAI,EAAE;QAC7B,IAAI,CAACA,YAAY,CAAChD,SAAS,GAAGC,KAAK;;MAErC,IAAI,IAAI,CAACiD,aAAa,IAAI,IAAI,EAAE;QAC9B,IAAI,CAACA,aAAa,CAAClD,SAAS,GAAGC,KAAK;;IAExC;EAAC;IAAA;IAAA,OAEQ,sBAAU;MACjB,OAAO,IAAI,CAAC+C,YAAY,CAAC1C,UAAU,EAAE,CAACsB,MAAM,CACxC,IAAI,CAACsB,aAAa,CAAC5C,UAAU,EAAE,CAAC;IACtC;EAAC;IAAA;IAAA,OAEQ,oBAAWC,OAAiB;MACnC,IAAMqD,UAAU,GAAGrD,OAAO,CAACe,MAAM;MACjC,IAAMuC,cAAc,GAAGC,IAAI,CAACC,KAAK,CAACH,UAAU,GAAG,CAAC,CAAC;MACjD,IAAI,CAACZ,YAAY,CAACxC,UAAU,CAACD,OAAO,CAACsB,KAAK,CAAC,CAAC,EAAEgC,cAAc,CAAC,CAAC;MAC9D,IAAI,CAACX,aAAa,CAAC1C,UAAU,CAACD,OAAO,CAACsB,KAAK,CAACgC,cAAc,CAAC,CAAC;IAC9D;EAAC;IAAA;IAAA,OAEQ,4BAAmB/D,UAAyB;MACnD,IAAIkE,WAAW,GACX,IAAI,CAAChB,YAAY,CAAChB,kBAAkB,CAAClC,UAAU,CAAC;MACpD,IAAI,EAAEmE,KAAK,CAACC,OAAO,CAACF,WAAW,CAAC,IAAIC,KAAK,CAACC,OAAO,CAACF,WAAW,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE;QAClEA,WAAW,GAAG,CAACA,WAAoB,CAAC;;MAEtCA,WAAW,GAAGA,WAAsB;MAEpC,IAAIG,WAAkB;MACtB,IAAIC,YAAqB;MACzB,IAAIC,UAAmB;MACvB,IAAI,IAAI,CAACZ,WAAW,EAAE;QACpBY,UAAU,GAAGL,WAAW,CAACnC,KAAK,CAAC,CAAC,CAAC;QACjCsC,WAAW,GAAGH,WAAW,CAAC,CAAC,CAAC;OAC7B,MAAM;QACLG,WAAW,GAAGH,WAAW,CAAC,CAAC,CAAC;;MAE9BG,WAAW,GAAGA,WAAW;MACzB,IAAI,IAAI,CAACf,SAAS,KAAK,QAAQ,EAAE;QAC/Be,WAAW,CAACA,WAAW,CAAC7C,MAAM,GAAG,CAAC,CAAC,IAAI,CAAC;QACxC8C,YAAY,GAAG,CAACD,WAAW,CAAC;OAC7B,MAAM,IAAI,IAAI,CAACf,SAAS,IAAI,IAAI,EAAE;QACjCgB,YAAY,GAAG,CAACD,WAAW,EAAEA,WAAW,CAACtC,KAAK,EAAE,CAAC;OAClD,MAAM;QACLuC,YAAY,GAAG,CAACD,WAAW,CAAC;;MAG9B,IAAI,IAAI,CAACV,WAAW,EAAE;QACpB,IAAI,IAAI,CAACL,SAAS,IAAI,IAAI,EAAE;UAC1B,OAAOgB,YAAY,CAACxC,MAAM,CAACyC,UAAU,CAAC,CAACzC,MAAM,CAACyC,UAAU,CAACxC,KAAK,EAAE,CAAC;;QAEnE,OAAO,CAACsC,WAAW,CAAC,CAACvC,MAAM,CAACyC,UAAU,CAAC,CAACzC,MAAM,CAACyC,UAAU,CAACxC,KAAK,EAAE,CAAC;;MAEpE,OAAOxC,aAAa,CAACiF,gBAAgB,CAACF,YAAY,CAAC;IACrD;EAAC;IAAA;IAAA,OAEQ,eACLlC,MAAuD,EACvDC,MAAe;MACjB,IAAIoC,YAAY,GACZpC,MAAM,IAAI,IAAI,GAAG,IAAI,GAAGA,MAAM,CAAC,cAAc,CAAC;MAClD,IAAIqC,SAAS,GACTrC,MAAM,IAAI,IAAI,GAAG,IAAI,GAAGA,MAAM,CAAC,WAAW,CAAC;MAC/C,IAAIA,MAAM,IAAI,IAAI,EAAE;QAClBA,MAAM,GAAG,EAAE;;MAEb,IAAMsC,YAAY,GACdhF,eAAe,CAACyC,MAAM,EAAEqC,YAAY,EAAEC,SAAS,EAAE,IAAI,CAACb,YAAY,CAAC;MACvEzB,MAAM,GAAGuC,YAAY,CAACvC,MAAM;MAC5BqC,YAAY,GAAGE,YAAY,CAACF,YAAY;MACxCC,SAAS,GAAGC,YAAY,CAACD,SAAS;MAElC,IAAIP,KAAK,CAACC,OAAO,CAAChC,MAAM,CAAC,EAAE;QACzBqC,YAAY,GAAIrC,MAAsC,CAACL,KAAK,CAAC,CAAC,CAAC;QAC/DK,MAAM,GAAIA,MAAsC,CAAC,CAAC,CAAC;;MAGrD,IAAI,CAACqC,YAAY,IAAI,IAAI,IAAIA,YAAY,CAACjD,MAAM,KAAK,CAAC,KAClDkD,SAAS,IAAI,IAAI,EAAE;QACrB,gFAAmBtC,MAAM,EAAEC,MAAM;;MAEnC,IAAMuC,gBAAgB,GAAiC,EAAE;MACzD,IAAMC,eAAe,GAAgB,EAAE;MACvC,IAAIJ,YAAY,IAAI,IAAI,EAAE;QACxB,IAAMK,SAAS,GAAGL,YAAY,CAACjD,MAAM;QACrC,IAAIsD,SAAS,GAAG,CAAC,GAAG,CAAC,EAAE;UACrB,MAAM,IAAIzF,UAAU,CAChB,qDAAqD,GACrD,wDAAwD,GACxD,sBAAsB,CAAC;;QAE7BgD,MAAM,CAAC,cAAc,CAAC,GAAGoC,YAAY;QACrCG,gBAAgB,CAACG,IAAI,OAArBH,gBAAgB,qBAASH,YAAY,EAAC;QACtC,IAAMO,UAAU,GAAIP,YAA6C,CACzCQ,GAAG,CAAC,eAAK;UAAA,OAAI,IAAIhG,SAAS,CAAC;YAAC2C,KAAK,EAAEsD,KAAK,CAACtD;UAAK,CAAC,CAAC;QAAA,EAAC;QACzE,IAAI,CAACsB,YAAY,CAACiC,SAAS,GAAGH,UAAU,CAACjD,KAAK,CAAC,CAAC,EAAE+C,SAAS,GAAG,CAAC,CAAC;QAChE,IAAI,CAAC1B,aAAa,CAAC+B,SAAS,GAAGH,UAAU,CAACjD,KAAK,CAAC+C,SAAS,GAAG,CAAC,CAAC;QAC9DD,eAAe,CAACE,IAAI,OAApBF,eAAe,qBAASG,UAAU,EAAC;;MAErC,IAAIN,SAAS,IAAI,IAAI,EAAE;QACrB,MAAM,IAAItF,mBAAmB,CACzB,uDAAuD,GACvD,kBAAkB,CAAC;;MAGzB,IAAMgG,gBAAgB,GAAGR,gBAAgB,CAAC,CAAC,CAAC,YAAYzF,cAAc;MACtE,qCAAqByF,gBAAgB,uCAAE;QAAlC,IAAMS,MAAM;QACf,IAAIA,MAAM,YAAYlG,cAAc,KAAKiG,gBAAgB,EAAE;UACzD,MAAM,IAAI/F,UAAU,CAChB,uDAAuD,GACvD,yDAAyD,CAAC;;;MAIlE,IAAI+F,gBAAgB,EAAE;QACpB;QACA,IAAME,SAAS,GAAG,CAAClD,MAAM,CAAC,CAACN,MAAM,CAAC8C,gBAAgB,CAAC;QACnD,IAAMW,aAAa,GAAG,IAAI,CAAC5D,SAAS,CAACG,MAAM,CAAC+C,eAAe,CAAC;QAC5D;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA;QACA,IAAMW,iBAAiB,GAAG,IAAI,CAAC7D,SAAS;QACxC,IAAI,CAACA,SAAS,GAAG4D,aAAa;QAC9B,IAAM/C,MAAM,4EACI8C,SAAwC,EAAEjD,MAAM,CAAC;QACjE,IAAI,CAACV,SAAS,GAAG6D,iBAAiB;QAClC,OAAOhD,MAAM;OACd,MAAM;QACL,gFAAmBJ,MAAM,EAAEC,MAAM;;IAErC;EAAC;IAAA;IAAA,OAEQ,cAAKD,MAAuB,EAAEC,MAAc;MAAA;MACnD,OAAOvD,IAAI,CAAC,YAAK;QACf,IAAM2F,YAAY,GAAGpC,MAAM,CAAC,cAAc,CAAC;QAE3C,IAAIM,CAAkB;QACtB,IAAI8C,IAAqB;QACzB,IAAIhB,YAAY,IAAI,IAAI,EAAE;UACxB9B,CAAC,GAAG,MAAI,CAACO,YAAY,CAACT,IAAI,CAACL,MAAM,EAAEC,MAAM,CAAC;UAC1CoD,IAAI,GAAG,MAAI,CAACrC,aAAa,CAACX,IAAI,CAACL,MAAM,EAAEC,MAAM,CAAC;SAC/C,MAAM;UACL,IAAMqD,YAAY,GAAGjB,YAAY,CAAC1C,KAAK,CAAC,CAAC,EAAE0C,YAAY,CAACjD,MAAM,GAAG,CAAC,CAAC;UACnE,IAAMmE,aAAa,GAAGlB,YAAY,CAAC1C,KAAK,CAAC0C,YAAY,CAACjD,MAAM,GAAG,CAAC,CAAC;UACjEmB,CAAC,GAAG,MAAI,CAACO,YAAY,CAACT,IAAI,CACtBL,MAAM,EAAErB,MAAM,CAACC,MAAM,CAACqB,MAAM,EAAE;YAACoC,YAAY,EAAEiB;UAAY,CAAC,CAAC,CAAC;UAChED,IAAI,GAAG,MAAI,CAACrC,aAAa,CAACX,IAAI,CAC1BL,MAAM,EAAErB,MAAM,CAACC,MAAM,CAACqB,MAAM,EAAE;YAACoC,YAAY,EAAEkB;UAAa,CAAC,CAAC,CAAC;;QAGnE,IAAIpD,MAAgB;QACpB,IAAI,MAAI,CAACoB,WAAW,EAAE;UACpB,IAAIQ,KAAK,CAACC,OAAO,CAACzB,CAAC,CAAC,EAAE;YACpBJ,MAAM,GAAGI,CAAC,CAACZ,KAAK,CAAC,CAAC,CAAC,CAACD,MAAM,CAAE2D,IAAiB,CAAC1D,KAAK,CAAC,CAAC,CAAC,CAAC;WACxD,MAAM;UAEPY,CAAC,GAAIA,CAAc,CAAC,CAAC,CAAC;UACtB8C,IAAI,GAAIA,IAAiB,CAAC,CAAC,CAAC;;QAG9B,IAAI,MAAI,CAAC/B,eAAe,EAAE;UACxB+B,IAAI,GAAG7G,GAAG,CAACgH,OAAO,CAACH,IAAc,EAAE,CAAC,CAAC;;QAGvC,IAAIjD,MAAuB;QAC3B,IAAI,MAAI,CAACc,SAAS,KAAK,QAAQ,EAAE;UAC/Bd,MAAM,GAAGzD,CAAC,CAAC8G,WAAW,CAAC,CAAClD,CAAW,EAAE8C,IAAc,CAAC,CAAC;SACtD,MAAM,IAAI,MAAI,CAACnC,SAAS,KAAK,KAAK,EAAE;UACnCd,MAAM,GAAG5D,GAAG,CAACkH,GAAG,CAACnD,CAAW,EAAE8C,IAAc,CAAC;SAC9C,MAAM,IAAI,MAAI,CAACnC,SAAS,KAAK,KAAK,EAAE;UACnCd,MAAM,GAAG5D,GAAG,CAACmH,GAAG,CAAC,EAAE,EAAEnH,GAAG,CAACkH,GAAG,CAACnD,CAAW,EAAE8C,IAAc,CAAC,CAAC;SAC3D,MAAM,IAAI,MAAI,CAACnC,SAAS,KAAK,KAAK,EAAE;UACnCd,MAAM,GAAG5D,GAAG,CAACmH,GAAG,CAACpD,CAAW,EAAE8C,IAAc,CAAC;SAC9C,MAAM,IAAI,MAAI,CAACnC,SAAS,IAAI,IAAI,EAAE;UACjCd,MAAM,GAAG,CAACG,CAAW,EAAE8C,IAAc,CAAC;;QAGxC;QACA,IAAI,MAAI,CAAC9B,WAAW,EAAE;UACpB,IAAI,MAAI,CAACL,SAAS,IAAI,IAAI,EAAE;YAC1B,OAAQd,MAAmB,CAACV,MAAM,CAACS,MAAM,CAAC;;UAE5C,OAAO,CAACC,MAAgB,CAAC,CAACV,MAAM,CAACS,MAAM,CAAC;;QAE1C,OAAOC,MAAM;MACf,CAAC,CAAC;IACJ;EAAC;IAAA;IAAA,OAEQ,qBAAYD,MAAwB;MAC3C,IAAI,CAACW,YAAY,CAAC8C,WAAW,EAAE;MAC/B,IAAI,CAAC5C,aAAa,CAAC4C,WAAW,EAAE;IAClC;EAAC;IAAA;IAAA,OAEQ,eAAMhG,UAAyB;MAAA;MACtChB,SAAS,CAAC,IAAI,CAACkE,YAAY,CAACG,IAAI,EAAE,YAAK;QACrC,MAAI,CAACH,YAAY,CAAClB,KAAK,CAAChC,UAAU,CAAC;MACrC,CAAC,CAAC;MACFhB,SAAS,CAAC,IAAI,CAACoE,aAAa,CAACC,IAAI,EAAE,YAAK;QACtC,MAAI,CAACD,aAAa,CAACpB,KAAK,CAAChC,UAAU,CAAC;MACtC,CAAC,CAAC;MACF,IAAI,CAACC,KAAK,GAAG,IAAI;IACnB;EAAC;IAAA;IAAA,OAEQ,qBAAYmC,MAAuB,EAAE6D,IAAsB;MAElE,IAAI9B,KAAK,CAACC,OAAO,CAAC6B,IAAI,CAAC,EAAE;QACvBA,IAAI,GAAGA,IAAI,CAAC,CAAC,CAAC;;MAEhB,IAAIC,UAA2B;MAC/B,IAAI,IAAI,CAACxC,eAAe,EAAE;QACxB,IAAI,IAAI,CAACJ,SAAS,IAAI,IAAI,EAAE;UAC1B4C,UAAU,GAAG,CAACD,IAAI,EAAEA,IAAI,CAAC;SAC1B,MAAM;UACLC,UAAU,GAAGD,IAAI;;OAEpB,MAAM;QACL,IAAI,IAAI,CAAC3C,SAAS,IAAI,IAAI,EAAE;UAC1B4C,UAAU,GAAG,CAAC,IAAI,EAAE,IAAI,CAAC;SAC1B,MAAM;UACLA,UAAU,GAAG,IAAI;;;MAGrB,IAAI,IAAI,CAACvC,WAAW,EAAE;QACpB,IAAMpB,MAAM,GAAG,IAAI,CAACW,YAAY,CAACX,MAAM;QACvC,IAAM4D,SAAS,GAAa5D,MAAM,CAAC0C,GAAG,CAAC,eAAK;UAAA,OAAI,IAAI;QAAA,EAAC;QACrD,IAAId,KAAK,CAACC,OAAO,CAAC8B,UAAU,CAAC,EAAE;UAC7B,OAAOA,UAAU,CAACpE,MAAM,CAACqE,SAAS,CAAC,CAACrE,MAAM,CAACqE,SAAS,CAAC;SACtD,MAAM;UACL,OAAO,CAACD,UAAU,CAAC,CAACpE,MAAM,CAACqE,SAAS,CAAC,CAACrE,MAAM,CAACqE,SAAS,CAAC;;OAE1D,MAAM;QACL,OAAOD,UAAU;;IAErB;EAAC;IAAA;IAAA,KAED,eAA6B;MAC3B,OAAO,IAAI,CAAChD,YAAY,CAAC9C,gBAAgB,CAAC0B,MAAM,CAC5C,IAAI,CAACsB,aAAa,CAAChD,gBAAgB,CAAC;IAC1C;EAAC;IAAA;IAAA,KAED,eAAgC;MAC9B,OAAO,IAAI,CAAC8C,YAAY,CAAC7C,mBAAmB,CAACyB,MAAM,CAC/C,IAAI,CAACsB,aAAa,CAAC/C,mBAAmB,CAAC;IAC7C;IAEA;EAAA;IAAA;IAAA,OAES,sCAA6BF,KAAc;MAClD,gGAAmCA,KAAK;MACxC,IAAI,IAAI,CAAC+C,YAAY,IAAI,IAAI,EAAE;QAC7B,IAAI,CAACA,YAAY,CAACjC,4BAA4B,CAACd,KAAK,CAAC;;MAEvD,IAAI,IAAI,CAACiD,aAAa,IAAI,IAAI,EAAE;QAC9B,IAAI,CAACA,aAAa,CAACnC,4BAA4B,CAACd,KAAK,CAAC;;IAE1D;EAAC;IAAA;IAAA,OAEQ,qBAAS;MAChB,IAAMQ,MAAM,GAA6B;QACvC,WAAW,EAAE,IAAI,CAAC2C;OACnB;MACD;MACA,IAAMxC,UAAU,+EAAoB;MACpCC,MAAM,CAACC,MAAM,CAACL,MAAM,EAAEG,UAAU,CAAC;MACjC,OAAOH,MAAM;IACf;IAEA;EAAA;IAAA;IAAA,OACA,oBACIO,GAA6C,EAC7CP,MAAgC;MAClC,IAAMyF,QAAQ,GACVxG,WAAW,CAACe,MAAM,CAAC,OAAO,CAA6B,CAAQ;MACnE,OAAOA,MAAM,CAAC,OAAO,CAAC;MACtB;MACA,IAAIA,MAAM,CAAC,cAAc,CAAC,IAAI,IAAI,EAAE;QAClC,MAAM,IAAIvB,mBAAmB,CACzB,+FAC+B,CAAC;;MAEtC;MACA,IAAMiC,SAAS,GAAyBV,MAAM;MAC9CU,SAAS,CAAC,OAAO,CAAC,GAAG+E,QAAQ;MAC7B,OAAO,IAAIlF,GAAG,CAACG,SAAS,CAAC;IAC3B;EAAC;EAAA;AAAA,EAhWgCxB,OAAO;AACxC;AACOmD,uBAAS,GAAG,eAAe;AAgWpCnE,aAAa,CAAC+D,aAAa,CAACI,aAAa,CAAC","names":["tfc","serialization","tidy","K","nameScope","InputSpec","Layer","SymbolicTensor","NotImplementedError","ValueError","VALID_BIDIRECTIONAL_MERGE_MODES","generic_utils","getExactlyOneShape","getExactlyOneTensor","rnn","standardizeArgs","deserialize","Wrapper","args","layer","inputShape","built","trainable","value","trainableWeights","nonTrainableWeights","_updates","losses","getWeights","weights","setWeights","config","getClassName","getConfig","baseConfig","Object","assign","setFastWeightInitDuringBuild","cls","customObjects","layerConfig","newConfig","TimeDistributed","supportsMasking","length","JSON","stringify","inputSpec","shape","childInputShape","concat","slice","build","childOutputShape","computeOutputShape","timesteps","inputs","kwargs","step","states","output","call","rnnOutputs","y","registerClass","checkBidirectionalMergeMode","checkStringTypeUnionValue","DEFAULT_BIDIRECTIONAL_MERGE_MODE","Bidirectional","forwDict","forwardLayer","backDict","backwardLayer","name","mergeMode","undefined","_stateful","stateful","returnSequences","returnState","_trainable","numConstants","numWeights","numeightsOver2","Math","floor","layerShapes","Array","isArray","outputShape","outputShapes","stateShape","singletonOrArray","initialState","constants","standardized","additionalInputs","additionalSpecs","numStates","push","stateSpecs","map","state","stateSpec","isSymbolicTensor","tensor","fullInput","fullInputSpec","originalInputSpec","yRev","forwardState","backwardState","reverse","concatenate","add","mul","resetStates","mask","outputMask","stateMask","rnnLayer"],"sources":["E:\\react-detect-toxicity-in-a-chat-app-youtube-2\\node_modules\\@tensorflow\\tfjs-layers\\src\\layers\\wrappers.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Layers that augment the functionality of a base layer.\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {serialization, Tensor, tidy} from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport {nameScope} from '../common';\nimport {InputSpec, Layer, LayerArgs, SymbolicTensor} from '../engine/topology';\nimport {NotImplementedError, ValueError} from '../errors';\nimport {BidirectionalMergeMode, Shape, VALID_BIDIRECTIONAL_MERGE_MODES} from '../keras_format/common';\nimport {Kwargs} from '../types';\nimport {RegularizerFn, RnnStepFunction} from '../types';\nimport * as generic_utils from '../utils/generic_utils';\nimport {getExactlyOneShape, getExactlyOneTensor} from '../utils/types_utils';\nimport {LayerVariable} from '../variables';\n\nimport {rnn, RNN, standardizeArgs} from './recurrent';\nimport {deserialize} from './serialization';\n\nexport declare interface WrapperLayerArgs extends LayerArgs {\n  /**\n   * The layer to be wrapped.\n   */\n  layer: Layer;\n}\n\n/**\n * Abstract wrapper base class.\n *\n * Wrappers take another layer and augment it in various ways.\n * Do not use this class as a layer, it is only an abstract base class.\n * Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n */\nexport abstract class Wrapper extends Layer {\n  readonly layer: Layer;\n\n  constructor(args: WrapperLayerArgs) {\n    // Porting Note: In PyKeras, `self.layer` is set prior to the calling\n    //   `super()`. But we can't do that here due to TypeScript's restriction.\n    //   See: https://github.com/Microsoft/TypeScript/issues/8277\n    //   As a result, we have to add checks in `get trainable()` and\n    //   `set trainable()` below in order to prevent using `this.layer` when\n    //   its value is `undefined`. The super constructor does use the getter\n    //   and the setter of `this.layer`.\n    super(args);\n    this.layer = args.layer;\n  }\n\n  override build(inputShape: Shape|Shape[]): void {\n    this.built = true;\n  }\n\n  // TODO(cais): Implement activityRegularizer getter.\n\n  override get trainable(): boolean {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    if (this.layer != null) {\n      return this.layer.trainable;\n    } else {\n      return false;\n    }\n  }\n\n  override set trainable(value: boolean) {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    if (this.layer != null) {\n      this.layer.trainable = value;\n    }\n  }\n\n  override get trainableWeights(): LayerVariable[] {\n    return this.layer.trainableWeights;\n  }\n  // TODO(cais): Implement setter for trainableWeights.\n\n  override get nonTrainableWeights(): LayerVariable[] {\n    return this.layer.nonTrainableWeights;\n  }\n  // TODO(cais): Implement setter for nonTrainableWeights.\n\n  override get updates(): Tensor[] {\n    // tslint:disable-next-line:no-any\n    return (this.layer as any)._updates;\n  }\n\n  // TODO(cais): Implement getUpdatesFor().\n\n  override get losses(): RegularizerFn[] {\n    return this.layer.losses;\n  }\n\n  // TODO(cais): Implement getLossesFor().\n\n  override getWeights(): Tensor[] {\n    return this.layer.getWeights();\n  }\n\n  override setWeights(weights: Tensor[]): void {\n    this.layer.setWeights(weights);\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      'layer': {\n        'className': this.layer.getClassName(),\n        'config': this.layer.getConfig(),\n      }\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  override setFastWeightInitDuringBuild(value: boolean) {\n    super.setFastWeightInitDuringBuild(value);\n    if (this.layer != null) {\n      this.layer.setFastWeightInitDuringBuild(value);\n    }\n  }\n\n  /** @nocollapse */\n  static override fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict,\n      customObjects = {} as serialization.ConfigDict): T {\n    const layerConfig = config['layer'] as serialization.ConfigDict;\n    const layer = deserialize(layerConfig, customObjects) as Layer;\n    delete config['layer'];\n    const newConfig = {layer};\n    Object.assign(newConfig, config);\n    return new cls(newConfig);\n  }\n}\n\nexport class TimeDistributed extends Wrapper {\n  /** @nocollapse */\n  static className = 'TimeDistributed';\n  constructor(args: WrapperLayerArgs) {\n    super(args);\n    this.supportsMasking = true;\n  }\n\n  override build(inputShape: Shape|Shape[]): void {\n    inputShape = getExactlyOneShape(inputShape);\n    if (inputShape.length < 3) {\n      throw new ValueError(\n          `TimeDistributed layer expects an input shape >= 3D, but received ` +\n          `input shape ${JSON.stringify(inputShape)}`);\n    }\n    this.inputSpec = [{shape: inputShape}];\n    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n    if (!this.layer.built) {\n      this.layer.build(childInputShape);\n      this.layer.built = true;\n    }\n    super.build(inputShape);\n  }\n\n  override computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    inputShape = getExactlyOneShape(inputShape);\n    const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n    const childOutputShape =\n        this.layer.computeOutputShape(childInputShape) as Shape;\n    const timesteps = inputShape[1];\n    return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      // TODO(cais): Add 'training' and 'useLearningPhase' to kwargs.\n      inputs = getExactlyOneTensor(inputs);\n      // Porting Note: In tfjs-layers, `inputs` are always concrete tensor\n      // values. Hence the inputs can't have an undetermined first (batch)\n      // dimension, which is why we always use the K.rnn approach here.\n      const step: RnnStepFunction = (inputs: Tensor, states: Tensor[]) => {\n        // TODO(cais): Add useLearningPhase.\n        // NOTE(cais): `layer.call` may return a length-1 array of Tensor in\n        //   some cases (e.g., `layer` is a `Sequential` instance), which is\n        //   why `getExactlyOneTensor` is used below.\n        const output = getExactlyOneTensor(this.layer.call(inputs, kwargs));\n        return [output, []];\n      };\n      const rnnOutputs =\n          rnn(step, inputs, [], false /* goBackwards */, null /* mask */,\n              null /* constants */, false /* unroll */,\n              true /* needPerStepOutputs */);\n      const y = rnnOutputs[1];\n      // TODO(cais): Add activity regularization.\n      // TODO(cais): Add useLearningPhase.\n      return y;\n    });\n  }\n\n  // TODO(cais): Implement detailed computeMask() logic.\n}\nserialization.registerClass(TimeDistributed);\n\nexport function checkBidirectionalMergeMode(value?: string): void {\n  generic_utils.checkStringTypeUnionValue(\n      VALID_BIDIRECTIONAL_MERGE_MODES, 'BidirectionalMergeMode', value);\n}\n\nexport declare interface BidirectionalLayerArgs extends WrapperLayerArgs {\n  /**\n   * The instance of an `RNN` layer to be wrapped.\n   */\n  layer: RNN;\n\n  /**\n   * Mode by which outputs of the forward and backward RNNs are\n   * combined. If `null` or `undefined`, the output will not be\n   * combined, they will be returned as an `Array`.\n   *\n   * If `undefined` (i.e., not provided), defaults to `'concat'`.\n   */\n  mergeMode?: BidirectionalMergeMode;\n}\n\nconst DEFAULT_BIDIRECTIONAL_MERGE_MODE: BidirectionalMergeMode = 'concat';\n\nexport class Bidirectional extends Wrapper {\n  /** @nocollapse */\n  static className = 'Bidirectional';\n  mergeMode: BidirectionalMergeMode;\n  private forwardLayer: RNN;\n  private backwardLayer: RNN;\n  private returnSequences: boolean;\n  private returnState: boolean;\n  private numConstants?: number;\n  private _trainable: boolean;\n\n  constructor(args: BidirectionalLayerArgs) {\n    super(args);\n\n    // Note: When creating `this.forwardLayer`, the original Layer object\n    //   (`config.layer`) ought to be cloned. This is why we call\n    //   `getConfig()` followed by `deserialize()`. Without this cloning,\n    //   the layer names saved during serialization will incorrectly contain\n    //   the 'forward_' prefix. In Python Keras, this is done using\n    //   `copy.copy` (shallow copy), which does not have a simple equivalent\n    //   in JavaScript. JavaScript's `Object.assign()` does not copy\n    //   methods.\n    const layerConfig = args.layer.getConfig();\n    const forwDict: serialization.ConfigDict = {};\n    forwDict['className'] = args.layer.getClassName();\n    forwDict['config'] = layerConfig;\n    this.forwardLayer = deserialize(forwDict) as RNN;\n    layerConfig['goBackwards'] =\n        layerConfig['goBackwards'] === true ? false : true;\n    const backDict: serialization.ConfigDict = {};\n    backDict['className'] = args.layer.getClassName();\n    backDict['config'] = layerConfig;\n    this.backwardLayer = deserialize(backDict) as RNN;\n    this.forwardLayer.name = 'forward_' + this.forwardLayer.name;\n    this.backwardLayer.name = 'backward_' + this.backwardLayer.name;\n\n    this.mergeMode = args.mergeMode === undefined ?\n        DEFAULT_BIDIRECTIONAL_MERGE_MODE :\n        args.mergeMode;\n    checkBidirectionalMergeMode(this.mergeMode);\n    if (args.weights) {\n      throw new NotImplementedError(\n          'weights support is not implemented for Bidirectional layer yet.');\n    }\n    this._stateful = args.layer.stateful;\n    this.returnSequences = args.layer.returnSequences;\n    this.returnState = args.layer.returnState;\n    this.supportsMasking = true;\n    this._trainable = true;\n    this.inputSpec = args.layer.inputSpec;\n    this.numConstants = null;\n  }\n\n  override get trainable(): boolean {\n    return this._trainable;\n  }\n\n  override set trainable(value: boolean) {\n    // Porting Note: the check of `this.layer` here is necessary due to the\n    //   way the `constructor` of this class is written (see Porting Note\n    //   above).\n    this._trainable = value;\n    if (this.forwardLayer != null) {\n      this.forwardLayer.trainable = value;\n    }\n    if (this.backwardLayer != null) {\n      this.backwardLayer.trainable = value;\n    }\n  }\n\n  override getWeights(): Tensor[] {\n    return this.forwardLayer.getWeights().concat(\n        this.backwardLayer.getWeights());\n  }\n\n  override setWeights(weights: Tensor[]): void {\n    const numWeights = weights.length;\n    const numeightsOver2 = Math.floor(numWeights / 2);\n    this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n    this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n  }\n\n  override computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    let layerShapes: Shape|Shape[] =\n        this.forwardLayer.computeOutputShape(inputShape);\n    if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n      layerShapes = [layerShapes as Shape];\n    }\n    layerShapes = layerShapes as Shape[];\n\n    let outputShape: Shape;\n    let outputShapes: Shape[];\n    let stateShape: Shape[];\n    if (this.returnState) {\n      stateShape = layerShapes.slice(1);\n      outputShape = layerShapes[0];\n    } else {\n      outputShape = layerShapes[0];\n    }\n    outputShape = outputShape;\n    if (this.mergeMode === 'concat') {\n      outputShape[outputShape.length - 1] *= 2;\n      outputShapes = [outputShape];\n    } else if (this.mergeMode == null) {\n      outputShapes = [outputShape, outputShape.slice()];\n    } else {\n      outputShapes = [outputShape];\n    }\n\n    if (this.returnState) {\n      if (this.mergeMode == null) {\n        return outputShapes.concat(stateShape).concat(stateShape.slice());\n      }\n      return [outputShape].concat(stateShape).concat(stateShape.slice());\n    }\n    return generic_utils.singletonOrArray(outputShapes);\n  }\n\n  override apply(\n      inputs: Tensor|Tensor[]|SymbolicTensor|SymbolicTensor[],\n      kwargs?: Kwargs): Tensor|Tensor[]|SymbolicTensor|SymbolicTensor[] {\n    let initialState: Tensor[]|SymbolicTensor[] =\n        kwargs == null ? null : kwargs['initialState'];\n    let constants: Tensor[]|SymbolicTensor[] =\n        kwargs == null ? null : kwargs['constants'];\n    if (kwargs == null) {\n      kwargs = {};\n    }\n    const standardized =\n        standardizeArgs(inputs, initialState, constants, this.numConstants);\n    inputs = standardized.inputs;\n    initialState = standardized.initialState;\n    constants = standardized.constants;\n\n    if (Array.isArray(inputs)) {\n      initialState = (inputs as Tensor[] | SymbolicTensor[]).slice(1);\n      inputs = (inputs as Tensor[] | SymbolicTensor[])[0];\n    }\n\n    if ((initialState == null || initialState.length === 0) &&\n        constants == null) {\n      return super.apply(inputs, kwargs);\n    }\n    const additionalInputs: Array<Tensor|SymbolicTensor> = [];\n    const additionalSpecs: InputSpec[] = [];\n    if (initialState != null) {\n      const numStates = initialState.length;\n      if (numStates % 2 > 0) {\n        throw new ValueError(\n            'When passing `initialState` to a Bidrectional RNN, ' +\n            'the state should be an Array containing the states of ' +\n            'the underlying RNNs.');\n      }\n      kwargs['initialState'] = initialState;\n      additionalInputs.push(...initialState);\n      const stateSpecs = (initialState as Array<Tensor|SymbolicTensor>)\n                             .map(state => new InputSpec({shape: state.shape}));\n      this.forwardLayer.stateSpec = stateSpecs.slice(0, numStates / 2);\n      this.backwardLayer.stateSpec = stateSpecs.slice(numStates / 2);\n      additionalSpecs.push(...stateSpecs);\n    }\n    if (constants != null) {\n      throw new NotImplementedError(\n          'Support for constants in Bidirectional layers is not ' +\n          'implemented yet.');\n    }\n\n    const isSymbolicTensor = additionalInputs[0] instanceof SymbolicTensor;\n    for (const tensor of additionalInputs) {\n      if (tensor instanceof SymbolicTensor !== isSymbolicTensor) {\n        throw new ValueError(\n            'The initial state of a Bidirectional layer cannot be ' +\n            'specified as a mix of symbolic and non-symbolic tensors');\n      }\n    }\n\n    if (isSymbolicTensor) {\n      // Compute the full input and specs, including the states.\n      const fullInput = [inputs].concat(additionalInputs);\n      const fullInputSpec = this.inputSpec.concat(additionalSpecs);\n      // Perform the call temporarily and replace inputSpec.\n      // Note: with initial states symbolic calls and non-symbolic calls to\n      // this method differ in how the initial states are passed. For\n      // symbolic calls, the initial states are passed in the first arg, as\n      // an Array of SymbolicTensors; for non-symbolic calls, they are\n      // passed in the second arg as a part of the kwargs. Hence the need to\n      // temporarily modify inputSpec here.\n      // TODO(cais): Make refactoring so that this hacky code below is no\n      // longer needed.\n      const originalInputSpec = this.inputSpec;\n      this.inputSpec = fullInputSpec;\n      const output =\n          super.apply(fullInput as Tensor[] | SymbolicTensor[], kwargs);\n      this.inputSpec = originalInputSpec;\n      return output;\n    } else {\n      return super.apply(inputs, kwargs);\n    }\n  }\n\n  override call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      const initialState = kwargs['initialState'];\n\n      let y: Tensor|Tensor[];\n      let yRev: Tensor|Tensor[];\n      if (initialState == null) {\n        y = this.forwardLayer.call(inputs, kwargs);\n        yRev = this.backwardLayer.call(inputs, kwargs);\n      } else {\n        const forwardState = initialState.slice(0, initialState.length / 2);\n        const backwardState = initialState.slice(initialState.length / 2);\n        y = this.forwardLayer.call(\n            inputs, Object.assign(kwargs, {initialState: forwardState}));\n        yRev = this.backwardLayer.call(\n            inputs, Object.assign(kwargs, {initialState: backwardState}));\n      }\n\n      let states: Tensor[];\n      if (this.returnState) {\n        if (Array.isArray(y)) {\n          states = y.slice(1).concat((yRev as Tensor[]).slice(1));\n        } else {\n        }\n        y = (y as Tensor[])[0];\n        yRev = (yRev as Tensor[])[0];\n      }\n\n      if (this.returnSequences) {\n        yRev = tfc.reverse(yRev as Tensor, 1);\n      }\n\n      let output: Tensor|Tensor[];\n      if (this.mergeMode === 'concat') {\n        output = K.concatenate([y as Tensor, yRev as Tensor]);\n      } else if (this.mergeMode === 'sum') {\n        output = tfc.add(y as Tensor, yRev as Tensor);\n      } else if (this.mergeMode === 'ave') {\n        output = tfc.mul(.5, tfc.add(y as Tensor, yRev as Tensor));\n      } else if (this.mergeMode === 'mul') {\n        output = tfc.mul(y as Tensor, yRev as Tensor);\n      } else if (this.mergeMode == null) {\n        output = [y as Tensor, yRev as Tensor];\n      }\n\n      // TODO(cais): Properly set learning phase.\n      if (this.returnState) {\n        if (this.mergeMode == null) {\n          return (output as Tensor[]).concat(states);\n        }\n        return [output as Tensor].concat(states);\n      }\n      return output;\n    });\n  }\n\n  override resetStates(states?: Tensor|Tensor[]): void {\n    this.forwardLayer.resetStates();\n    this.backwardLayer.resetStates();\n  }\n\n  override build(inputShape: Shape|Shape[]): void {\n    nameScope(this.forwardLayer.name, () => {\n      this.forwardLayer.build(inputShape);\n    });\n    nameScope(this.backwardLayer.name, () => {\n      this.backwardLayer.build(inputShape);\n    });\n    this.built = true;\n  }\n\n  override computeMask(inputs: Tensor|Tensor[], mask?: Tensor|Tensor[]): Tensor\n      |Tensor[] {\n    if (Array.isArray(mask)) {\n      mask = mask[0];\n    }\n    let outputMask: Tensor|Tensor[];\n    if (this.returnSequences) {\n      if (this.mergeMode == null) {\n        outputMask = [mask, mask];\n      } else {\n        outputMask = mask;\n      }\n    } else {\n      if (this.mergeMode == null) {\n        outputMask = [null, null];\n      } else {\n        outputMask = null;\n      }\n    }\n    if (this.returnState) {\n      const states = this.forwardLayer.states;\n      const stateMask: Tensor[] = states.map(state => null);\n      if (Array.isArray(outputMask)) {\n        return outputMask.concat(stateMask).concat(stateMask);\n      } else {\n        return [outputMask].concat(stateMask).concat(stateMask);\n      }\n    } else {\n      return outputMask;\n    }\n  }\n\n  override get trainableWeights(): LayerVariable[] {\n    return this.forwardLayer.trainableWeights.concat(\n        this.backwardLayer.trainableWeights);\n  }\n\n  override get nonTrainableWeights(): LayerVariable[] {\n    return this.forwardLayer.nonTrainableWeights.concat(\n        this.backwardLayer.nonTrainableWeights);\n  }\n\n  // TODO(cais): Implement constraints().\n\n  override setFastWeightInitDuringBuild(value: boolean) {\n    super.setFastWeightInitDuringBuild(value);\n    if (this.forwardLayer != null) {\n      this.forwardLayer.setFastWeightInitDuringBuild(value);\n    }\n    if (this.backwardLayer != null) {\n      this.backwardLayer.setFastWeightInitDuringBuild(value);\n    }\n  }\n\n  override getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      'mergeMode': this.mergeMode,\n    };\n    // TODO(cais): Add logic for `numConstants` once the property is added.\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  /** @nocollapse */\n  static override fromConfig<T extends serialization.Serializable>(\n      cls: serialization.SerializableConstructor<T>,\n      config: serialization.ConfigDict): T {\n    const rnnLayer =\n        deserialize(config['layer'] as serialization.ConfigDict) as RNN;\n    delete config['layer'];\n    // TODO(cais): Add logic for `numConstants` once the property is added.\n    if (config['numConstants'] != null) {\n      throw new NotImplementedError(\n          `Deserialization of a Bidirectional layer with numConstants ` +\n          `present is not supported yet.`);\n    }\n    // tslint:disable-next-line:no-any\n    const newConfig: {[key: string]: any} = config;\n    newConfig['layer'] = rnnLayer;\n    return new cls(newConfig);\n  }\n}\nserialization.registerClass(Bidirectional);\n"]},"metadata":{},"sourceType":"module","externalDependencies":[]}