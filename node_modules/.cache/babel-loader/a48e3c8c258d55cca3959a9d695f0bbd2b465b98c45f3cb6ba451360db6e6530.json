{"ast":null,"code":"import _createForOfIteratorHelper from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createForOfIteratorHelper.js\";\nimport _classCallCheck from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";\nimport _createClass from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createClass.js\";\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Executor: Evaluates SymbolicTensor based on feeds.\n */\nimport { cast, dispose, memory, util } from '@tensorflow/tfjs-core';\nimport { ValueError } from '../errors';\nimport { LruCache } from '../utils/executor_utils';\nimport { toList } from '../utils/generic_utils';\nimport { InputLayer } from './input_layer';\nimport { SymbolicTensor } from './topology';\n/**\n * Helper function to check the dtype and shape compatibility of a feed value.\n */\nfunction assertFeedCompatibility(key, val) {\n  // Check dtype compatibility.\n  if (key.dtype == null || key.dtype === val.dtype) {\n    //  a.  If types match, return val tensor as is.\n    return val;\n  }\n  try {\n    //  b. Attempt to convert to expected type.\n    return cast(val, key.dtype);\n  } catch (err) {\n    //  c. If conversion fails, return helpful error.\n    throw new ValueError(\"The dtype of the feed (\".concat(val.dtype, \") can not be cast to the dtype \") + \"of the key '\".concat(key.name, \"' (\").concat(key.dtype, \").\"));\n  }\n}\n/**\n * FeedDict: A mapping from unique SymbolicTensors to feed values for them.\n * A feed value is a concrete value represented as an `Tensor`.\n */\nexport var FeedDict = /*#__PURE__*/function () {\n  /**\n   * Constructor, optionally does copy-construction.\n   * @param feeds An Array of `Feed`s, or another `FeedDict`, in which case\n   *   copy-construction will be performed.\n   */\n  function FeedDict(feeds) {\n    _classCallCheck(this, FeedDict);\n    this.id2Value = {};\n    this.id2Mask = {};\n    this.name2Id = {};\n    if (feeds instanceof FeedDict) {\n      for (var id in feeds.id2Value) {\n        this.id2Value[id] = feeds.id2Value[id];\n        if (id in feeds.id2Mask) {\n          this.id2Mask[id] = feeds.id2Mask[id];\n        }\n      }\n    } else {\n      if (feeds == null) {\n        return;\n      }\n      var _iterator = _createForOfIteratorHelper(feeds),\n        _step;\n      try {\n        for (_iterator.s(); !(_step = _iterator.n()).done;) {\n          var feed = _step.value;\n          this.add(feed.key, feed.value);\n        }\n      } catch (err) {\n        _iterator.e(err);\n      } finally {\n        _iterator.f();\n      }\n    }\n  }\n  /**\n   * Add a key-value pair to the FeedDict.\n   *\n   * @param key The key of the feed.\n   * @param value The value of the tensor feed.\n   * @param mask The value of the mask feed (optional).\n   * @returns This `FeedDict`.\n   * @throws ValueError: If the key `SymbolicTensor` already exists in the\n   *   `FeedDict`.\n   */\n  _createClass(FeedDict, [{\n    key: \"add\",\n    value: function add(key, value, mask) {\n      if (this.id2Value[key.id] == null) {\n        this.id2Value[key.id] = assertFeedCompatibility(key, value);\n        this.name2Id[key.name] = key.id;\n        if (mask != null) {\n          this.id2Mask[key.id] = mask;\n        }\n      } else {\n        throw new ValueError(\"Duplicate key: name=\".concat(key.name, \", id=\").concat(key.id));\n      }\n      return this;\n    }\n    /**\n     * Add a Feed to the FeedDict.\n     * @param feed The new `Feed` to add.\n     * @returns This `FeedDict`.\n     */\n  }, {\n    key: \"addFeed\",\n    value: function addFeed(feed) {\n      this.add(feed.key, feed.value);\n    }\n    /**\n     * Probe whether a key already exists in the FeedDict.\n     * @param key\n     */\n  }, {\n    key: \"hasKey\",\n    value: function hasKey(key) {\n      return this.id2Value[key.id] != null;\n    }\n    /**\n     * Get all the SymbolicTensor available in this FeedDict.\n     */\n  }, {\n    key: \"names\",\n    value: function names() {\n      return Object.keys(this.name2Id);\n    }\n    /**\n     * Get the feed value for given key.\n     * @param key The SymbolicTensor, or its name (as a string), of which the\n     *     value is sought.\n     * @returns If `key` exists, the corresponding feed value.\n     * @throws ValueError: If `key` does not exist in this `FeedDict`.\n     */\n  }, {\n    key: \"getValue\",\n    value: function getValue(key) {\n      if (key instanceof SymbolicTensor) {\n        if (this.id2Value[key.id] == null) {\n          throw new ValueError(\"Nonexistent key: \".concat(key.name));\n        } else {\n          return this.id2Value[key.id];\n        }\n      } else {\n        var id = this.name2Id[key];\n        if (id == null) {\n          throw new ValueError(\"Feed dict has no SymbolicTensor name: \".concat(key));\n        }\n        return this.id2Value[id];\n      }\n    }\n    /**\n     * Get the feed mask for given key.\n     * @param key The SymbolicTensor, or its name (as a string), of which the\n     *     value is sought.\n     * @returns If `key` exists, the corresponding feed mask.\n     * @throws ValueError: If `key` does not exist in this `FeedDict`.\n     */\n  }, {\n    key: \"getMask\",\n    value: function getMask(key) {\n      if (key instanceof SymbolicTensor) {\n        if (this.id2Value[key.id] == null) {\n          throw new ValueError(\"Nonexistent key: \".concat(key.name));\n        } else {\n          return this.id2Mask[key.id];\n        }\n      } else {\n        var id = this.name2Id[key];\n        if (id == null) {\n          throw new ValueError(\"Feed dict has no SymbolicTensor name: \".concat(key));\n        }\n        return this.id2Mask[id];\n      }\n    }\n    /** Dispose all mask Tensors held by this object. */\n  }, {\n    key: \"disposeMasks\",\n    value: function disposeMasks() {\n      if (this.id2Mask != null) {\n        dispose(this.id2Mask);\n      }\n    }\n  }]);\n  return FeedDict;\n}();\n// Cache for topologically sorted SymbolicTensors for given execution\n// targets (i.e., fetches).\nexport var cachedSorted = new LruCache();\n// Cache for recipient count maps for given execution targets (i.e., fetches).\nexport var cachedRecipientCounts = new LruCache();\nexport function updateCacheMaxEntries(maxEntries) {\n  if (cachedSorted != null) {\n    cachedSorted.setMaxEntries(maxEntries);\n  }\n  if (cachedRecipientCounts != null) {\n    cachedRecipientCounts.setMaxEntries(maxEntries);\n  }\n}\n/**\n * Execute a SymbolicTensor by using concrete feed values.\n *\n * A `SymbolicTensor` object is a node in a computation graph of TF.js\n * Layers. The object is backed by a source layer and input\n * `SymbolicTensor`s to the source layer. This method evaluates\n * the `call()` method of the source layer, using concrete values of the\n * inputs obtained from either\n * * `feedDict`, if the input key exists in `feedDict`, or else,\n * * a recursive call to `execute()` itself.\n *\n * @param x: The `SymbolicTensor` to execute.\n * @param feedDict: The feed values, as base condition of the recursion.\n *   execution.\n * @param kwargs: Optional keyword arguments.\n * @param probe: A probe object (of interface `ExecutionProbe`) used for\n *   testing memory footprint of `execute` calls.\n * @returns Result of the execution.\n * @throws ValueError: If any `SymbolicTensor`s from `InputLayer`s\n *   encountered during the execution lacks a feed value in `feedDict`.\n */\nexport function execute(fetches, feedDict, kwargs, probe) {\n  var training = kwargs == null ? false : kwargs['training'];\n  var arrayFetches = Array.isArray(fetches);\n  var fetchArray = arrayFetches ? fetches : [fetches];\n  var outputNames = fetchArray.map(function (t) {\n    return t.name;\n  });\n  var finalOutputs = [];\n  var feedNames = feedDict.names();\n  var _iterator2 = _createForOfIteratorHelper(outputNames),\n    _step2;\n  try {\n    for (_iterator2.s(); !(_step2 = _iterator2.n()).done;) {\n      var outputName = _step2.value;\n      if (feedNames.indexOf(outputName) !== -1) {\n        finalOutputs.push(feedDict.getValue(outputName));\n      } else {\n        finalOutputs.push(null);\n      }\n    }\n  } catch (err) {\n    _iterator2.e(err);\n  } finally {\n    _iterator2.f();\n  }\n  if (probe != null) {\n    // For optional probing of memory footprint during execution.\n    probe.maxNumTensors = -Infinity;\n    probe.minNumTensors = Infinity;\n  }\n  // Check cache.\n  var fetchAndFeedKey = outputNames.join(',') + '|' + feedDict.names().sort().join(',');\n  var sorted = cachedSorted.get(fetchAndFeedKey);\n  var recipientCounts;\n  if (sorted == null) {\n    // Cache doesn't contain the desired combination of fetches. Compute\n    // topological sort for the combination for the first time.\n    var out = getTopologicalSortAndRecipientCounts(fetchArray, feedDict);\n    sorted = out.sorted;\n    recipientCounts = out.recipientCounts;\n    // Store results in cache for future use.\n    cachedSorted.put(fetchAndFeedKey, sorted);\n    cachedRecipientCounts.put(fetchAndFeedKey, recipientCounts);\n  }\n  recipientCounts = {};\n  if (!training) {\n    Object.assign(recipientCounts, cachedRecipientCounts.get(fetchAndFeedKey));\n  }\n  var internalFeedDict = new FeedDict(feedDict);\n  // Start iterative execution on the topologically-sorted SymbolicTensors.\n  for (var i = 0; i < sorted.length; ++i) {\n    if (probe != null) {\n      // For optional probing of memory usage during execution.\n      var numTensors = memory().numTensors;\n      if (numTensors > probe.maxNumTensors) {\n        probe.maxNumTensors = numTensors;\n      }\n      if (numTensors < probe.minNumTensors) {\n        probe.minNumTensors = numTensors;\n      }\n    }\n    var symbolic = sorted[i];\n    var srcLayer = symbolic.sourceLayer;\n    if (srcLayer instanceof InputLayer) {\n      continue;\n    }\n    var inputValues = [];\n    var inputMasks = [];\n    var tensorsToDispose = [];\n    var maskExists = false;\n    var _iterator3 = _createForOfIteratorHelper(symbolic.inputs),\n      _step3;\n    try {\n      for (_iterator3.s(); !(_step3 = _iterator3.n()).done;) {\n        var input = _step3.value;\n        var value = internalFeedDict.getValue(input);\n        var mask = internalFeedDict.getMask(input);\n        inputValues.push(value);\n        inputMasks.push(mask);\n        if (mask != null) {\n          maskExists = true;\n        }\n        if (!training) {\n          recipientCounts[input.name]--;\n          if (recipientCounts[input.name] === 0 && !feedDict.hasKey(input) && outputNames.indexOf(input.name) === -1 && !value.isDisposed && input.sourceLayer.stateful !== true) {\n            tensorsToDispose.push(value);\n          }\n        }\n      }\n    } catch (err) {\n      _iterator3.e(err);\n    } finally {\n      _iterator3.f();\n    }\n    if (maskExists) {\n      kwargs = kwargs || {};\n      kwargs['mask'] = inputMasks[0];\n    }\n    var outputTensors = toList(srcLayer.apply(inputValues, kwargs));\n    var outputMask = null;\n    if (srcLayer.supportsMasking) {\n      outputMask = srcLayer.computeMask(inputValues, inputMasks);\n    }\n    var layerOutputs = getNodeOutputs(symbolic);\n    var outputSymbolicTensors = Array.isArray(layerOutputs) ? layerOutputs : [layerOutputs];\n    for (var _i = 0; _i < outputSymbolicTensors.length; ++_i) {\n      if (!internalFeedDict.hasKey(outputSymbolicTensors[_i])) {\n        internalFeedDict.add(outputSymbolicTensors[_i], outputTensors[_i], Array.isArray(outputMask) ? outputMask[0] : outputMask);\n      }\n      var index = outputNames.indexOf(outputSymbolicTensors[_i].name);\n      if (index !== -1) {\n        finalOutputs[index] = outputTensors[_i];\n      }\n    }\n    if (!training) {\n      // Clean up Tensors that are no longer needed.\n      dispose(tensorsToDispose);\n    }\n  }\n  // NOTE(cais): Unlike intermediate tensors, we don't discard mask\n  // tensors as we go, because these tensors are sometimes passed over a\n  // series of mutliple layers, i.e., not obeying the immediate input\n  // relations in the graph. If this becomes a memory-usage concern,\n  // we can improve this in the future.\n  internalFeedDict.disposeMasks();\n  return arrayFetches ? finalOutputs : finalOutputs[0];\n}\n/**\n * Sort the `SymbolicTensor`s topologically, for an array of fetches.\n *\n * This function calls getTopologicalSortAndRecipientCountsForOneFetch and\n * merges their results.\n *\n * @param fetch The array of fetches requested. Must be a non-empty array.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientCounts: Recipient counts for all SymbolicTensors in `sorted`.\n */\nfunction getTopologicalSortAndRecipientCounts(fetches, feedDict) {\n  util.assert(fetches != null && fetches.length > 0, function () {\n    return \"Expected at least one fetch, got none\";\n  });\n  var finalSorted = [];\n  var finalRecipientMap = {};\n  if (fetches.length === 1) {\n    // Special-casing 1 fetch for efficiency.\n    var out = getTopologicalSortAndRecipientCountsForOneFetch(fetches[0], feedDict);\n    finalSorted = out.sorted;\n    finalRecipientMap = out.recipientMap;\n  } else {\n    var visited = new Set();\n    var _iterator4 = _createForOfIteratorHelper(fetches),\n      _step4;\n    try {\n      for (_iterator4.s(); !(_step4 = _iterator4.n()).done;) {\n        var fetch = _step4.value;\n        var _getTopologicalSortAn = getTopologicalSortAndRecipientCountsForOneFetch(fetch, feedDict),\n          sorted = _getTopologicalSortAn.sorted,\n          recipientMap = _getTopologicalSortAn.recipientMap;\n        // Merge sorted SymbolicTensor Arrays.\n        var _iterator5 = _createForOfIteratorHelper(sorted),\n          _step5;\n        try {\n          for (_iterator5.s(); !(_step5 = _iterator5.n()).done;) {\n            var symbolicTensor = _step5.value;\n            if (!visited.has(symbolicTensor.name)) {\n              finalSorted.push(symbolicTensor);\n              visited.add(symbolicTensor.name);\n            }\n          }\n          // Merge recipient maps.\n        } catch (err) {\n          _iterator5.e(err);\n        } finally {\n          _iterator5.f();\n        }\n        var _loop = function _loop(name) {\n          if (finalRecipientMap[name] == null) {\n            finalRecipientMap[name] = new Set();\n          }\n          recipientMap[name].forEach(function (recipient) {\n            return finalRecipientMap[name].add(recipient);\n          });\n        };\n        for (var name in recipientMap) {\n          _loop(name);\n        }\n      }\n    } catch (err) {\n      _iterator4.e(err);\n    } finally {\n      _iterator4.f();\n    }\n  }\n  return {\n    sorted: finalSorted,\n    recipientCounts: recipientMap2Counts(finalRecipientMap)\n  };\n}\nfunction recipientMap2Counts(recipientMap) {\n  var recipientCounts = {};\n  for (var name in recipientMap) {\n    recipientCounts[name] = recipientMap[name].size;\n  }\n  return recipientCounts;\n}\n/**\n * Sort the `SymbolicTensor`s topologically, for a single fetch.\n *\n * This helper function processes the upstream SymbolicTensors of a single\n * fetch.\n *\n * @param fetch The single fetch requested.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientMap: Recipient names for all SymbolicTensors in `sorted`.\n */\nexport function getTopologicalSortAndRecipientCountsForOneFetch(fetch, feedDict) {\n  var visited = new Set();\n  var sorted = [];\n  var recipientMap = {};\n  // Put keys of the feedDict into visited first, so they don't have to be\n  // walked. This is needed in case where there are feeds for intermediate\n  // SymbolicTensors of the graph.\n  var _iterator6 = _createForOfIteratorHelper(feedDict.names()),\n    _step6;\n  try {\n    for (_iterator6.s(); !(_step6 = _iterator6.n()).done;) {\n      var key = _step6.value;\n      visited.add(key);\n    }\n  } catch (err) {\n    _iterator6.e(err);\n  } finally {\n    _iterator6.f();\n  }\n  var stack = [];\n  var marks = [];\n  // Initial population of stack and marks.\n  stack.push(fetch);\n  while (stack.length > 0) {\n    var top = stack[stack.length - 1];\n    if (visited.has(top.name)) {\n      stack.pop();\n      continue;\n    }\n    var topIsMarked = marks[marks.length - 1] === stack.length - 1;\n    if (top.inputs.length === 0 || topIsMarked) {\n      // Input SymbolicTensor or all children have been visited.\n      stack.pop();\n      sorted.push(top);\n      visited.add(top.name);\n      if (topIsMarked) {\n        marks.pop();\n      }\n    } else {\n      // A non-input SymbolicTensor whose upstream SymbolicTensors haven't\n      // been visited yet. Push them onto the stack.\n      marks.push(stack.length - 1);\n      var _iterator7 = _createForOfIteratorHelper(top.inputs),\n        _step7;\n      try {\n        for (_iterator7.s(); !(_step7 = _iterator7.n()).done;) {\n          var input = _step7.value;\n          // Increment the recipient count. Note that this needs to happen\n          // regardless of whether the SymbolicTensor has been visited before.\n          if (recipientMap[input.name] == null) {\n            recipientMap[input.name] = new Set();\n          }\n          recipientMap[input.name].add(top.name);\n          if (visited.has(input.name)) {\n            continue; // Avoid repeated visits to the same SymbolicTensor.\n          }\n\n          stack.push(input);\n        }\n      } catch (err) {\n        _iterator7.e(err);\n      } finally {\n        _iterator7.f();\n      }\n    }\n  }\n  return {\n    sorted: sorted,\n    recipientMap: recipientMap\n  };\n}\n/**\n * Get the symbolic output tensors of the node to which a given fetch belongs.\n * @param fetch The fetched symbolic tensor.\n * @returns The Array of symbolic tensors output by the node to which `fetch`\n *   belongs.\n */\nfunction getNodeOutputs(fetch) {\n  var layerOutputs;\n  if (fetch.sourceLayer.inboundNodes.length === 1) {\n    layerOutputs = fetch.sourceLayer.output;\n  } else {\n    var nodeIndex = null;\n    for (var i = 0; i < fetch.sourceLayer.inboundNodes.length; ++i) {\n      var _iterator8 = _createForOfIteratorHelper(fetch.sourceLayer.inboundNodes[i].outputTensors),\n        _step8;\n      try {\n        for (_iterator8.s(); !(_step8 = _iterator8.n()).done;) {\n          var outputTensor = _step8.value;\n          if (outputTensor.id === fetch.id) {\n            nodeIndex = i;\n            break;\n          }\n        }\n      } catch (err) {\n        _iterator8.e(err);\n      } finally {\n        _iterator8.f();\n      }\n    }\n    layerOutputs = fetch.sourceLayer.getOutputAt(nodeIndex);\n  }\n  return layerOutputs;\n}","map":{"version":3,"mappings":";;;AAAA;;;;;;;;;AAUA;;;AAIA,SAAQA,IAAI,EAAEC,OAAO,EAAEC,MAAM,EAAUC,IAAI,QAAO,uBAAuB;AAEzE,SAAQC,UAAU,QAAO,WAAW;AAEpC,SAAQC,QAAQ,QAAO,yBAAyB;AAChD,SAAQC,MAAM,QAAO,wBAAwB;AAE7C,SAAQC,UAAU,QAAO,eAAe;AACxC,SAAQC,cAAc,QAAO,YAAY;AAEzC;;;AAGA,SAASC,uBAAuB,CAACC,GAAmB,EAAEC,GAAW;EAC/D;EACA,IAAID,GAAG,CAACE,KAAK,IAAI,IAAI,IAAIF,GAAG,CAACE,KAAK,KAAKD,GAAG,CAACC,KAAK,EAAE;IAChD;IACA,OAAOD,GAAG;;EAEZ,IAAI;IACF;IACA,OAAOX,IAAI,CAACW,GAAG,EAAED,GAAG,CAACE,KAAK,CAAC;GAC5B,CAAC,OAAOC,GAAG,EAAE;IACZ;IACA,MAAM,IAAIT,UAAU,CAChB,iCAA0BO,GAAG,CAACC,KAAK,6DACpBF,GAAG,CAACI,IAAI,gBAAMJ,GAAG,CAACE,KAAK,OAAI,CAAC;;AAEnD;AAUA;;;;AAIA,WAAaG,QAAQ;EAKnB;;;;;EAKA,kBAAYC,KAAuB;IAAA;IAT3B,aAAQ,GAA2B,EAAE;IACrC,YAAO,GAA2B,EAAE;IACpC,YAAO,GAA6B,EAAE;IAQ5C,IAAIA,KAAK,YAAYD,QAAQ,EAAE;MAC7B,KAAK,IAAME,EAAE,IAAID,KAAK,CAACE,QAAQ,EAAE;QAC/B,IAAI,CAACA,QAAQ,CAACD,EAAE,CAAC,GAAGD,KAAK,CAACE,QAAQ,CAACD,EAAE,CAAC;QACtC,IAAIA,EAAE,IAAID,KAAK,CAACG,OAAO,EAAE;UACvB,IAAI,CAACA,OAAO,CAACF,EAAE,CAAC,GAAGD,KAAK,CAACG,OAAO,CAACF,EAAE,CAAC;;;KAGzC,MAAM;MACL,IAAID,KAAK,IAAI,IAAI,EAAE;QACjB;;MACD,2CACkBA,KAAK;QAAA;MAAA;QAAxB,oDAA0B;UAAA,IAAfI,IAAI;UACb,IAAI,CAACC,GAAG,CAACD,IAAI,CAACV,GAAG,EAAEU,IAAI,CAACE,KAAK,CAAC;;MAC/B;QAAA;MAAA;QAAA;MAAA;;EAEL;EAEA;;;;;;;;;;EAAA;IAAA;IAAA,OAUA,aAAIZ,GAAmB,EAAEY,KAAa,EAAEC,IAAa;MACnD,IAAI,IAAI,CAACL,QAAQ,CAACR,GAAG,CAACO,EAAE,CAAC,IAAI,IAAI,EAAE;QACjC,IAAI,CAACC,QAAQ,CAACR,GAAG,CAACO,EAAE,CAAC,GAAGR,uBAAuB,CAACC,GAAG,EAAEY,KAAK,CAAC;QAC3D,IAAI,CAACE,OAAO,CAACd,GAAG,CAACI,IAAI,CAAC,GAAGJ,GAAG,CAACO,EAAE;QAC/B,IAAIM,IAAI,IAAI,IAAI,EAAE;UAChB,IAAI,CAACJ,OAAO,CAACT,GAAG,CAACO,EAAE,CAAC,GAAGM,IAAI;;OAE9B,MAAM;QACL,MAAM,IAAInB,UAAU,+BAAwBM,GAAG,CAACI,IAAI,kBAAQJ,GAAG,CAACO,EAAE,EAAG;;MAEvE,OAAO,IAAI;IACb;IAEA;;;;;EAAA;IAAA;IAAA,OAKA,iBAAQG,IAAU;MAChB,IAAI,CAACC,GAAG,CAACD,IAAI,CAACV,GAAG,EAAEU,IAAI,CAACE,KAAK,CAAC;IAChC;IAEA;;;;EAAA;IAAA;IAAA,OAIA,gBAAOZ,GAAmB;MACxB,OAAO,IAAI,CAACQ,QAAQ,CAACR,GAAG,CAACO,EAAE,CAAC,IAAI,IAAI;IACtC;IAEA;;;EAAA;IAAA;IAAA,OAGA,iBAAK;MACH,OAAOQ,MAAM,CAACC,IAAI,CAAC,IAAI,CAACF,OAAO,CAAC;IAClC;IAEA;;;;;;;EAAA;IAAA;IAAA,OAOA,kBAASd,GAA0B;MACjC,IAAIA,GAAG,YAAYF,cAAc,EAAE;QACjC,IAAI,IAAI,CAACU,QAAQ,CAACR,GAAG,CAACO,EAAE,CAAC,IAAI,IAAI,EAAE;UACjC,MAAM,IAAIb,UAAU,4BAAqBM,GAAG,CAACI,IAAI,EAAG;SACrD,MAAM;UACL,OAAO,IAAI,CAACI,QAAQ,CAACR,GAAG,CAACO,EAAE,CAAC;;OAE/B,MAAM;QACL,IAAMA,EAAE,GAAG,IAAI,CAACO,OAAO,CAACd,GAAG,CAAC;QAC5B,IAAIO,EAAE,IAAI,IAAI,EAAE;UACd,MAAM,IAAIb,UAAU,iDAA0CM,GAAG,EAAG;;QAEtE,OAAO,IAAI,CAACQ,QAAQ,CAACD,EAAE,CAAC;;IAE5B;IAEA;;;;;;;EAAA;IAAA;IAAA,OAOA,iBAAQP,GAA0B;MAChC,IAAIA,GAAG,YAAYF,cAAc,EAAE;QACjC,IAAI,IAAI,CAACU,QAAQ,CAACR,GAAG,CAACO,EAAE,CAAC,IAAI,IAAI,EAAE;UACjC,MAAM,IAAIb,UAAU,4BAAqBM,GAAG,CAACI,IAAI,EAAG;SACrD,MAAM;UACL,OAAO,IAAI,CAACK,OAAO,CAACT,GAAG,CAACO,EAAE,CAAC;;OAE9B,MAAM;QACL,IAAMA,EAAE,GAAG,IAAI,CAACO,OAAO,CAACd,GAAG,CAAC;QAC5B,IAAIO,EAAE,IAAI,IAAI,EAAE;UACd,MAAM,IAAIb,UAAU,iDAA0CM,GAAG,EAAG;;QAEtE,OAAO,IAAI,CAACS,OAAO,CAACF,EAAE,CAAC;;IAE3B;IAEA;EAAA;IAAA;IAAA,OACA,wBAAY;MACV,IAAI,IAAI,CAACE,OAAO,IAAI,IAAI,EAAE;QACxBlB,OAAO,CAAC,IAAI,CAACkB,OAAO,CAAC;;IAEzB;EAAC;EAAA;AAAA;AAGH;AACA;AACA,OAAO,IAAMQ,YAAY,GACrB,IAAItB,QAAQ,EAAoB;AAEpC;AACA,OAAO,IAAMuB,qBAAqB,GAC9B,IAAIvB,QAAQ,EAAmB;AAEnC,OAAM,SAAUwB,qBAAqB,CAACC,UAAkB;EACtD,IAAIH,YAAY,IAAI,IAAI,EAAE;IACxBA,YAAY,CAACI,aAAa,CAACD,UAAU,CAAC;;EAExC,IAAIF,qBAAqB,IAAI,IAAI,EAAE;IACjCA,qBAAqB,CAACG,aAAa,CAACD,UAAU,CAAC;;AAEnD;AAsBA;;;;;;;;;;;;;;;;;;;;;AAqBA,OAAM,SAAUE,OAAO,CACnBC,OAAwC,EAAEC,QAAkB,EAC5DC,MAAe,EAAEC,KAAsB;EAEzC,IAAMC,QAAQ,GAAYF,MAAM,IAAI,IAAI,GAAG,KAAK,GAAGA,MAAM,CAAC,UAAU,CAAC;EAErE,IAAMG,YAAY,GAAGC,KAAK,CAACC,OAAO,CAACP,OAAO,CAAC;EAC3C,IAAMQ,UAAU,GACZH,YAAY,GAAGL,OAAO,GAAG,CAACA,OAAO,CAAC;EAEtC,IAAMS,WAAW,GAAGD,UAAU,CAACE,GAAG,CAAC,WAAC;IAAA,OAAIC,CAAC,CAAC9B,IAAI;EAAA,EAAC;EAC/C,IAAM+B,YAAY,GAAa,EAAE;EACjC,IAAMC,SAAS,GAAGZ,QAAQ,CAACa,KAAK,EAAE;EAAC,4CACVL,WAAW;IAAA;EAAA;IAApC,uDAAsC;MAAA,IAA3BM,UAAU;MACnB,IAAIF,SAAS,CAACG,OAAO,CAACD,UAAU,CAAC,KAAK,CAAC,CAAC,EAAE;QACxCH,YAAY,CAACK,IAAI,CAAChB,QAAQ,CAACiB,QAAQ,CAACH,UAAU,CAAC,CAAC;OACjD,MAAM;QACLH,YAAY,CAACK,IAAI,CAAC,IAAI,CAAC;;;EAE1B;IAAA;EAAA;IAAA;EAAA;EAED,IAAId,KAAK,IAAI,IAAI,EAAE;IACjB;IACAA,KAAK,CAACgB,aAAa,GAAG,CAACC,QAAQ;IAC/BjB,KAAK,CAACkB,aAAa,GAAGD,QAAQ;;EAGhC;EACA,IAAME,eAAe,GACjBb,WAAW,CAACc,IAAI,CAAC,GAAG,CAAC,GAAG,GAAG,GAAGtB,QAAQ,CAACa,KAAK,EAAE,CAACU,IAAI,EAAE,CAACD,IAAI,CAAC,GAAG,CAAC;EACnE,IAAIE,MAAM,GAAqB/B,YAAY,CAACgC,GAAG,CAACJ,eAAe,CAAC;EAChE,IAAIK,eAA8C;EAClD,IAAIF,MAAM,IAAI,IAAI,EAAE;IAClB;IACA;IACA,IAAMG,GAAG,GAAGC,oCAAoC,CAACrB,UAAU,EAAEP,QAAQ,CAAC;IACtEwB,MAAM,GAAGG,GAAG,CAACH,MAAM;IACnBE,eAAe,GAAGC,GAAG,CAACD,eAAe;IAErC;IACAjC,YAAY,CAACoC,GAAG,CAACR,eAAe,EAAEG,MAAM,CAAC;IACzC9B,qBAAqB,CAACmC,GAAG,CAACR,eAAe,EAAEK,eAAe,CAAC;;EAE7DA,eAAe,GAAG,EAAE;EACpB,IAAI,CAACvB,QAAQ,EAAE;IACbZ,MAAM,CAACuC,MAAM,CAACJ,eAAe,EAAEhC,qBAAqB,CAAC+B,GAAG,CAACJ,eAAe,CAAC,CAAC;;EAG5E,IAAMU,gBAAgB,GAAG,IAAIlD,QAAQ,CAACmB,QAAQ,CAAC;EAE/C;EACA,KAAK,IAAIgC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGR,MAAM,CAACS,MAAM,EAAE,EAAED,CAAC,EAAE;IACtC,IAAI9B,KAAK,IAAI,IAAI,EAAE;MACjB;MACA,IAAMgC,UAAU,GAAGlE,MAAM,EAAE,CAACkE,UAAU;MACtC,IAAIA,UAAU,GAAGhC,KAAK,CAACgB,aAAa,EAAE;QACpChB,KAAK,CAACgB,aAAa,GAAGgB,UAAU;;MAElC,IAAIA,UAAU,GAAGhC,KAAK,CAACkB,aAAa,EAAE;QACpClB,KAAK,CAACkB,aAAa,GAAGc,UAAU;;;IAIpC,IAAMC,QAAQ,GAAGX,MAAM,CAACQ,CAAC,CAAC;IAC1B,IAAMI,QAAQ,GAAGD,QAAQ,CAACE,WAAW;IACrC,IAAID,QAAQ,YAAY/D,UAAU,EAAE;MAClC;;IAEF,IAAMiE,WAAW,GAAa,EAAE;IAChC,IAAMC,UAAU,GAAa,EAAE;IAC/B,IAAMC,gBAAgB,GAAa,EAAE;IAErC,IAAIC,UAAU,GAAG,KAAK;IAAC,4CACHN,QAAQ,CAACO,MAAM;MAAA;IAAA;MAAnC,uDAAqC;QAAA,IAA1BC,KAAK;QACd,IAAMvD,KAAK,GAAG2C,gBAAgB,CAACd,QAAQ,CAAC0B,KAAK,CAAC;QAC9C,IAAMtD,IAAI,GAAG0C,gBAAgB,CAACa,OAAO,CAACD,KAAK,CAAC;QAC5CL,WAAW,CAACtB,IAAI,CAAC5B,KAAK,CAAC;QACvBmD,UAAU,CAACvB,IAAI,CAAC3B,IAAI,CAAC;QACrB,IAAIA,IAAI,IAAI,IAAI,EAAE;UAChBoD,UAAU,GAAG,IAAI;;QAEnB,IAAI,CAACtC,QAAQ,EAAE;UACbuB,eAAe,CAACiB,KAAK,CAAC/D,IAAI,CAAC,EAAE;UAC7B,IAAI8C,eAAe,CAACiB,KAAK,CAAC/D,IAAI,CAAC,KAAK,CAAC,IAAI,CAACoB,QAAQ,CAAC6C,MAAM,CAACF,KAAK,CAAC,IAC5DnC,WAAW,CAACO,OAAO,CAAC4B,KAAK,CAAC/D,IAAI,CAAC,KAAK,CAAC,CAAC,IAAI,CAACQ,KAAK,CAAC0D,UAAU,IAC3DH,KAAK,CAACN,WAAW,CAACU,QAAQ,KAAK,IAAI,EAAE;YACvCP,gBAAgB,CAACxB,IAAI,CAAC5B,KAAK,CAAC;;;;IAGjC;MAAA;IAAA;MAAA;IAAA;IAED,IAAIqD,UAAU,EAAE;MACdxC,MAAM,GAAGA,MAAM,IAAI,EAAE;MACrBA,MAAM,CAAC,MAAM,CAAC,GAAGsC,UAAU,CAAC,CAAC,CAAC;;IAEhC,IAAMS,aAAa,GACf5E,MAAM,CAACgE,QAAQ,CAACa,KAAK,CAACX,WAAW,EAAErC,MAAM,CAAC,CAAa;IAC3D,IAAIiD,UAAU,GAAoB,IAAI;IACtC,IAAId,QAAQ,CAACe,eAAe,EAAE;MAC5BD,UAAU,GAAGd,QAAQ,CAACgB,WAAW,CAACd,WAAW,EAAEC,UAAU,CAAC;;IAE5D,IAAMc,YAAY,GAAGC,cAAc,CAACnB,QAAQ,CAAC;IAC7C,IAAMoB,qBAAqB,GACvBlD,KAAK,CAACC,OAAO,CAAC+C,YAAY,CAAC,GAAGA,YAAY,GAAG,CAACA,YAAY,CAAC;IAC/D,KAAK,IAAIrB,EAAC,GAAG,CAAC,EAAEA,EAAC,GAAGuB,qBAAqB,CAACtB,MAAM,EAAE,EAAED,EAAC,EAAE;MACrD,IAAI,CAACD,gBAAgB,CAACc,MAAM,CAACU,qBAAqB,CAACvB,EAAC,CAAC,CAAC,EAAE;QACtDD,gBAAgB,CAAC5C,GAAG,CAChBoE,qBAAqB,CAACvB,EAAC,CAAC,EAAEgB,aAAa,CAAChB,EAAC,CAAC,EAC1C3B,KAAK,CAACC,OAAO,CAAC4C,UAAU,CAAC,GAAGA,UAAU,CAAC,CAAC,CAAC,GAAGA,UAAU,CAAC;;MAE7D,IAAMM,KAAK,GAAGhD,WAAW,CAACO,OAAO,CAACwC,qBAAqB,CAACvB,EAAC,CAAC,CAACpD,IAAI,CAAC;MAChE,IAAI4E,KAAK,KAAK,CAAC,CAAC,EAAE;QAChB7C,YAAY,CAAC6C,KAAK,CAAC,GAAGR,aAAa,CAAChB,EAAC,CAAC;;;IAI1C,IAAI,CAAC7B,QAAQ,EAAE;MACb;MACApC,OAAO,CAACyE,gBAAgB,CAAC;;;EAG7B;EACA;EACA;EACA;EACA;EACAT,gBAAgB,CAAC0B,YAAY,EAAE;EAE/B,OAAOrD,YAAY,GAAGO,YAAY,GAAGA,YAAY,CAAC,CAAC,CAAC;AACtD;AAUA;;;;;;;;;;;AAWA,SAASiB,oCAAoC,CACzC7B,OAAyB,EAAEC,QAAkB;EAE/C/B,IAAI,CAACyF,MAAM,CACP3D,OAAO,IAAI,IAAI,IAAIA,OAAO,CAACkC,MAAM,GAAG,CAAC,EACrC;IAAA;EAAA,CAA6C,CAAC;EAElD,IAAI0B,WAAW,GAAqB,EAAE;EACtC,IAAIC,iBAAiB,GAAiB,EAAE;EACxC,IAAI7D,OAAO,CAACkC,MAAM,KAAK,CAAC,EAAE;IACxB;IACA,IAAMN,GAAG,GACLkC,+CAA+C,CAAC9D,OAAO,CAAC,CAAC,CAAC,EAAEC,QAAQ,CAAC;IACzE2D,WAAW,GAAGhC,GAAG,CAACH,MAAM;IACxBoC,iBAAiB,GAAGjC,GAAG,CAACmC,YAAY;GACrC,MAAM;IACL,IAAMC,OAAO,GAAG,IAAIC,GAAG,EAAU;IAAC,4CACdjE,OAAO;MAAA;IAAA;MAA3B,uDAA6B;QAAA,IAAlBkE,KAAK;QACd,4BACIJ,+CAA+C,CAACI,KAAK,EAAEjE,QAAQ,CAAC;UAD7DwB,MAAM,yBAANA,MAAM;UAAEsC,YAAY,yBAAZA,YAAY;QAG3B;QAAA,4CAC6BtC,MAAM;UAAA;QAAA;UAAnC,uDAAqC;YAAA,IAA1B0C,cAAc;YACvB,IAAI,CAACH,OAAO,CAACI,GAAG,CAACD,cAAc,CAACtF,IAAI,CAAC,EAAE;cACrC+E,WAAW,CAAC3C,IAAI,CAACkD,cAAc,CAAC;cAChCH,OAAO,CAAC5E,GAAG,CAAC+E,cAAc,CAACtF,IAAI,CAAC;;;UAIpC;QAAA;UAAA;QAAA;UAAA;QAAA;QAAA,iCACiC;UAC/B,IAAIgF,iBAAiB,CAAChF,IAAI,CAAC,IAAI,IAAI,EAAE;YACnCgF,iBAAiB,CAAChF,IAAI,CAAC,GAAG,IAAIoF,GAAG,EAAU;;UAE7CF,YAAY,CAAClF,IAAI,CAAC,CAACwF,OAAO,CACtB,mBAAS;YAAA,OAAIR,iBAAiB,CAAChF,IAAI,CAAC,CAACO,GAAG,CAACkF,SAAS,CAAC;UAAA,EAAC;SACzD;QAND,KAAK,IAAMzF,IAAI,IAAIkF,YAAY;UAAA;QAAA;;IAOhC;MAAA;IAAA;MAAA;IAAA;;EAEH,OAAO;IACLtC,MAAM,EAAEmC,WAAW;IACnBjC,eAAe,EAAE4C,mBAAmB,CAACV,iBAAiB;GACvD;AACH;AAEA,SAASU,mBAAmB,CAACR,YAA0B;EACrD,IAAMpC,eAAe,GAAoB,EAAE;EAC3C,KAAK,IAAM9C,IAAI,IAAIkF,YAAY,EAAE;IAC/BpC,eAAe,CAAC9C,IAAI,CAAC,GAAGkF,YAAY,CAAClF,IAAI,CAAC,CAAC2F,IAAI;;EAEjD,OAAO7C,eAAe;AACxB;AAEA;;;;;;;;;;;AAWA,OAAM,SAAUmC,+CAA+C,CAC3DI,KAAqB,EAAEjE,QAAkB;EAE3C,IAAM+D,OAAO,GAAG,IAAIC,GAAG,EAAU;EACjC,IAAMxC,MAAM,GAAqB,EAAE;EACnC,IAAMsC,YAAY,GAAiB,EAAE;EAErC;EACA;EACA;EAAA,4CACkB9D,QAAQ,CAACa,KAAK,EAAE;IAAA;EAAA;IAAlC,uDAAoC;MAAA,IAAzBrC,GAAG;MACZuF,OAAO,CAAC5E,GAAG,CAACX,GAAG,CAAC;;EACjB;IAAA;EAAA;IAAA;EAAA;EAED,IAAMgG,KAAK,GAAqB,EAAE;EAClC,IAAMC,KAAK,GAAa,EAAE;EAE1B;EACAD,KAAK,CAACxD,IAAI,CAACiD,KAAK,CAAC;EAEjB,OAAOO,KAAK,CAACvC,MAAM,GAAG,CAAC,EAAE;IACvB,IAAMyC,GAAG,GAAGF,KAAK,CAACA,KAAK,CAACvC,MAAM,GAAG,CAAC,CAAC;IACnC,IAAI8B,OAAO,CAACI,GAAG,CAACO,GAAG,CAAC9F,IAAI,CAAC,EAAE;MACzB4F,KAAK,CAACG,GAAG,EAAE;MACX;;IAEF,IAAMC,WAAW,GAAGH,KAAK,CAACA,KAAK,CAACxC,MAAM,GAAG,CAAC,CAAC,KAAKuC,KAAK,CAACvC,MAAM,GAAG,CAAC;IAChE,IAAIyC,GAAG,CAAChC,MAAM,CAACT,MAAM,KAAK,CAAC,IAAI2C,WAAW,EAAE;MAC1C;MACAJ,KAAK,CAACG,GAAG,EAAE;MACXnD,MAAM,CAACR,IAAI,CAAC0D,GAAG,CAAC;MAChBX,OAAO,CAAC5E,GAAG,CAACuF,GAAG,CAAC9F,IAAI,CAAC;MACrB,IAAIgG,WAAW,EAAE;QACfH,KAAK,CAACE,GAAG,EAAE;;KAEd,MAAM;MACL;MACA;MACAF,KAAK,CAACzD,IAAI,CAACwD,KAAK,CAACvC,MAAM,GAAG,CAAC,CAAC;MAAC,4CACTyC,GAAG,CAAChC,MAAM;QAAA;MAAA;QAA9B,uDAAgC;UAAA,IAArBC,KAAK;UACd;UACA;UACA,IAAImB,YAAY,CAACnB,KAAK,CAAC/D,IAAI,CAAC,IAAI,IAAI,EAAE;YACpCkF,YAAY,CAACnB,KAAK,CAAC/D,IAAI,CAAC,GAAG,IAAIoF,GAAG,EAAU;;UAE9CF,YAAY,CAACnB,KAAK,CAAC/D,IAAI,CAAC,CAACO,GAAG,CAACuF,GAAG,CAAC9F,IAAI,CAAC;UAEtC,IAAImF,OAAO,CAACI,GAAG,CAACxB,KAAK,CAAC/D,IAAI,CAAC,EAAE;YAC3B,SAAS,CAAE;;;UAEb4F,KAAK,CAACxD,IAAI,CAAC2B,KAAK,CAAC;;MAClB;QAAA;MAAA;QAAA;MAAA;;;EAGL,OAAO;IAACnB,MAAM,EAANA,MAAM;IAAEsC,YAAY,EAAZA;EAAY,CAAC;AAC/B;AAEA;;;;;;AAMA,SAASR,cAAc,CAACW,KAAqB;EAE3C,IAAIZ,YAA6C;EACjD,IAAIY,KAAK,CAAC5B,WAAW,CAACwC,YAAY,CAAC5C,MAAM,KAAK,CAAC,EAAE;IAC/CoB,YAAY,GAAGY,KAAK,CAAC5B,WAAW,CAACyC,MAAM;GACxC,MAAM;IACL,IAAIC,SAAS,GAAW,IAAI;IAC5B,KAAK,IAAI/C,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGiC,KAAK,CAAC5B,WAAW,CAACwC,YAAY,CAAC5C,MAAM,EAAE,EAAED,CAAC,EAAE;MAAA,4CACnCiC,KAAK,CAAC5B,WAAW,CAACwC,YAAY,CAAC7C,CAAC,CAAC,CAClDgB,aAAa;QAAA;MAAA;QADvB,uDACyB;UAAA,IADdgC,YAAY;UAErB,IAAIA,YAAY,CAACjG,EAAE,KAAKkF,KAAK,CAAClF,EAAE,EAAE;YAChCgG,SAAS,GAAG/C,CAAC;YACb;;;MAEH;QAAA;MAAA;QAAA;MAAA;;IAEHqB,YAAY,GAAGY,KAAK,CAAC5B,WAAW,CAAC4C,WAAW,CAACF,SAAS,CAAC;;EAEzD,OAAO1B,YAAY;AACrB","names":["cast","dispose","memory","util","ValueError","LruCache","toList","InputLayer","SymbolicTensor","assertFeedCompatibility","key","val","dtype","err","name","FeedDict","feeds","id","id2Value","id2Mask","feed","add","value","mask","name2Id","Object","keys","cachedSorted","cachedRecipientCounts","updateCacheMaxEntries","maxEntries","setMaxEntries","execute","fetches","feedDict","kwargs","probe","training","arrayFetches","Array","isArray","fetchArray","outputNames","map","t","finalOutputs","feedNames","names","outputName","indexOf","push","getValue","maxNumTensors","Infinity","minNumTensors","fetchAndFeedKey","join","sort","sorted","get","recipientCounts","out","getTopologicalSortAndRecipientCounts","put","assign","internalFeedDict","i","length","numTensors","symbolic","srcLayer","sourceLayer","inputValues","inputMasks","tensorsToDispose","maskExists","inputs","input","getMask","hasKey","isDisposed","stateful","outputTensors","apply","outputMask","supportsMasking","computeMask","layerOutputs","getNodeOutputs","outputSymbolicTensors","index","disposeMasks","assert","finalSorted","finalRecipientMap","getTopologicalSortAndRecipientCountsForOneFetch","recipientMap","visited","Set","fetch","symbolicTensor","has","forEach","recipient","recipientMap2Counts","size","stack","marks","top","pop","topIsMarked","inboundNodes","output","nodeIndex","outputTensor","getOutputAt"],"sources":["E:\\react-detect-toxicity-in-a-chat-app-youtube-2\\node_modules\\@tensorflow\\tfjs-layers\\src\\engine\\executor.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Executor: Evaluates SymbolicTensor based on feeds.\n */\n\nimport {cast, dispose, memory, Tensor, util} from '@tensorflow/tfjs-core';\n\nimport {ValueError} from '../errors';\nimport {Kwargs} from '../types';\nimport {LruCache} from '../utils/executor_utils';\nimport {toList} from '../utils/generic_utils';\n\nimport {InputLayer} from './input_layer';\nimport {SymbolicTensor} from './topology';\n\n/**\n * Helper function to check the dtype and shape compatibility of a feed value.\n */\nfunction assertFeedCompatibility(key: SymbolicTensor, val: Tensor): Tensor {\n  // Check dtype compatibility.\n  if (key.dtype == null || key.dtype === val.dtype) {\n    //  a.  If types match, return val tensor as is.\n    return val;\n  }\n  try {\n    //  b. Attempt to convert to expected type.\n    return cast(val, key.dtype);\n  } catch (err) {\n    //  c. If conversion fails, return helpful error.\n    throw new ValueError(\n        `The dtype of the feed (${val.dtype}) can not be cast to the dtype ` +\n        `of the key '${key.name}' (${key.dtype}).`);\n  }\n}\n\n/**\n * A concrete Tensor value for a symbolic tensor as the key.\n */\nexport interface Feed {\n  key: SymbolicTensor;\n  value: Tensor;\n}\n\n/**\n * FeedDict: A mapping from unique SymbolicTensors to feed values for them.\n * A feed value is a concrete value represented as an `Tensor`.\n */\nexport class FeedDict {\n  private id2Value: {[id: number]: Tensor} = {};\n  private id2Mask: {[id: number]: Tensor} = {};\n  private name2Id: {[name: string]: number} = {};\n\n  /**\n   * Constructor, optionally does copy-construction.\n   * @param feeds An Array of `Feed`s, or another `FeedDict`, in which case\n   *   copy-construction will be performed.\n   */\n  constructor(feeds?: Feed[]|FeedDict) {\n    if (feeds instanceof FeedDict) {\n      for (const id in feeds.id2Value) {\n        this.id2Value[id] = feeds.id2Value[id];\n        if (id in feeds.id2Mask) {\n          this.id2Mask[id] = feeds.id2Mask[id];\n        }\n      }\n    } else {\n      if (feeds == null) {\n        return;\n      }\n      for (const feed of feeds) {\n        this.add(feed.key, feed.value);\n      }\n    }\n  }\n\n  /**\n   * Add a key-value pair to the FeedDict.\n   *\n   * @param key The key of the feed.\n   * @param value The value of the tensor feed.\n   * @param mask The value of the mask feed (optional).\n   * @returns This `FeedDict`.\n   * @throws ValueError: If the key `SymbolicTensor` already exists in the\n   *   `FeedDict`.\n   */\n  add(key: SymbolicTensor, value: Tensor, mask?: Tensor): FeedDict {\n    if (this.id2Value[key.id] == null) {\n      this.id2Value[key.id] = assertFeedCompatibility(key, value);\n      this.name2Id[key.name] = key.id;\n      if (mask != null) {\n        this.id2Mask[key.id] = mask;\n      }\n    } else {\n      throw new ValueError(`Duplicate key: name=${key.name}, id=${key.id}`);\n    }\n    return this;\n  }\n\n  /**\n   * Add a Feed to the FeedDict.\n   * @param feed The new `Feed` to add.\n   * @returns This `FeedDict`.\n   */\n  addFeed(feed: Feed) {\n    this.add(feed.key, feed.value);\n  }\n\n  /**\n   * Probe whether a key already exists in the FeedDict.\n   * @param key\n   */\n  hasKey(key: SymbolicTensor): boolean {\n    return this.id2Value[key.id] != null;\n  }\n\n  /**\n   * Get all the SymbolicTensor available in this FeedDict.\n   */\n  names(): string[] {\n    return Object.keys(this.name2Id);\n  }\n\n  /**\n   * Get the feed value for given key.\n   * @param key The SymbolicTensor, or its name (as a string), of which the\n   *     value is sought.\n   * @returns If `key` exists, the corresponding feed value.\n   * @throws ValueError: If `key` does not exist in this `FeedDict`.\n   */\n  getValue(key: SymbolicTensor|string): Tensor {\n    if (key instanceof SymbolicTensor) {\n      if (this.id2Value[key.id] == null) {\n        throw new ValueError(`Nonexistent key: ${key.name}`);\n      } else {\n        return this.id2Value[key.id];\n      }\n    } else {\n      const id = this.name2Id[key];\n      if (id == null) {\n        throw new ValueError(`Feed dict has no SymbolicTensor name: ${key}`);\n      }\n      return this.id2Value[id];\n    }\n  }\n\n  /**\n   * Get the feed mask for given key.\n   * @param key The SymbolicTensor, or its name (as a string), of which the\n   *     value is sought.\n   * @returns If `key` exists, the corresponding feed mask.\n   * @throws ValueError: If `key` does not exist in this `FeedDict`.\n   */\n  getMask(key: SymbolicTensor|string): Tensor {\n    if (key instanceof SymbolicTensor) {\n      if (this.id2Value[key.id] == null) {\n        throw new ValueError(`Nonexistent key: ${key.name}`);\n      } else {\n        return this.id2Mask[key.id];\n      }\n    } else {\n      const id = this.name2Id[key];\n      if (id == null) {\n        throw new ValueError(`Feed dict has no SymbolicTensor name: ${key}`);\n      }\n      return this.id2Mask[id];\n    }\n  }\n\n  /** Dispose all mask Tensors held by this object. */\n  disposeMasks() {\n    if (this.id2Mask != null) {\n      dispose(this.id2Mask);\n    }\n  }\n}\n\n// Cache for topologically sorted SymbolicTensors for given execution\n// targets (i.e., fetches).\nexport const cachedSorted: LruCache<SymbolicTensor[]> =\n    new LruCache<SymbolicTensor[]>();\n\n// Cache for recipient count maps for given execution targets (i.e., fetches).\nexport const cachedRecipientCounts: LruCache<RecipientCounts> =\n    new LruCache<RecipientCounts>();\n\nexport function updateCacheMaxEntries(maxEntries: number) {\n  if (cachedSorted != null) {\n    cachedSorted.setMaxEntries(maxEntries);\n  }\n  if (cachedRecipientCounts != null) {\n    cachedRecipientCounts.setMaxEntries(maxEntries);\n  }\n}\n\n/**\n * Interface for the optional object used for probing the memory\n * usage and other statistics during execution.\n */\nexport interface ExecutionProbe {\n  /**\n   * Maximum number of tensors that exist during all steps of the\n   * execution. Tensor counts are measured at the beginning of every\n   * step.\n   */\n  maxNumTensors?: number;\n\n  /**\n   * Minimum number of tensors that exist during all steps of the\n   * execution. Tensor counts are measured at the beginning of every\n   * step.\n   */\n  minNumTensors?: number;\n}\n\n/**\n * Execute a SymbolicTensor by using concrete feed values.\n *\n * A `SymbolicTensor` object is a node in a computation graph of TF.js\n * Layers. The object is backed by a source layer and input\n * `SymbolicTensor`s to the source layer. This method evaluates\n * the `call()` method of the source layer, using concrete values of the\n * inputs obtained from either\n * * `feedDict`, if the input key exists in `feedDict`, or else,\n * * a recursive call to `execute()` itself.\n *\n * @param x: The `SymbolicTensor` to execute.\n * @param feedDict: The feed values, as base condition of the recursion.\n *   execution.\n * @param kwargs: Optional keyword arguments.\n * @param probe: A probe object (of interface `ExecutionProbe`) used for\n *   testing memory footprint of `execute` calls.\n * @returns Result of the execution.\n * @throws ValueError: If any `SymbolicTensor`s from `InputLayer`s\n *   encountered during the execution lacks a feed value in `feedDict`.\n */\nexport function execute(\n    fetches: SymbolicTensor|SymbolicTensor[], feedDict: FeedDict,\n    kwargs?: Kwargs, probe?: ExecutionProbe): Tensor|\n    Tensor[]|[Tensor | Tensor[]] {\n  const training: boolean = kwargs == null ? false : kwargs['training'];\n\n  const arrayFetches = Array.isArray(fetches);\n  const fetchArray: SymbolicTensor[] =\n      arrayFetches ? fetches : [fetches];\n\n  const outputNames = fetchArray.map(t => t.name);\n  const finalOutputs: Tensor[] = [];\n  const feedNames = feedDict.names();\n  for (const outputName of outputNames) {\n    if (feedNames.indexOf(outputName) !== -1) {\n      finalOutputs.push(feedDict.getValue(outputName));\n    } else {\n      finalOutputs.push(null);\n    }\n  }\n\n  if (probe != null) {\n    // For optional probing of memory footprint during execution.\n    probe.maxNumTensors = -Infinity;\n    probe.minNumTensors = Infinity;\n  }\n\n  // Check cache.\n  const fetchAndFeedKey =\n      outputNames.join(',') + '|' + feedDict.names().sort().join(',');\n  let sorted: SymbolicTensor[] = cachedSorted.get(fetchAndFeedKey);\n  let recipientCounts: {[fetchName: string]: number};\n  if (sorted == null) {\n    // Cache doesn't contain the desired combination of fetches. Compute\n    // topological sort for the combination for the first time.\n    const out = getTopologicalSortAndRecipientCounts(fetchArray, feedDict);\n    sorted = out.sorted;\n    recipientCounts = out.recipientCounts;\n\n    // Store results in cache for future use.\n    cachedSorted.put(fetchAndFeedKey, sorted);\n    cachedRecipientCounts.put(fetchAndFeedKey, recipientCounts);\n  }\n  recipientCounts = {};\n  if (!training) {\n    Object.assign(recipientCounts, cachedRecipientCounts.get(fetchAndFeedKey));\n  }\n\n  const internalFeedDict = new FeedDict(feedDict);\n\n  // Start iterative execution on the topologically-sorted SymbolicTensors.\n  for (let i = 0; i < sorted.length; ++i) {\n    if (probe != null) {\n      // For optional probing of memory usage during execution.\n      const numTensors = memory().numTensors;\n      if (numTensors > probe.maxNumTensors) {\n        probe.maxNumTensors = numTensors;\n      }\n      if (numTensors < probe.minNumTensors) {\n        probe.minNumTensors = numTensors;\n      }\n    }\n\n    const symbolic = sorted[i];\n    const srcLayer = symbolic.sourceLayer;\n    if (srcLayer instanceof InputLayer) {\n      continue;\n    }\n    const inputValues: Tensor[] = [];\n    const inputMasks: Tensor[] = [];\n    const tensorsToDispose: Tensor[] = [];\n\n    let maskExists = false;\n    for (const input of symbolic.inputs) {\n      const value = internalFeedDict.getValue(input);\n      const mask = internalFeedDict.getMask(input);\n      inputValues.push(value);\n      inputMasks.push(mask);\n      if (mask != null) {\n        maskExists = true;\n      }\n      if (!training) {\n        recipientCounts[input.name]--;\n        if (recipientCounts[input.name] === 0 && !feedDict.hasKey(input) &&\n            outputNames.indexOf(input.name) === -1 && !value.isDisposed &&\n            input.sourceLayer.stateful !== true) {\n          tensorsToDispose.push(value);\n        }\n      }\n    }\n\n    if (maskExists) {\n      kwargs = kwargs || {};\n      kwargs['mask'] = inputMasks[0];\n    }\n    const outputTensors =\n        toList(srcLayer.apply(inputValues, kwargs)) as Tensor[];\n    let outputMask: Tensor|Tensor[] = null;\n    if (srcLayer.supportsMasking) {\n      outputMask = srcLayer.computeMask(inputValues, inputMasks);\n    }\n    const layerOutputs = getNodeOutputs(symbolic);\n    const outputSymbolicTensors =\n        Array.isArray(layerOutputs) ? layerOutputs : [layerOutputs];\n    for (let i = 0; i < outputSymbolicTensors.length; ++i) {\n      if (!internalFeedDict.hasKey(outputSymbolicTensors[i])) {\n        internalFeedDict.add(\n            outputSymbolicTensors[i], outputTensors[i],\n            Array.isArray(outputMask) ? outputMask[0] : outputMask);\n      }\n      const index = outputNames.indexOf(outputSymbolicTensors[i].name);\n      if (index !== -1) {\n        finalOutputs[index] = outputTensors[i];\n      }\n    }\n\n    if (!training) {\n      // Clean up Tensors that are no longer needed.\n      dispose(tensorsToDispose);\n    }\n  }\n  // NOTE(cais): Unlike intermediate tensors, we don't discard mask\n  // tensors as we go, because these tensors are sometimes passed over a\n  // series of mutliple layers, i.e., not obeying the immediate input\n  // relations in the graph. If this becomes a memory-usage concern,\n  // we can improve this in the future.\n  internalFeedDict.disposeMasks();\n\n  return arrayFetches ? finalOutputs : finalOutputs[0];\n}\n\ntype RecipientCounts = {\n  [fetchName: string]: number\n};\n\nexport type RecipientMap = {\n  [fetchName: string]: Set<string>;\n};\n\n/**\n * Sort the `SymbolicTensor`s topologically, for an array of fetches.\n *\n * This function calls getTopologicalSortAndRecipientCountsForOneFetch and\n * merges their results.\n *\n * @param fetch The array of fetches requested. Must be a non-empty array.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientCounts: Recipient counts for all SymbolicTensors in `sorted`.\n */\nfunction getTopologicalSortAndRecipientCounts(\n    fetches: SymbolicTensor[], feedDict: FeedDict):\n    {sorted: SymbolicTensor[], recipientCounts: RecipientCounts} {\n  util.assert(\n      fetches != null && fetches.length > 0,\n      () => `Expected at least one fetch, got none`);\n\n  let finalSorted: SymbolicTensor[] = [];\n  let finalRecipientMap: RecipientMap = {};\n  if (fetches.length === 1) {\n    // Special-casing 1 fetch for efficiency.\n    const out =\n        getTopologicalSortAndRecipientCountsForOneFetch(fetches[0], feedDict);\n    finalSorted = out.sorted;\n    finalRecipientMap = out.recipientMap;\n  } else {\n    const visited = new Set<string>();\n    for (const fetch of fetches) {\n      const {sorted, recipientMap} =\n          getTopologicalSortAndRecipientCountsForOneFetch(fetch, feedDict);\n\n      // Merge sorted SymbolicTensor Arrays.\n      for (const symbolicTensor of sorted) {\n        if (!visited.has(symbolicTensor.name)) {\n          finalSorted.push(symbolicTensor);\n          visited.add(symbolicTensor.name);\n        }\n      }\n\n      // Merge recipient maps.\n      for (const name in recipientMap) {\n        if (finalRecipientMap[name] == null) {\n          finalRecipientMap[name] = new Set<string>();\n        }\n        recipientMap[name].forEach(\n            recipient => finalRecipientMap[name].add(recipient));\n      }\n    }\n  }\n  return {\n    sorted: finalSorted,\n    recipientCounts: recipientMap2Counts(finalRecipientMap)\n  };\n}\n\nfunction recipientMap2Counts(recipientMap: RecipientMap): RecipientCounts {\n  const recipientCounts: RecipientCounts = {};\n  for (const name in recipientMap) {\n    recipientCounts[name] = recipientMap[name].size;\n  }\n  return recipientCounts;\n}\n\n/**\n * Sort the `SymbolicTensor`s topologically, for a single fetch.\n *\n * This helper function processes the upstream SymbolicTensors of a single\n * fetch.\n *\n * @param fetch The single fetch requested.\n * @param feedDict The dictionary of fed values.\n * @returns sorted: Topologically-sorted array of SymbolicTensors.\n *   recipientMap: Recipient names for all SymbolicTensors in `sorted`.\n */\nexport function getTopologicalSortAndRecipientCountsForOneFetch(\n    fetch: SymbolicTensor, feedDict: FeedDict):\n    {sorted: SymbolicTensor[], recipientMap: RecipientMap} {\n  const visited = new Set<string>();\n  const sorted: SymbolicTensor[] = [];\n  const recipientMap: RecipientMap = {};\n\n  // Put keys of the feedDict into visited first, so they don't have to be\n  // walked. This is needed in case where there are feeds for intermediate\n  // SymbolicTensors of the graph.\n  for (const key of feedDict.names()) {\n    visited.add(key);\n  }\n\n  const stack: SymbolicTensor[] = [];\n  const marks: number[] = [];\n\n  // Initial population of stack and marks.\n  stack.push(fetch);\n\n  while (stack.length > 0) {\n    const top = stack[stack.length - 1];\n    if (visited.has(top.name)) {\n      stack.pop();\n      continue;\n    }\n    const topIsMarked = marks[marks.length - 1] === stack.length - 1;\n    if (top.inputs.length === 0 || topIsMarked) {\n      // Input SymbolicTensor or all children have been visited.\n      stack.pop();\n      sorted.push(top);\n      visited.add(top.name);\n      if (topIsMarked) {\n        marks.pop();\n      }\n    } else {\n      // A non-input SymbolicTensor whose upstream SymbolicTensors haven't\n      // been visited yet. Push them onto the stack.\n      marks.push(stack.length - 1);\n      for (const input of top.inputs) {\n        // Increment the recipient count. Note that this needs to happen\n        // regardless of whether the SymbolicTensor has been visited before.\n        if (recipientMap[input.name] == null) {\n          recipientMap[input.name] = new Set<string>();\n        }\n        recipientMap[input.name].add(top.name);\n\n        if (visited.has(input.name)) {\n          continue;  // Avoid repeated visits to the same SymbolicTensor.\n        }\n        stack.push(input);\n      }\n    }\n  }\n  return {sorted, recipientMap};\n}\n\n/**\n * Get the symbolic output tensors of the node to which a given fetch belongs.\n * @param fetch The fetched symbolic tensor.\n * @returns The Array of symbolic tensors output by the node to which `fetch`\n *   belongs.\n */\nfunction getNodeOutputs(fetch: SymbolicTensor): SymbolicTensor|\n    SymbolicTensor[] {\n  let layerOutputs: SymbolicTensor|SymbolicTensor[];\n  if (fetch.sourceLayer.inboundNodes.length === 1) {\n    layerOutputs = fetch.sourceLayer.output;\n  } else {\n    let nodeIndex: number = null;\n    for (let i = 0; i < fetch.sourceLayer.inboundNodes.length; ++i) {\n      for (const outputTensor of fetch.sourceLayer.inboundNodes[i]\n               .outputTensors) {\n        if (outputTensor.id === fetch.id) {\n          nodeIndex = i;\n          break;\n        }\n      }\n    }\n    layerOutputs = fetch.sourceLayer.getOutputAt(nodeIndex);\n  }\n  return layerOutputs;\n}\n"]},"metadata":{},"sourceType":"module","externalDependencies":[]}