{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { backend_util, Softmax, util } from '@tensorflow/tfjs-core';\nimport { exp } from './Exp';\nimport { max } from './Max';\nimport { div } from './RealDiv';\nimport { reshape } from './Reshape';\nimport { sub } from './Sub';\nimport { sum } from './Sum';\nexport function softmax(args) {\n  var inputs = args.inputs,\n    backend = args.backend,\n    attrs = args.attrs;\n  var logits = inputs.logits;\n  var dim = attrs.dim;\n  var logitsRank = logits.shape.length;\n  var $dim = dim;\n  if ($dim === -1) {\n    $dim = logitsRank - 1;\n  }\n  if ($dim !== logitsRank - 1) {\n    throw Error('Softmax along a non-last dimension is not yet supported. ' + \"Logits was rank \".concat(logitsRank, \" and dim was \").concat($dim));\n  }\n  var axes = util.parseAxisParam([$dim], logits.shape);\n  var maxLogit = max({\n    inputs: {\n      x: logits\n    },\n    backend: backend,\n    attrs: {\n      reductionIndices: axes,\n      keepDims: false\n    }\n  });\n  var expandedShape = backend_util.expandShapeToKeepDim(maxLogit.shape, axes);\n  var maxLogitReshaped = reshape({\n    inputs: {\n      x: maxLogit\n    },\n    backend: backend,\n    attrs: {\n      shape: expandedShape\n    }\n  });\n  var a = sub({\n    inputs: {\n      a: logits,\n      b: maxLogitReshaped\n    },\n    backend: backend\n  });\n  var b = exp({\n    inputs: {\n      x: a\n    },\n    backend: backend\n  });\n  var sumExp = sum({\n    inputs: {\n      x: b\n    },\n    backend: backend,\n    attrs: {\n      axis: axes,\n      keepDims: false\n    }\n  });\n  var sumReshaped = reshape({\n    inputs: {\n      x: sumExp\n    },\n    backend: backend,\n    attrs: {\n      shape: expandedShape\n    }\n  });\n  var result = div({\n    inputs: {\n      a: b,\n      b: sumReshaped\n    },\n    backend: backend\n  });\n  backend.disposeIntermediateTensorInfo(maxLogit);\n  backend.disposeIntermediateTensorInfo(maxLogitReshaped);\n  backend.disposeIntermediateTensorInfo(a);\n  backend.disposeIntermediateTensorInfo(b);\n  backend.disposeIntermediateTensorInfo(sumExp);\n  backend.disposeIntermediateTensorInfo(sumReshaped);\n  return result;\n}\nexport var softmaxConfig = {\n  kernelName: Softmax,\n  backendName: 'cpu',\n  kernelFunc: softmax\n};","map":{"version":3,"mappings":"AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,YAAY,EAA4BC,OAAO,EAA2CC,IAAI,QAAO,uBAAuB;AAIpI,SAAQC,GAAG,QAAO,OAAO;AACzB,SAAQC,GAAG,QAAO,OAAO;AACzB,SAAQC,GAAG,QAAO,WAAW;AAC7B,SAAQC,OAAO,QAAO,WAAW;AACjC,SAAQC,GAAG,QAAO,OAAO;AACzB,SAAQC,GAAG,QAAO,OAAO;AAEzB,OAAM,SAAUC,OAAO,CACnBC,IACyE;EAE3E,IAAOC,MAAM,GAAoBD,IAAI,CAA9BC,MAAM;IAAEC,OAAO,GAAWF,IAAI,CAAtBE,OAAO;IAAEC,KAAK,GAAIH,IAAI,CAAbG,KAAK;EAC7B,IAAOC,MAAM,GAAIH,MAAM,CAAhBG,MAAM;EACb,IAAOC,GAAG,GAAIF,KAAK,CAAZE,GAAG;EAEV,IAAMC,UAAU,GAAGF,MAAM,CAACG,KAAK,CAACC,MAAM;EAEtC,IAAIC,IAAI,GAAGJ,GAAG;EACd,IAAII,IAAI,KAAK,CAAC,CAAC,EAAE;IACfA,IAAI,GAAGH,UAAU,GAAG,CAAC;;EAEvB,IAAIG,IAAI,KAAKH,UAAU,GAAG,CAAC,EAAE;IAC3B,MAAMI,KAAK,CACP,2DAA2D,6BACxCJ,UAAU,0BAAgBG,IAAI,CAAE,CAAC;;EAG1D,IAAME,IAAI,GAAGnB,IAAI,CAACoB,cAAc,CAAC,CAACH,IAAI,CAAC,EAAEL,MAAM,CAACG,KAAK,CAAC;EACtD,IAAMM,QAAQ,GAAGnB,GAAG,CAAC;IACnBO,MAAM,EAAE;MAACa,CAAC,EAAEV;IAAM,CAAC;IACnBF,OAAO,EAAPA,OAAO;IACPC,KAAK,EAAE;MAACY,gBAAgB,EAAEJ,IAAI;MAAEK,QAAQ,EAAE;IAAK;GAChD,CAAC;EACF,IAAMC,aAAa,GAAG3B,YAAY,CAAC4B,oBAAoB,CAACL,QAAQ,CAACN,KAAK,EAAEI,IAAI,CAAC;EAE7E,IAAMQ,gBAAgB,GAClBvB,OAAO,CAAC;IAACK,MAAM,EAAE;MAACa,CAAC,EAAED;IAAQ,CAAC;IAAEX,OAAO,EAAPA,OAAO;IAAEC,KAAK,EAAE;MAACI,KAAK,EAAEU;IAAa;EAAC,CAAC,CAAC;EAC5E,IAAMG,CAAC,GACHvB,GAAG,CAAC;IAACI,MAAM,EAAE;MAACmB,CAAC,EAAEhB,MAAM;MAAEiB,CAAC,EAAEF;IAAgB,CAAC;IAAEjB,OAAO,EAAPA;EAAO,CAAC,CAAe;EAC1E,IAAMmB,CAAC,GAAG5B,GAAG,CAAC;IAACQ,MAAM,EAAE;MAACa,CAAC,EAAEM;IAAC,CAAC;IAAElB,OAAO,EAAPA;EAAO,CAAC,CAAe;EACtD,IAAMoB,MAAM,GACRxB,GAAG,CAAC;IAACG,MAAM,EAAE;MAACa,CAAC,EAAEO;IAAC,CAAC;IAAEnB,OAAO,EAAPA,OAAO;IAAEC,KAAK,EAAE;MAACoB,IAAI,EAAEZ,IAAI;MAAEK,QAAQ,EAAE;IAAK;EAAC,CAAC,CAAC;EACxE,IAAMQ,WAAW,GACb5B,OAAO,CAAC;IAACK,MAAM,EAAE;MAACa,CAAC,EAAEQ;IAAM,CAAC;IAAEpB,OAAO,EAAPA,OAAO;IAAEC,KAAK,EAAE;MAACI,KAAK,EAAEU;IAAa;EAAC,CAAC,CAAC;EAE1E,IAAMQ,MAAM,GAAG9B,GAAG,CAAC;IAACM,MAAM,EAAE;MAACmB,CAAC,EAAEC,CAAC;MAAEA,CAAC,EAAEG;IAAW,CAAC;IAAEtB,OAAO,EAAPA;EAAO,CAAC,CAAe;EAE3EA,OAAO,CAACwB,6BAA6B,CAACb,QAAQ,CAAC;EAC/CX,OAAO,CAACwB,6BAA6B,CAACP,gBAAgB,CAAC;EACvDjB,OAAO,CAACwB,6BAA6B,CAACN,CAAC,CAAC;EACxClB,OAAO,CAACwB,6BAA6B,CAACL,CAAC,CAAC;EACxCnB,OAAO,CAACwB,6BAA6B,CAACJ,MAAM,CAAC;EAC7CpB,OAAO,CAACwB,6BAA6B,CAACF,WAAW,CAAC;EAElD,OAAOC,MAAM;AACf;AAEA,OAAO,IAAME,aAAa,GAAiB;EACzCC,UAAU,EAAErC,OAAO;EACnBsC,WAAW,EAAE,KAAK;EAClBC,UAAU,EAAE/B;CACb","names":["backend_util","Softmax","util","exp","max","div","reshape","sub","sum","softmax","args","inputs","backend","attrs","logits","dim","logitsRank","shape","length","$dim","Error","axes","parseAxisParam","maxLogit","x","reductionIndices","keepDims","expandedShape","expandShapeToKeepDim","maxLogitReshaped","a","b","sumExp","axis","sumReshaped","result","disposeIntermediateTensorInfo","softmaxConfig","kernelName","backendName","kernelFunc"],"sources":["E:\\react-detect-toxicity-in-a-chat-app-youtube-2\\node_modules\\@tensorflow\\tfjs-backend-cpu\\src\\kernels\\Softmax.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {backend_util, KernelConfig, KernelFunc, Softmax, SoftmaxAttrs, SoftmaxInputs, TensorInfo, util} from '@tensorflow/tfjs-core';\n\nimport {MathBackendCPU} from '../backend_cpu';\n\nimport {exp} from './Exp';\nimport {max} from './Max';\nimport {div} from './RealDiv';\nimport {reshape} from './Reshape';\nimport {sub} from './Sub';\nimport {sum} from './Sum';\n\nexport function softmax(\n    args:\n        {inputs: SoftmaxInputs, backend: MathBackendCPU, attrs: SoftmaxAttrs}):\n    TensorInfo {\n  const {inputs, backend, attrs} = args;\n  const {logits} = inputs;\n  const {dim} = attrs;\n\n  const logitsRank = logits.shape.length;\n\n  let $dim = dim;\n  if ($dim === -1) {\n    $dim = logitsRank - 1;\n  }\n  if ($dim !== logitsRank - 1) {\n    throw Error(\n        'Softmax along a non-last dimension is not yet supported. ' +\n        `Logits was rank ${logitsRank} and dim was ${$dim}`);\n  }\n\n  const axes = util.parseAxisParam([$dim], logits.shape);\n  const maxLogit = max({\n    inputs: {x: logits},\n    backend,\n    attrs: {reductionIndices: axes, keepDims: false}\n  });\n  const expandedShape = backend_util.expandShapeToKeepDim(maxLogit.shape, axes);\n\n  const maxLogitReshaped =\n      reshape({inputs: {x: maxLogit}, backend, attrs: {shape: expandedShape}});\n  const a =\n      sub({inputs: {a: logits, b: maxLogitReshaped}, backend}) as TensorInfo;\n  const b = exp({inputs: {x: a}, backend}) as TensorInfo;\n  const sumExp =\n      sum({inputs: {x: b}, backend, attrs: {axis: axes, keepDims: false}});\n  const sumReshaped =\n      reshape({inputs: {x: sumExp}, backend, attrs: {shape: expandedShape}});\n\n  const result = div({inputs: {a: b, b: sumReshaped}, backend}) as TensorInfo;\n\n  backend.disposeIntermediateTensorInfo(maxLogit);\n  backend.disposeIntermediateTensorInfo(maxLogitReshaped);\n  backend.disposeIntermediateTensorInfo(a);\n  backend.disposeIntermediateTensorInfo(b);\n  backend.disposeIntermediateTensorInfo(sumExp);\n  backend.disposeIntermediateTensorInfo(sumReshaped);\n\n  return result;\n}\n\nexport const softmaxConfig: KernelConfig = {\n  kernelName: Softmax,\n  backendName: 'cpu',\n  kernelFunc: softmax as unknown as KernelFunc\n};\n"]},"metadata":{},"sourceType":"module","externalDependencies":[]}