{"ast":null,"code":"import _regeneratorRuntime from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/regeneratorRuntime.js\";\nimport _asyncToGenerator from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\nimport _classCallCheck from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";\nimport _createClass from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createClass.js\";\nimport _inherits from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/inherits.js\";\nimport _createSuper from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createSuper.js\";\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose as _dispose, tidy } from '../globals';\nimport { abs } from '../ops/abs';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { maximum } from '../ops/maximum';\nimport { mul } from '../ops/mul';\nimport { scalar } from '../ops/scalar';\nimport { sub } from '../ops/sub';\nimport { zerosLike } from '../ops/zeros_like';\nimport { Optimizer } from './optimizer';\nexport var AdamaxOptimizer = /*#__PURE__*/function (_Optimizer) {\n  _inherits(AdamaxOptimizer, _Optimizer);\n  var _super = _createSuper(AdamaxOptimizer);\n  function AdamaxOptimizer(learningRate, beta1, beta2) {\n    var _this;\n    var epsilon = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n    var decay = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 0.0;\n    _classCallCheck(this, AdamaxOptimizer);\n    _this = _super.call(this);\n    _this.learningRate = learningRate;\n    _this.beta1 = beta1;\n    _this.beta2 = beta2;\n    _this.epsilon = epsilon;\n    _this.decay = decay;\n    _this.accumulatedFirstMoment = [];\n    _this.accumulatedWeightedInfNorm = [];\n    tidy(function () {\n      _this.iteration = scalar(0).variable();\n      _this.accBeta1 = scalar(beta1).variable();\n    });\n    if (epsilon == null) {\n      _this.epsilon = ENGINE.backend.epsilon();\n    }\n    return _this;\n  }\n  /** @nocollapse */\n  _createClass(AdamaxOptimizer, [{\n    key: \"applyGradients\",\n    value: function applyGradients(variableGradients) {\n      var _this2 = this;\n      var variableNames = Array.isArray(variableGradients) ? variableGradients.map(function (item) {\n        return item.name;\n      }) : Object.keys(variableGradients);\n      tidy(function () {\n        var oneMinusAccBeta1 = sub(1, _this2.accBeta1);\n        var lr = div(-_this2.learningRate, add(mul(_this2.iteration, _this2.decay), 1));\n        variableNames.forEach(function (name, i) {\n          var value = ENGINE.registeredVariables[name];\n          var trainable = false;\n          if (_this2.accumulatedFirstMoment[i] == null) {\n            _this2.accumulatedFirstMoment[i] = {\n              originalName: \"\".concat(name, \"/m\"),\n              variable: zerosLike(value).variable(trainable)\n            };\n          }\n          if (_this2.accumulatedWeightedInfNorm[i] == null) {\n            _this2.accumulatedWeightedInfNorm[i] = {\n              originalName: \"\".concat(name, \"/v\"),\n              variable: zerosLike(value).variable(trainable)\n            };\n          }\n          var gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];\n          if (gradient == null) {\n            return;\n          }\n          var firstMoment = _this2.accumulatedFirstMoment[i].variable;\n          var weightedInfNorm = _this2.accumulatedWeightedInfNorm[i].variable;\n          var newFirstMoment = add(mul(firstMoment, _this2.beta1), mul(gradient, 1 - _this2.beta1));\n          var ut0 = mul(weightedInfNorm, _this2.beta2);\n          var ut1 = abs(gradient);\n          var newWeightedInfNorm = maximum(ut0, ut1);\n          firstMoment.assign(newFirstMoment);\n          weightedInfNorm.assign(newWeightedInfNorm);\n          var newValue = add(mul(div(lr, oneMinusAccBeta1), div(newFirstMoment, add(newWeightedInfNorm, _this2.epsilon))), value);\n          value.assign(newValue);\n        });\n        _this2.iteration.assign(add(_this2.iteration, 1));\n        _this2.accBeta1.assign(mul(_this2.accBeta1, _this2.beta1));\n      });\n      this.incrementIterations();\n    }\n  }, {\n    key: \"dispose\",\n    value: function dispose() {\n      this.accBeta1.dispose();\n      this.iteration.dispose();\n      if (this.accumulatedFirstMoment != null) {\n        _dispose(this.accumulatedFirstMoment.map(function (v) {\n          return v.variable;\n        }));\n      }\n      if (this.accumulatedWeightedInfNorm != null) {\n        _dispose(this.accumulatedWeightedInfNorm.map(function (v) {\n          return v.variable;\n        }));\n      }\n    }\n  }, {\n    key: \"getWeights\",\n    value: function () {\n      var _getWeights = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee() {\n        return _regeneratorRuntime().wrap(function _callee$(_context) {\n          while (1) switch (_context.prev = _context.next) {\n            case 0:\n              throw new Error('getWeights() is not implemented for Adamax yet.');\n            case 1:\n            case \"end\":\n              return _context.stop();\n          }\n        }, _callee);\n      }));\n      function getWeights() {\n        return _getWeights.apply(this, arguments);\n      }\n      return getWeights;\n    }()\n  }, {\n    key: \"setWeights\",\n    value: function () {\n      var _setWeights = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee2(weightValues) {\n        return _regeneratorRuntime().wrap(function _callee2$(_context2) {\n          while (1) switch (_context2.prev = _context2.next) {\n            case 0:\n              throw new Error('setWeights() is not implemented for Adamax yet.');\n            case 1:\n            case \"end\":\n              return _context2.stop();\n          }\n        }, _callee2);\n      }));\n      function setWeights(_x) {\n        return _setWeights.apply(this, arguments);\n      }\n      return setWeights;\n    }()\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      return {\n        'learningRate': this.learningRate,\n        'beta1': this.beta1,\n        'beta2': this.beta2,\n        'epsilon': this.epsilon,\n        'decay': this.decay\n      };\n    }\n    /** @nocollapse */\n  }], [{\n    key: \"className\",\n    get: function get() {\n      // Name matters for Python compatibility.\n      // This is a getter instead of a property because when it's a property, it\n      // prevents the entire class from being tree-shaken.\n      return 'Adamax';\n    }\n  }, {\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      return new cls(config['learningRate'], config['beta1'], config['beta2'], config['epsilon'], config['decay']);\n    }\n  }]);\n  return AdamaxOptimizer;\n}(Optimizer);","map":{"version":3,"mappings":";;;;;;AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,MAAM,QAAO,WAAW;AAChC,SAAQC,OAAO,IAAPA,QAAO,EAAEC,IAAI,QAAO,YAAY;AACxC,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,OAAO,QAAO,gBAAgB;AACtC,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,MAAM,QAAO,eAAe;AACpC,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,SAAS,QAAO,mBAAmB;AAK3C,SAAQC,SAAS,QAA0B,aAAa;AAExD,WAAaC,eAAgB;EAAA;EAAA;EAc3B,yBACcC,YAAoB,EAAYC,KAAa,EAC7CC,KAAa,EACF;IAAA;IAAA,IADcC,8EAAkB,IAAI;IAAA,IAC/CC,4EAAQ,GAAG;IAAA;IACvB;IAHY,kBAAY,GAAZJ,YAAY;IAAoB,WAAK,GAALC,KAAK;IACrC,WAAK,GAALC,KAAK;IAAoB,aAAO,GAAPC,OAAO;IAChC,WAAK,GAALC,KAAK;IANX,4BAAsB,GAAwB,EAAE;IAChD,gCAA0B,GAAwB,EAAE;IAQ1Df,IAAI,CAAC,YAAK;MACR,MAAKgB,SAAS,GAAGV,MAAM,CAAC,CAAC,CAAC,CAACW,QAAQ,EAAE;MACrC,MAAKC,QAAQ,GAAGZ,MAAM,CAACM,KAAK,CAAC,CAACK,QAAQ,EAAE;IAC1C,CAAC,CAAC;IAEF,IAAIH,OAAO,IAAI,IAAI,EAAE;MACnB,MAAKA,OAAO,GAAGhB,MAAM,CAACqB,OAAO,CAACL,OAAO,EAAE;;IACxC;EACH;EA3BA;EAAA;IAAA;IAAA,OA6BA,wBAAeM,iBAAiD;MAAA;MAC9D,IAAMC,aAAa,GAAGC,KAAK,CAACC,OAAO,CAACH,iBAAiB,CAAC,GAClDA,iBAAiB,CAACI,GAAG,CAAC,cAAI;QAAA,OAAIC,IAAI,CAACC,IAAI;MAAA,EAAC,GACxCC,MAAM,CAACC,IAAI,CAACR,iBAAiB,CAAC;MAElCpB,IAAI,CAAC,YAAK;QACR,IAAM6B,gBAAgB,GAAGtB,GAAG,CAAC,CAAC,EAAE,MAAI,CAACW,QAAQ,CAAC;QAC9C,IAAMY,EAAE,GACJ3B,GAAG,CAAC,CAAC,MAAI,CAACQ,YAAY,EAAET,GAAG,CAACG,GAAG,CAAC,MAAI,CAACW,SAAS,EAAE,MAAI,CAACD,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC;QAEpEM,aAAa,CAACU,OAAO,CAAC,UAACL,IAAI,EAAEM,CAAC,EAAI;UAChC,IAAMC,KAAK,GAAGnC,MAAM,CAACoC,mBAAmB,CAACR,IAAI,CAAC;UAC9C,IAAMS,SAAS,GAAG,KAAK;UACvB,IAAI,MAAI,CAACC,sBAAsB,CAACJ,CAAC,CAAC,IAAI,IAAI,EAAE;YAC1C,MAAI,CAACI,sBAAsB,CAACJ,CAAC,CAAC,GAAG;cAC/BK,YAAY,YAAKX,IAAI,OAAI;cACzBT,QAAQ,EAAET,SAAS,CAACyB,KAAK,CAAC,CAAChB,QAAQ,CAACkB,SAAS;aAC9C;;UAEH,IAAI,MAAI,CAACG,0BAA0B,CAACN,CAAC,CAAC,IAAI,IAAI,EAAE;YAC9C,MAAI,CAACM,0BAA0B,CAACN,CAAC,CAAC,GAAG;cACnCK,YAAY,YAAKX,IAAI,OAAI;cACzBT,QAAQ,EAAET,SAAS,CAACyB,KAAK,CAAC,CAAChB,QAAQ,CAACkB,SAAS;aAC9C;;UAGH,IAAMI,QAAQ,GAAGjB,KAAK,CAACC,OAAO,CAACH,iBAAiB,CAAC,GAC7CA,iBAAiB,CAACY,CAAC,CAAC,CAACQ,MAAM,GAC3BpB,iBAAiB,CAACM,IAAI,CAAC;UAC3B,IAAIa,QAAQ,IAAI,IAAI,EAAE;YACpB;;UAGF,IAAME,WAAW,GAAG,MAAI,CAACL,sBAAsB,CAACJ,CAAC,CAAC,CAACf,QAAQ;UAC3D,IAAMyB,eAAe,GAAG,MAAI,CAACJ,0BAA0B,CAACN,CAAC,CAAC,CAACf,QAAQ;UAEnE,IAAM0B,cAAc,GAChBzC,GAAG,CAACG,GAAG,CAACoC,WAAW,EAAE,MAAI,CAAC7B,KAAK,CAAC,EAAEP,GAAG,CAACkC,QAAQ,EAAE,CAAC,GAAG,MAAI,CAAC3B,KAAK,CAAC,CAAC;UAEpE,IAAMgC,GAAG,GAAGvC,GAAG,CAACqC,eAAe,EAAE,MAAI,CAAC7B,KAAK,CAAC;UAC5C,IAAMgC,GAAG,GAAG5C,GAAG,CAACsC,QAAQ,CAAC;UAEzB,IAAMO,kBAAkB,GAAG1C,OAAO,CAACwC,GAAG,EAAEC,GAAG,CAAC;UAE5CJ,WAAW,CAACM,MAAM,CAACJ,cAAc,CAAC;UAClCD,eAAe,CAACK,MAAM,CAACD,kBAAkB,CAAC;UAE1C,IAAME,QAAQ,GACV9C,GAAG,CAACG,GAAG,CAACF,GAAG,CAAC2B,EAAE,EAAED,gBAAgB,CAAC,EACzB1B,GAAG,CAACwC,cAAc,EAAEzC,GAAG,CAAC4C,kBAAkB,EAAE,MAAI,CAAChC,OAAO,CAAC,CAAC,CAAC,EAC/DmB,KAAK,CAAC;UAEdA,KAAK,CAACc,MAAM,CAACC,QAAQ,CAAC;QACxB,CAAC,CAAC;QAEF,MAAI,CAAChC,SAAS,CAAC+B,MAAM,CAAC7C,GAAG,CAAC,MAAI,CAACc,SAAS,EAAE,CAAC,CAAC,CAAC;QAC7C,MAAI,CAACE,QAAQ,CAAC6B,MAAM,CAAC1C,GAAG,CAAC,MAAI,CAACa,QAAQ,EAAE,MAAI,CAACN,KAAK,CAAC,CAAC;MACtD,CAAC,CAAC;MACF,IAAI,CAACqC,mBAAmB,EAAE;IAC5B;EAAC;IAAA;IAAA,OAEQ,mBAAO;MACd,IAAI,CAAC/B,QAAQ,CAACnB,OAAO,EAAE;MACvB,IAAI,CAACiB,SAAS,CAACjB,OAAO,EAAE;MAExB,IAAI,IAAI,CAACqC,sBAAsB,IAAI,IAAI,EAAE;QACvCrC,QAAO,CAAC,IAAI,CAACqC,sBAAsB,CAACZ,GAAG,CAAC,WAAC;UAAA,OAAI0B,CAAC,CAACjC,QAAQ;QAAA,EAAC,CAAC;;MAE3D,IAAI,IAAI,CAACqB,0BAA0B,IAAI,IAAI,EAAE;QAC3CvC,QAAO,CAAC,IAAI,CAACuC,0BAA0B,CAACd,GAAG,CAAC,WAAC;UAAA,OAAI0B,CAAC,CAACjC,QAAQ;QAAA,EAAC,CAAC;;IAEjE;EAAC;IAAA;IAAA;MAAA,6EAEQ;QAAA;UAAA;YAAA;cAAA,MACD,IAAIkC,KAAK,CAAC,iDAAiD,CAAC;YAAA;YAAA;cAAA;UAAA;QAAA;MAAA,CACnE;MAAA;QAAA;MAAA;MAAA;IAAA;EAAA;IAAA;IAAA;MAAA,6EAEQ,kBAAiBC,YAA2B;QAAA;UAAA;YAAA;cAAA,MAC7C,IAAID,KAAK,CAAC,iDAAiD,CAAC;YAAA;YAAA;cAAA;UAAA;QAAA;MAAA,CACnE;MAAA;QAAA;MAAA;MAAA;IAAA;EAAA;IAAA;IAAA,OAED,qBAAS;MACP,OAAO;QACL,cAAc,EAAE,IAAI,CAACxC,YAAY;QACjC,OAAO,EAAE,IAAI,CAACC,KAAK;QACnB,OAAO,EAAE,IAAI,CAACC,KAAK;QACnB,SAAS,EAAE,IAAI,CAACC,OAAO;QACvB,OAAO,EAAE,IAAI,CAACC;OACf;IACH;IAEA;EAAA;IAAA;IAAA,KAvHA,eAAoB;MAClB;MACA;MACA;MACA,OAAO,QAAQ;IACjB;EAAC;IAAA;IAAA,OAmHD,oBACIsC,GAA+B,EAAEC,MAAkB;MACrD,OAAO,IAAID,GAAG,CACVC,MAAM,CAAC,cAAc,CAAC,EAAEA,MAAM,CAAC,OAAO,CAAC,EAAEA,MAAM,CAAC,OAAO,CAAC,EACxDA,MAAM,CAAC,SAAS,CAAC,EAAEA,MAAM,CAAC,OAAO,CAAC,CAAC;IACzC;EAAC;EAAA;AAAA,EA/HkC7C,SAAS","names":["ENGINE","dispose","tidy","abs","add","div","maximum","mul","scalar","sub","zerosLike","Optimizer","AdamaxOptimizer","learningRate","beta1","beta2","epsilon","decay","iteration","variable","accBeta1","backend","variableGradients","variableNames","Array","isArray","map","item","name","Object","keys","oneMinusAccBeta1","lr","forEach","i","value","registeredVariables","trainable","accumulatedFirstMoment","originalName","accumulatedWeightedInfNorm","gradient","tensor","firstMoment","weightedInfNorm","newFirstMoment","ut0","ut1","newWeightedInfNorm","assign","newValue","incrementIterations","v","Error","weightValues","cls","config"],"sources":["E:\\react-detect-toxicity-in-a-chat-app-youtube-2\\Toxic-Word-Checker\\node_modules\\@tensorflow\\tfjs-core\\src\\optimizers\\adamax_optimizer.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {dispose, tidy} from '../globals';\nimport {abs} from '../ops/abs';\nimport {add} from '../ops/add';\nimport {div} from '../ops/div';\nimport {maximum} from '../ops/maximum';\nimport {mul} from '../ops/mul';\nimport {scalar} from '../ops/scalar';\nimport {sub} from '../ops/sub';\nimport {zerosLike} from '../ops/zeros_like';\nimport {ConfigDict, Serializable, SerializableConstructor} from '../serialization';\nimport {Variable} from '../tensor';\nimport {NamedTensor, NamedVariableMap} from '../tensor_types';\n\nimport {Optimizer, OptimizerVariable} from './optimizer';\n\nexport class AdamaxOptimizer extends Optimizer {\n  /** @nocollapse */\n  static get className() {\n    // Name matters for Python compatibility.\n    // This is a getter instead of a property because when it's a property, it\n    // prevents the entire class from being tree-shaken.\n    return 'Adamax';\n  }\n  private accBeta1: Variable;\n  private iteration: Variable;\n\n  private accumulatedFirstMoment: OptimizerVariable[] = [];\n  private accumulatedWeightedInfNorm: OptimizerVariable[] = [];\n\n  constructor(\n      protected learningRate: number, protected beta1: number,\n      protected beta2: number, protected epsilon: number = null,\n      protected decay = 0.0) {\n    super();\n\n    tidy(() => {\n      this.iteration = scalar(0).variable();\n      this.accBeta1 = scalar(beta1).variable();\n    });\n\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n  }\n\n  applyGradients(variableGradients: NamedVariableMap|NamedTensor[]) {\n    const variableNames = Array.isArray(variableGradients) ?\n        variableGradients.map(item => item.name) :\n        Object.keys(variableGradients);\n\n    tidy(() => {\n      const oneMinusAccBeta1 = sub(1, this.accBeta1);\n      const lr =\n          div(-this.learningRate, add(mul(this.iteration, this.decay), 1));\n\n      variableNames.forEach((name, i) => {\n        const value = ENGINE.registeredVariables[name];\n        const trainable = false;\n        if (this.accumulatedFirstMoment[i] == null) {\n          this.accumulatedFirstMoment[i] = {\n            originalName: `${name}/m`,\n            variable: zerosLike(value).variable(trainable)\n          };\n        }\n        if (this.accumulatedWeightedInfNorm[i] == null) {\n          this.accumulatedWeightedInfNorm[i] = {\n            originalName: `${name}/v`,\n            variable: zerosLike(value).variable(trainable)\n          };\n        }\n\n        const gradient = Array.isArray(variableGradients) ?\n            variableGradients[i].tensor :\n            variableGradients[name];\n        if (gradient == null) {\n          return;\n        }\n\n        const firstMoment = this.accumulatedFirstMoment[i].variable;\n        const weightedInfNorm = this.accumulatedWeightedInfNorm[i].variable;\n\n        const newFirstMoment =\n            add(mul(firstMoment, this.beta1), mul(gradient, 1 - this.beta1));\n\n        const ut0 = mul(weightedInfNorm, this.beta2);\n        const ut1 = abs(gradient);\n\n        const newWeightedInfNorm = maximum(ut0, ut1);\n\n        firstMoment.assign(newFirstMoment);\n        weightedInfNorm.assign(newWeightedInfNorm);\n\n        const newValue =\n            add(mul(div(lr, oneMinusAccBeta1),\n                    div(newFirstMoment, add(newWeightedInfNorm, this.epsilon))),\n                value);\n\n        value.assign(newValue);\n      });\n\n      this.iteration.assign(add(this.iteration, 1));\n      this.accBeta1.assign(mul(this.accBeta1, this.beta1));\n    });\n    this.incrementIterations();\n  }\n\n  override dispose(): void {\n    this.accBeta1.dispose();\n    this.iteration.dispose();\n\n    if (this.accumulatedFirstMoment != null) {\n      dispose(this.accumulatedFirstMoment.map(v => v.variable));\n    }\n    if (this.accumulatedWeightedInfNorm != null) {\n      dispose(this.accumulatedWeightedInfNorm.map(v => v.variable));\n    }\n  }\n\n  override async getWeights(): Promise<NamedTensor[]> {\n    throw new Error('getWeights() is not implemented for Adamax yet.');\n  }\n\n  override async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    throw new Error('setWeights() is not implemented for Adamax yet.');\n  }\n\n  getConfig(): ConfigDict {\n    return {\n      'learningRate': this.learningRate,\n      'beta1': this.beta1,\n      'beta2': this.beta2,\n      'epsilon': this.epsilon,\n      'decay': this.decay\n    };\n  }\n\n  /** @nocollapse */\n  static override fromConfig<T extends Serializable>(\n      cls: SerializableConstructor<T>, config: ConfigDict): T {\n    return new cls(\n        config['learningRate'], config['beta1'], config['beta2'],\n        config['epsilon'], config['decay']);\n  }\n}\n"]},"metadata":{},"sourceType":"module","externalDependencies":[]}