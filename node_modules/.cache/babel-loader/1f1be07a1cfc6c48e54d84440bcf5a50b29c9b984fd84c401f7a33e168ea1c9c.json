{"ast":null,"code":"import _createForOfIteratorHelper from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createForOfIteratorHelper.js\";\nimport _regeneratorRuntime from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/regeneratorRuntime.js\";\nimport _asyncToGenerator from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\nimport _slicedToArray from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/slicedToArray.js\";\nimport _toConsumableArray from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/toConsumableArray.js\";\nimport _classCallCheck from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";\nimport _createClass from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/Toxic-Word-Checker/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createClass.js\";\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { env, keep, tidy, util } from '@tensorflow/tfjs-core';\nimport { getNodeNameAndIndex, getParamValue, getTensor, getTensorsForCurrentContenxt, parseNodeName } from '../operations/executors/utils';\nimport { executeOp } from '../operations/operation_executor';\nimport { ExecutionContext } from './execution_context';\nimport { getExecutionSubgraph, getNodesInTopologicalOrder, isControlFlow } from './model_analysis';\nexport var GraphExecutor = /*#__PURE__*/function () {\n  /**\n   *\n   * @param graph Graph the model or function graph to be executed.\n   * @param parent When building function exector you need to set the parent\n   * executor. Since the weights and function executor maps are set at parant\n   * level, that function executor can access the function maps and weight maps\n   * through the parent.\n   */\n  function GraphExecutor(graph, parent) {\n    var _this = this;\n    _classCallCheck(this, GraphExecutor);\n    this.graph = graph;\n    this.parent = parent;\n    this.compiledMap = new Map();\n    this._weightMap = {};\n    this.SEPERATOR = ',';\n    this._functions = {};\n    this._functionExecutorMap = {};\n    this.keepIntermediateTensors = false;\n    this._outputs = graph.outputs;\n    this._inputs = graph.inputs;\n    this._initNodes = graph.initNodes;\n    this._signature = graph.signature;\n    this._functions = graph.functions;\n    // create sub-graph executors\n    if (graph.functions != null) {\n      Object.keys(graph.functions).forEach(function (name) {\n        _this._functionExecutorMap[name] = new GraphExecutor(graph.functions[name], _this);\n      });\n    }\n  }\n  _createClass(GraphExecutor, [{\n    key: \"weightIds\",\n    get: function get() {\n      return this.parent ? this.parent.weightIds : this._weightIds;\n    }\n  }, {\n    key: \"functionExecutorMap\",\n    get: function get() {\n      return this.parent ? this.parent.functionExecutorMap : this._functionExecutorMap;\n    }\n  }, {\n    key: \"weightMap\",\n    get: function get() {\n      return this.parent ? this.parent.weightMap : this._weightMap;\n    },\n    set: function set(weightMap) {\n      var _ref;\n      var weightIds = Object.keys(weightMap).map(function (key) {\n        return weightMap[key].map(function (tensor) {\n          return tensor.id;\n        });\n      });\n      this._weightIds = (_ref = []).concat.apply(_ref, _toConsumableArray(weightIds));\n      this._weightMap = weightMap;\n    }\n    /**\n     * Set `ResourceManager` shared by executors of a model.\n     * @param resourceManager: `ResourceManager` of the `GraphModel`.\n     */\n  }, {\n    key: \"resourceManager\",\n    set: function set(resourceManager) {\n      this._resourceManager = resourceManager;\n    }\n  }, {\n    key: \"inputs\",\n    get: function get() {\n      return this._inputs.map(function (node) {\n        return {\n          name: node.name,\n          shape: node.attrParams['shape'] ? node.attrParams['shape'].value : undefined,\n          dtype: node.attrParams['dtype'] ? node.attrParams['dtype'].value : undefined\n        };\n      });\n    }\n  }, {\n    key: \"outputs\",\n    get: function get() {\n      return this._outputs.map(function (node) {\n        return {\n          name: node.name,\n          shape: node.attrParams['shape'] ? node.attrParams['shape'].value : undefined,\n          dtype: node.attrParams['dtype'] ? node.attrParams['dtype'].value : undefined\n        };\n      });\n    }\n  }, {\n    key: \"inputNodes\",\n    get: function get() {\n      return this._inputs.map(function (node) {\n        return node.signatureKey || node.name;\n      });\n    }\n  }, {\n    key: \"outputNodes\",\n    get: function get() {\n      return this._outputs.map(function (node) {\n        var name = node.signatureKey || node.name;\n        return node.defaultOutput ? \"\".concat(name, \":\").concat(node.defaultOutput) : name;\n      });\n    }\n  }, {\n    key: \"functions\",\n    get: function get() {\n      var _this2 = this;\n      return Object.keys(this._functions).reduce(function (map, key) {\n        map[key] = _this2._functions[key].signature;\n        return map;\n      }, {});\n    }\n  }, {\n    key: \"getCompilationKey\",\n    value: function getCompilationKey(inputs, outputs) {\n      var sortedInputs = inputs.map(function (node) {\n        return node.name;\n      }).sort();\n      var sortedOutputs = outputs.map(function (node) {\n        return node.name;\n      }).sort();\n      return sortedInputs.join(this.SEPERATOR) + '--' + sortedOutputs.join(this.SEPERATOR);\n    }\n    /**\n     * Compiles the inference graph and returns the minimal set of nodes that are\n     * required for execution, in the correct execution order.\n     */\n  }, {\n    key: \"compile\",\n    value: function compile(inputs, outputs) {\n      var executionInfo = getExecutionSubgraph(inputs, outputs, this.weightMap, this._initNodes);\n      var missingInputs = executionInfo.missingInputs,\n        dynamicNode = executionInfo.dynamicNode,\n        syncInputs = executionInfo.syncInputs;\n      if (dynamicNode != null) {\n        throw new Error(\"This execution contains the node '\".concat(dynamicNode.name, \"', which has \") + \"the dynamic op '\".concat(dynamicNode.op, \"'. Please use \") + \"model.executeAsync() instead. Alternatively, to avoid the \" + \"dynamic ops, specify the inputs [\".concat(syncInputs, \"]\"));\n      }\n      if (missingInputs.length > 0) {\n        var outNames = outputs.map(function (n) {\n          return n.name;\n        });\n        var inNames = Object.keys(inputs);\n        throw new Error(\"Cannot compute the outputs [\".concat(outNames, \"] from the provided inputs \") + \"[\".concat(inNames, \"]. Missing the following inputs: [\").concat(missingInputs, \"]\"));\n      }\n      return getNodesInTopologicalOrder(this.graph, this.weightMap, executionInfo);\n    }\n  }, {\n    key: \"cloneAndKeepTensor\",\n    value: function cloneAndKeepTensor(tensor) {\n      if (tensor == null) {\n        return null;\n      }\n      var clone = tensor.clone();\n      // Keep the clone because`model.execute()` may be called within\n      // a `tidy()`, but the user may inspect these tensors after the\n      // tidy.\n      keep(clone);\n      return clone;\n    }\n  }, {\n    key: \"cloneTensorList\",\n    value: function cloneTensorList(tensors) {\n      var _this3 = this;\n      if (!tensors) {\n        return null;\n      }\n      var clonedTensor = tensors.map(function (tensor) {\n        return _this3.cloneAndKeepTensor(tensor);\n      });\n      return clonedTensor;\n    }\n  }, {\n    key: \"cloneTensorMap\",\n    value: function cloneTensorMap(tensorsMap) {\n      var _this4 = this;\n      return Object.fromEntries(Object.entries(tensorsMap).map(function (_ref2) {\n        var _ref3 = _slicedToArray(_ref2, 2),\n          name = _ref3[0],\n          tensorsList = _ref3[1];\n        return [name, _this4.cloneTensorList(tensorsList)];\n      }));\n    }\n    /**\n     * Executes the inference for given input tensors.\n     * @param inputs Tensor map for the model inputs, keyed by the input node\n     * names.\n     * @param outputs Optional. output node name from the Tensorflow model, if\n     * no outputs are specified, the default outputs of the model would be used.\n     * You can inspect intermediate nodes of the model by adding them to the\n     * outputs array.\n     */\n  }, {\n    key: \"execute\",\n    value: function execute(inputs, outputs) {\n      var _this5 = this;\n      // Dispose any tensors from a prior run to avoid leaking them.\n      this.disposeIntermediateTensors();\n      inputs = this.mapInputs(inputs);\n      var names = Object.keys(inputs).sort();\n      this.checkInputs(inputs);\n      this.checkInputShapeAndType(inputs);\n      outputs = this.mapOutputs(outputs);\n      this.checkOutputs(outputs);\n      var inputNodes = names.map(function (name) {\n        return _this5.graph.nodes[parseNodeName(name)[0]];\n      });\n      var outputNodeNames = outputs.map(function (name) {\n        return parseNodeName(name)[0];\n      });\n      var outputNodes = outputNodeNames.map(function (name) {\n        return _this5.graph.nodes[name];\n      });\n      // If no outputs are specified, then use the default outputs of the model.\n      if (outputNodes.length === 0) {\n        outputNodes = this._outputs;\n      }\n      var compilationKey = this.getCompilationKey(inputNodes, outputNodes);\n      // Do nothing if the compiled graph cache contains the input.\n      var orderedNodes = this.compiledMap.get(compilationKey);\n      if (orderedNodes == null) {\n        orderedNodes = this.compile(inputs, outputNodes);\n        this.compiledMap.set(compilationKey, orderedNodes);\n      }\n      // Keep tensors if KEEP_INTERMEDIATE_TENSORS is on.\n      try {\n        this.keepIntermediateTensors = env().getBool('KEEP_INTERMEDIATE_TENSORS');\n      } catch (e) {\n        this.keepIntermediateTensors = false;\n        console.warn(e.message);\n      }\n      var tensorArrayMap = {};\n      var tensorListMap = {};\n      return tidy(function () {\n        var context = new ExecutionContext(_this5.weightMap, tensorArrayMap, tensorListMap, _this5.functionExecutorMap);\n        var tensorsMap = Object.assign({}, _this5.weightMap);\n        if (_this5.keepIntermediateTensors) {\n          _this5.clonedTensorsMap = _this5.cloneTensorMap(_this5.weightMap);\n        }\n        Object.keys(inputs).forEach(function (name) {\n          var _parseNodeName = parseNodeName(name),\n            _parseNodeName2 = _slicedToArray(_parseNodeName, 2),\n            nodeName = _parseNodeName2[0],\n            index = _parseNodeName2[1];\n          var tensors = [];\n          tensors[index] = inputs[name];\n          tensorsMap[nodeName] = tensors;\n          if (_this5.keepIntermediateTensors) {\n            _this5.clonedTensorsMap[nodeName] = _this5.cloneTensorList(tensors);\n          }\n        });\n        var tensorsToKeep = _this5.getFrozenTensorIds(tensorsMap);\n        var intermediateTensorConsumerCount = {};\n        for (var i = 0; i < orderedNodes.length; i++) {\n          var node = orderedNodes[i];\n          if (!tensorsMap[node.name]) {\n            var tensors = executeOp(node, tensorsMap, context, _this5._resourceManager);\n            if (util.isPromise(tensors)) {\n              throw new Error(\"The execution of the op '\".concat(node.op, \"' returned a promise. \") + \"Please use model.executeAsync() instead.\");\n            }\n            tensorsMap[node.name] = tensors;\n            if (_this5.keepIntermediateTensors) {\n              _this5.clonedTensorsMap[node.name] = _this5.cloneTensorList(tensors);\n            }\n            _this5.checkTensorForDisposal(node.name, node, tensorsMap, context, tensorsToKeep, outputNodeNames, intermediateTensorConsumerCount);\n          }\n        }\n        // dispose the context for the root executor\n        if (_this5.parent == null) {\n          context.dispose(tensorsToKeep);\n        }\n        return outputs.map(function (name) {\n          return getTensor(name, tensorsMap, context);\n        });\n      });\n    }\n  }, {\n    key: \"getFrozenTensorIds\",\n    value: function getFrozenTensorIds(tensorMap) {\n      var ids = [].concat.apply([], Object.keys(tensorMap).map(function (key) {\n        return tensorMap[key];\n      }).map(function (tensors) {\n        return tensors.map(function (tensor) {\n          return tensor.id;\n        });\n      }));\n      return new Set(ids);\n    }\n  }, {\n    key: \"checkTensorForDisposal\",\n    value: function checkTensorForDisposal(nodeName, node, tensorMap, context, tensorsToKeep, outputNames, intermediateTensorConsumerCount) {\n      // Skip output nodes and any control flow nodes, since its dependency is\n      // tricky to track correctly.\n      if (node.category === 'control' || outputNames.indexOf(nodeName) !== -1) {\n        return;\n      }\n      tensorMap[nodeName].forEach(function (tensor) {\n        if (tensor != null) {\n          intermediateTensorConsumerCount[tensor.id] = (intermediateTensorConsumerCount[tensor.id] || 0) + node.children.length;\n        }\n      });\n      node.inputs.forEach(function (input) {\n        // Skip any control flow nodes, since its dependency is tricky to track\n        // correctly.\n        if (input.category !== 'control') {\n          var tensors = getTensorsForCurrentContenxt(input.name, tensorMap, context);\n          if (tensors != null) {\n            tensors.forEach(function (tensor) {\n              if (tensor && !tensor.kept && !tensorsToKeep.has(tensor.id)) {\n                var count = intermediateTensorConsumerCount[tensor.id];\n                if (count === 1) {\n                  tensor.dispose();\n                  delete intermediateTensorConsumerCount[tensor.id];\n                } else if (count != null) {\n                  // only intermediate nodes has count set, inputs and weights\n                  // are not.\n                  intermediateTensorConsumerCount[tensor.id]--;\n                }\n              }\n            });\n          }\n        }\n      });\n    }\n    /**\n     * Executes the inference for given input tensors in Async fashion.\n     * @param inputs Tensor map for the model inputs, keyed by the input node\n     * names.\n     * @param outputs output node name from the Tensorflow model, if no outputs\n     * are specified, the default outputs of the model would be used. You can\n     * inspect intermediate nodes of the model by adding them to the outputs\n     * array.\n     */\n  }, {\n    key: \"executeAsync\",\n    value: function () {\n      var _executeAsync2 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee(inputs, outputs) {\n        return _regeneratorRuntime().wrap(function _callee$(_context) {\n          while (1) switch (_context.prev = _context.next) {\n            case 0:\n              return _context.abrupt(\"return\", this._executeAsync(inputs, outputs));\n            case 1:\n            case \"end\":\n              return _context.stop();\n          }\n        }, _callee, this);\n      }));\n      function executeAsync(_x, _x2) {\n        return _executeAsync2.apply(this, arguments);\n      }\n      return executeAsync;\n    }()\n  }, {\n    key: \"disposeIntermediateTensors\",\n    value: function disposeIntermediateTensors() {\n      if (!this.clonedTensorsMap) {\n        return;\n      }\n      Object.values(this.clonedTensorsMap).forEach(function (tensorsList) {\n        var _iterator = _createForOfIteratorHelper(tensorsList),\n          _step;\n        try {\n          for (_iterator.s(); !(_step = _iterator.n()).done;) {\n            var tensor = _step.value;\n            if (tensor && !tensor.isDisposed) {\n              tensor.dispose();\n            }\n          }\n        } catch (err) {\n          _iterator.e(err);\n        } finally {\n          _iterator.f();\n        }\n      });\n      this.clonedTensorsMap = null;\n    }\n  }, {\n    key: \"getIntermediateTensors\",\n    value: function getIntermediateTensors() {\n      return this.clonedTensorsMap;\n    }\n    /**\n     * Executes the inference for given input tensors in Async fashion.\n     * @param inputs Tensor map for the model inputs, keyed by the input node\n     * names.\n     * @param outputs Optional. output node name from the Tensorflow model,\n     * if no outputs are specified, the default outputs of the model would be\n     * used. You can inspect intermediate nodes of the model by adding them to\n     * the outputs array.\n     * @param isFunctionExecution Optional. Flag for executing a function.\n     * @param tensorArrayMap Optional, global TensorArray map by id. Used for\n     * function execution.\n     * @param tensorArrayMap Optinal global TensorList map by id. Used for\n     * function execution.\n     */\n  }, {\n    key: \"_executeAsync\",\n    value: function () {\n      var _executeAsync3 = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee2(inputs, outputs) {\n        var isFunctionExecution,\n          tensorArrayMap,\n          tensorListMap,\n          context,\n          tensorsMap,\n          results,\n          outputIds,\n          inputIds,\n          keepIds,\n          _args2 = arguments;\n        return _regeneratorRuntime().wrap(function _callee2$(_context2) {\n          while (1) switch (_context2.prev = _context2.next) {\n            case 0:\n              isFunctionExecution = _args2.length > 2 && _args2[2] !== undefined ? _args2[2] : false;\n              tensorArrayMap = _args2.length > 3 && _args2[3] !== undefined ? _args2[3] : {};\n              tensorListMap = _args2.length > 4 && _args2[4] !== undefined ? _args2[4] : {};\n              // Dispose any tensors from a prior run to avoid leaking them.\n              this.disposeIntermediateTensors();\n              if (!isFunctionExecution) {\n                inputs = this.mapInputs(inputs);\n                this.checkInputs(inputs);\n                this.checkInputShapeAndType(inputs);\n                outputs = this.mapOutputs(outputs);\n                this.checkOutputs(outputs);\n              }\n              // Keep tensors if KEEP_INTERMEDIATE_TENSORS is on.\n              try {\n                this.keepIntermediateTensors = env().getBool('KEEP_INTERMEDIATE_TENSORS');\n              } catch (e) {\n                this.keepIntermediateTensors = false;\n                console.warn(e.message);\n              }\n              context = new ExecutionContext(this.weightMap, tensorArrayMap, tensorListMap, this.functionExecutorMap);\n              if (this.keepIntermediateTensors) {\n                this.clonedTensorsMap = this.cloneTensorMap(this.weightMap);\n              }\n              // Graph with control flow op requires runtime evaluation of the execution\n              // order, while without control flow the execution order is pre-determined\n              // in the compile method.\n              _context2.next = 10;\n              return this.executeWithControlFlow(inputs, context, outputs, isFunctionExecution);\n            case 10:\n              tensorsMap = _context2.sent;\n              results = outputs.map(function (name) {\n                return getTensor(name, tensorsMap, context);\n              }); // dispose all the intermediate tensors\n              outputIds = results.map(function (t) {\n                return t.id;\n              });\n              inputIds = Object.keys(inputs).map(function (name) {\n                return inputs[name].id;\n              });\n              keepIds = new Set([].concat(_toConsumableArray(outputIds), _toConsumableArray(inputIds), _toConsumableArray(this.weightIds)));\n              Object.values(tensorsMap).forEach(function (tensorsList) {\n                tensorsList.forEach(function (tensor) {\n                  if (tensor && !tensor.isDisposed && !keepIds.has(tensor.id)) {\n                    tensor.dispose();\n                  }\n                });\n              });\n              // dispose the context for the root executor\n              if (this.parent == null) {\n                context.dispose(keepIds);\n              }\n              return _context2.abrupt(\"return\", results);\n            case 18:\n            case \"end\":\n              return _context2.stop();\n          }\n        }, _callee2, this);\n      }));\n      function _executeAsync(_x3, _x4) {\n        return _executeAsync3.apply(this, arguments);\n      }\n      return _executeAsync;\n    }()\n  }, {\n    key: \"executeFunctionAsync\",\n    value: function () {\n      var _executeFunctionAsync = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee3(inputs, tensorArrayMap, tensorListMap) {\n        var _this6 = this;\n        var mappedInputs;\n        return _regeneratorRuntime().wrap(function _callee3$(_context3) {\n          while (1) switch (_context3.prev = _context3.next) {\n            case 0:\n              mappedInputs = inputs.reduce(function (map, tensor, index) {\n                map[_this6.inputs[index].name] = tensor;\n                return map;\n              }, {});\n              return _context3.abrupt(\"return\", this._executeAsync(mappedInputs, this.outputNodes, true, tensorArrayMap, tensorListMap));\n            case 2:\n            case \"end\":\n              return _context3.stop();\n          }\n        }, _callee3, this);\n      }));\n      function executeFunctionAsync(_x5, _x6, _x7) {\n        return _executeFunctionAsync.apply(this, arguments);\n      }\n      return executeFunctionAsync;\n    }()\n    /**\n     * When there are control flow nodes in the graph, the graph execution use\n     * ExecutionContext to keep track of the frames and loop iterators.\n     * @param inputs placeholder tensors for the graph.\n     * @param context the execution context object for current execution.\n     * @param outputNames Optional. output node name from the Tensorflow model,\n     * if no outputs are specified, the default outputs of the model would be\n     * used. You can inspect intermediate nodes of the model by adding them to\n     * the outputs array.\n     * @param isFunctionExecution Flag for executing a function.\n     */\n  }, {\n    key: \"executeWithControlFlow\",\n    value: function () {\n      var _executeWithControlFlow = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee4(inputs, context, outputNames, isFunctionExecution) {\n        var _this7 = this;\n        var names, inputNodes, outputNodeNames, outputNodes, _getExecutionSubgraph, usedNodes, missingInputs, dynamicNode, syncInputs, stack, tensorsMap, intermediateTensorConsumerCount, tensorsToKeep, added, promises, missingOutputs, alternativeMsg;\n        return _regeneratorRuntime().wrap(function _callee4$(_context4) {\n          while (1) switch (_context4.prev = _context4.next) {\n            case 0:\n              names = Object.keys(inputs);\n              inputNodes = names.map(function (name) {\n                return _this7.graph.nodes[parseNodeName(name)[0]];\n              });\n              outputNodeNames = outputNames.map(function (name) {\n                return parseNodeName(name)[0];\n              });\n              outputNodes = outputNodeNames.map(function (name) {\n                return _this7.graph.nodes[name];\n              }); // If no outputs are specified, then use the default outputs of the model.\n              if (outputNodes.length === 0) {\n                outputNodes = this._outputs;\n              }\n              _getExecutionSubgraph = getExecutionSubgraph(inputs, outputNodes, this.weightMap, this._initNodes), usedNodes = _getExecutionSubgraph.usedNodes, missingInputs = _getExecutionSubgraph.missingInputs, dynamicNode = _getExecutionSubgraph.dynamicNode, syncInputs = _getExecutionSubgraph.syncInputs; // First nodes to execute include inputNodes, weights, and initNodes.\n              stack = [].concat(_toConsumableArray(inputNodes), _toConsumableArray(this.graph.weights), _toConsumableArray(this._initNodes || [])).map(function (node) {\n                return {\n                  node: node,\n                  contexts: context.currentContext\n                };\n              });\n              tensorsMap = Object.assign({}, this.weightMap);\n              Object.keys(inputs).forEach(function (name) {\n                var _parseNodeName3 = parseNodeName(name),\n                  _parseNodeName4 = _slicedToArray(_parseNodeName3, 2),\n                  nodeName = _parseNodeName4[0],\n                  index = _parseNodeName4[1];\n                var tensors = [];\n                tensors[index] = inputs[name];\n                tensorsMap[nodeName] = tensors;\n              });\n              intermediateTensorConsumerCount = {};\n              tensorsToKeep = this.getFrozenTensorIds(tensorsMap);\n              added = {};\n            case 12:\n              if (!(stack.length > 0)) {\n                _context4.next = 18;\n                break;\n              }\n              promises = this.processStack(inputNodes, stack, context, tensorsMap, added, tensorsToKeep, outputNodeNames, intermediateTensorConsumerCount, usedNodes);\n              _context4.next = 16;\n              return Promise.all(promises);\n            case 16:\n              _context4.next = 12;\n              break;\n            case 18:\n              if (dynamicNode == null && !isFunctionExecution) {\n                console.warn(\"This model execution did not contain any nodes with control flow \" + \"or dynamic output shapes. You can use model.execute() instead.\");\n              }\n              missingOutputs = outputNodes.filter(function (node) {\n                return !isControlFlow(node) && !getTensor(node.name, tensorsMap, context);\n              }).map(function (node) {\n                return node.name;\n              });\n              if (!(missingOutputs.length > 0)) {\n                _context4.next = 24;\n                break;\n              }\n              alternativeMsg = '';\n              if (dynamicNode != null) {\n                alternativeMsg = \"Alternatively, to avoid the dynamic ops, use model.execute() \" + \"and specify the inputs [\".concat(syncInputs, \"]\");\n              }\n              throw new Error(\"Cannot compute the outputs [\".concat(missingOutputs, \"] from the provided \") + \"inputs [\".concat(names, \"]. Consider providing the following inputs: \") + \"[\".concat(missingInputs, \"]. \").concat(alternativeMsg));\n            case 24:\n              return _context4.abrupt(\"return\", tensorsMap);\n            case 25:\n            case \"end\":\n              return _context4.stop();\n          }\n        }, _callee4, this);\n      }));\n      function executeWithControlFlow(_x8, _x9, _x10, _x11) {\n        return _executeWithControlFlow.apply(this, arguments);\n      }\n      return executeWithControlFlow;\n    }()\n  }, {\n    key: \"processStack\",\n    value: function processStack(inputNodes, stack, context, tensorMap, added, tensorsToKeep, outputNames, intermediateTensorConsumerCount, usedNodes) {\n      var _this8 = this;\n      var promises = [];\n      var _loop = function _loop() {\n        var item = stack.pop();\n        context.currentContext = item.contexts;\n        var nodeName = '';\n        // The tensor of the Enter op with isConstant set should be set\n        // in the parent scope, so it will be available as constant for the\n        // whole loop.\n        if (item.node.op === 'Enter' && getParamValue('isConstant', item.node, tensorMap, context)) {\n          var _getNodeNameAndIndex = getNodeNameAndIndex(item.node.name, context);\n          var _getNodeNameAndIndex2 = _slicedToArray(_getNodeNameAndIndex, 1);\n          nodeName = _getNodeNameAndIndex2[0];\n        }\n        // only process nodes that are not in the tensorMap yet, this include\n        // inputNodes and internal initNodes.\n        if (tensorMap[item.node.name] == null) {\n          var tensors = executeOp(item.node, tensorMap, context, _this8._resourceManager);\n          if (!nodeName) {\n            var _getNodeNameAndIndex3 = getNodeNameAndIndex(item.node.name, context);\n            var _getNodeNameAndIndex4 = _slicedToArray(_getNodeNameAndIndex3, 1);\n            nodeName = _getNodeNameAndIndex4[0];\n          }\n          var currentContext = context.currentContext;\n          if (util.isPromise(tensors)) {\n            promises.push(tensors.then(function (t) {\n              tensorMap[nodeName] = t;\n              if (_this8.keepIntermediateTensors) {\n                _this8.clonedTensorsMap[nodeName] = _this8.cloneTensorList(t);\n              }\n              context.currentContext = currentContext;\n              _this8.checkTensorForDisposal(nodeName, item.node, tensorMap, context, tensorsToKeep, outputNames, intermediateTensorConsumerCount);\n              _this8.processChildNodes(item.node, stack, context, tensorMap, added, usedNodes);\n              return t;\n            }));\n          } else {\n            tensorMap[nodeName] = tensors;\n            if (_this8.keepIntermediateTensors) {\n              _this8.clonedTensorsMap[nodeName] = _this8.cloneTensorList(tensors);\n            }\n            _this8.checkTensorForDisposal(nodeName, item.node, tensorMap, context, tensorsToKeep, outputNames, intermediateTensorConsumerCount);\n            _this8.processChildNodes(item.node, stack, context, tensorMap, added, usedNodes);\n          }\n        } else {\n          _this8.processChildNodes(item.node, stack, context, tensorMap, added, usedNodes);\n        }\n      };\n      while (stack.length > 0) {\n        _loop();\n      }\n      return promises;\n    }\n  }, {\n    key: \"processChildNodes\",\n    value: function processChildNodes(node, stack, context, tensorMap, added, usedNodes) {\n      node.children.forEach(function (childNode) {\n        var _getNodeNameAndIndex5 = getNodeNameAndIndex(childNode.name, context),\n          _getNodeNameAndIndex6 = _slicedToArray(_getNodeNameAndIndex5, 1),\n          nodeName = _getNodeNameAndIndex6[0];\n        if (added[nodeName] || !usedNodes.has(childNode.name)) {\n          return;\n        }\n        // Merge op can be pushed if any of its inputs has value.\n        if (childNode.op === 'Merge') {\n          if (childNode.inputNames.some(function (name) {\n            return !!getTensor(name, tensorMap, context);\n          })) {\n            added[nodeName] = true;\n            stack.push({\n              contexts: context.currentContext,\n              node: childNode\n            });\n          }\n        } else\n          // Otherwise all inputs must to have value.\n          if (childNode.inputNames.every(function (name) {\n            return !!getTensor(name, tensorMap, context);\n          })) {\n            added[nodeName] = true;\n            stack.push({\n              contexts: context.currentContext,\n              node: childNode\n            });\n          }\n      });\n    }\n    /**\n     * Releases the memory used by the weight tensors.\n     */\n  }, {\n    key: \"dispose\",\n    value: function dispose() {\n      var _this9 = this;\n      Object.keys(this.weightMap).forEach(function (key) {\n        return _this9.weightMap[key].forEach(function (tensor) {\n          return tensor.dispose();\n        });\n      });\n    }\n  }, {\n    key: \"checkInputShapeAndType\",\n    value: function checkInputShapeAndType(inputs) {\n      var _this10 = this;\n      Object.keys(inputs).forEach(function (name) {\n        var input = inputs[name];\n        var _parseNodeName5 = parseNodeName(name),\n          _parseNodeName6 = _slicedToArray(_parseNodeName5, 1),\n          nodeName = _parseNodeName6[0];\n        var node = _this10.graph.nodes[nodeName];\n        if (node.attrParams['shape'] && node.attrParams['shape'].value) {\n          var shape = node.attrParams['shape'].value;\n          var match = shape.length === input.shape.length && input.shape.every(function (dim, index) {\n            return shape[index] === -1 || shape[index] === dim;\n          });\n          util.assert(match, function () {\n            return \"The shape of dict['\".concat(node.name, \"'] provided in \") + \"model.execute(dict) must be [\".concat(shape, \"], but was \") + \"[\".concat(input.shape, \"]\");\n          });\n        }\n        if (node.attrParams['dtype'] && node.attrParams['dtype'].value) {\n          util.assert(input.dtype === node.attrParams['dtype'].value, function () {\n            return \"The dtype of dict['\".concat(node.name, \"'] provided in \") + \"model.execute(dict) must be \" + \"\".concat(node.attrParams['dtype'].value, \", but was \").concat(input.dtype);\n          });\n        }\n      });\n    }\n  }, {\n    key: \"mapInputs\",\n    value: function mapInputs(inputs) {\n      var _a, _b;\n      var result = {};\n      for (var inputName in inputs) {\n        var tensor = (_b = (_a = this._signature) === null || _a === void 0 ? void 0 : _a.inputs) === null || _b === void 0 ? void 0 : _b[inputName];\n        if (tensor != null) {\n          result[tensor.name] = inputs[inputName];\n        } else {\n          result[inputName] = inputs[inputName];\n        }\n      }\n      return result;\n    }\n  }, {\n    key: \"checkInputs\",\n    value: function checkInputs(inputs) {\n      var _this11 = this;\n      var notInGraph = Object.keys(inputs).filter(function (name) {\n        var _parseNodeName7 = parseNodeName(name),\n          _parseNodeName8 = _slicedToArray(_parseNodeName7, 1),\n          nodeName = _parseNodeName8[0];\n        return _this11.graph.nodes[nodeName] == null;\n      });\n      if (notInGraph.length > 0) {\n        throw new Error(\"The dict provided in model.execute(dict) has \" + \"keys: [\".concat(notInGraph, \"] that are not part of graph\"));\n      }\n    }\n  }, {\n    key: \"mapOutputs\",\n    value: function mapOutputs(outputs) {\n      var _this12 = this;\n      return outputs.map(function (name) {\n        var _a, _b;\n        var tensor = (_b = (_a = _this12._signature) === null || _a === void 0 ? void 0 : _a.outputs) === null || _b === void 0 ? void 0 : _b[name];\n        if (tensor != null) {\n          return tensor.name;\n        }\n        return name;\n      }, {});\n    }\n  }, {\n    key: \"checkOutputs\",\n    value: function checkOutputs(outputs) {\n      var _this13 = this;\n      outputs.forEach(function (name) {\n        var _parseNodeName9 = parseNodeName(name),\n          _parseNodeName10 = _slicedToArray(_parseNodeName9, 1),\n          normalizedName = _parseNodeName10[0];\n        if (!_this13.graph.nodes[normalizedName]) {\n          throw new Error(\"The output '\".concat(name, \"' is not found in the graph\"));\n        }\n      });\n    }\n  }]);\n  return GraphExecutor;\n}();","map":{"version":3,"mappings":";;;;;;;AAAA;;;;;;;;;;;;;;;;AAiBA,SAAkBA,GAAG,EAAEC,IAAI,EAA0BC,IAAI,EAAEC,IAAI,QAAO,uBAAuB;AAI7F,SAAQC,mBAAmB,EAAEC,aAAa,EAAEC,SAAS,EAAEC,4BAA4B,EAAEC,aAAa,QAAO,+BAA+B;AACxI,SAAQC,SAAS,QAAO,kCAAkC;AAG1D,SAAQC,gBAAgB,QAA6B,qBAAqB;AAC1E,SAAQC,oBAAoB,EAAEC,0BAA0B,EAAEC,aAAa,QAAO,kBAAkB;AAShG,WAAaC,aAAa;EAyFxB;;;;;;;;EAQA,uBAAoBC,KAAY,EAAUC,MAAsB;IAAA;IAAA;IAA5C,UAAK,GAALD,KAAK;IAAiB,WAAM,GAANC,MAAM;IAhGxC,gBAAW,GAAwB,IAAIC,GAAG,EAAE;IAC5C,eAAU,GAAoB,EAAE;IAMhC,cAAS,GAAG,GAAG;IACf,eAAU,GAA2B,EAAE;IACvC,yBAAoB,GAAsC,EAAE;IAG5D,4BAAuB,GAAG,KAAK;IAqFrC,IAAI,CAACC,QAAQ,GAAGH,KAAK,CAACI,OAAO;IAC7B,IAAI,CAACC,OAAO,GAAGL,KAAK,CAACM,MAAM;IAC3B,IAAI,CAACC,UAAU,GAAGP,KAAK,CAACQ,SAAS;IACjC,IAAI,CAACC,UAAU,GAAGT,KAAK,CAACU,SAAS;IACjC,IAAI,CAACC,UAAU,GAAGX,KAAK,CAACY,SAAS;IACjC;IACA,IAAIZ,KAAK,CAACY,SAAS,IAAI,IAAI,EAAE;MAC3BC,MAAM,CAACC,IAAI,CAACd,KAAK,CAACY,SAAS,CAAC,CAACG,OAAO,CAAC,cAAI,EAAG;QAC1C,KAAI,CAACC,oBAAoB,CAACC,IAAI,CAAC,GAC3B,IAAIlB,aAAa,CAACC,KAAK,CAACY,SAAS,CAACK,IAAI,CAAC,EAAE,KAAI,CAAC;MACpD,CAAC,CAAC;;EAEN;EAAC;IAAA;IAAA,KA/FD,eAAa;MACX,OAAO,IAAI,CAAChB,MAAM,GAAG,IAAI,CAACA,MAAM,CAACiB,SAAS,GAAG,IAAI,CAACC,UAAU;IAC9D;EAAC;IAAA;IAAA,KAED,eAAuB;MACrB,OAAO,IAAI,CAAClB,MAAM,GAAG,IAAI,CAACA,MAAM,CAACmB,mBAAmB,GAC/B,IAAI,CAACJ,oBAAoB;IAChD;EAAC;IAAA;IAAA,KAED,eAAa;MACX,OAAO,IAAI,CAACf,MAAM,GAAG,IAAI,CAACA,MAAM,CAACoB,SAAS,GAAG,IAAI,CAACC,UAAU;IAC9D,CAAC;IAAA,KAED,aAAcD,SAA0B;MAAA;MACtC,IAAMH,SAAS,GAAGL,MAAM,CAACC,IAAI,CAACO,SAAS,CAAC,CAACE,GAAG,CACxC,aAAG;QAAA,OAAIF,SAAS,CAACG,GAAG,CAAC,CAACD,GAAG,CAAC,gBAAM;UAAA,OAAIE,MAAM,CAACC,EAAE;QAAA,EAAC;MAAA,EAAC;MACnD,IAAI,CAACP,UAAU,GAAG,UAAE,EAACQ,MAAM,gCAAIT,SAAS,EAAC;MACzC,IAAI,CAACI,UAAU,GAAGD,SAAS;IAC7B;IAEA;;;;EAAA;IAAA;IAAA,KAIA,aAAoBO,eAAgC;MAClD,IAAI,CAACC,gBAAgB,GAAGD,eAAe;IACzC;EAAC;IAAA;IAAA,KAED,eAAU;MACR,OAAO,IAAI,CAACvB,OAAO,CAACkB,GAAG,CAAC,cAAI,EAAG;QAC7B,OAAO;UACLN,IAAI,EAAEa,IAAI,CAACb,IAAI;UACfc,KAAK,EAAED,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,GAC3BF,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,CAACC,KAAiB,GAC1CC,SAAS;UACbC,KAAK,EAAEL,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,GAC3BF,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,CAACC,KAAiB,GAC1CC;SACL;MACH,CAAC,CAAC;IACJ;EAAC;IAAA;IAAA,KAED,eAAW;MACT,OAAO,IAAI,CAAC/B,QAAQ,CAACoB,GAAG,CAAC,cAAI,EAAG;QAC9B,OAAO;UACLN,IAAI,EAAEa,IAAI,CAACb,IAAI;UACfc,KAAK,EAAED,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,GAC3BF,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,CAACC,KAAiB,GAC1CC,SAAS;UACbC,KAAK,EAAEL,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,GAC3BF,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,CAACC,KAAiB,GAC1CC;SACL;MACH,CAAC,CAAC;IACJ;EAAC;IAAA;IAAA,KAED,eAAc;MACZ,OAAO,IAAI,CAAC7B,OAAO,CAACkB,GAAG,CAAC,cAAI;QAAA,OAAIO,IAAI,CAACM,YAAY,IAAIN,IAAI,CAACb,IAAI;MAAA,EAAC;IACjE;EAAC;IAAA;IAAA,KAED,eAAe;MACb,OAAO,IAAI,CAACd,QAAQ,CAACoB,GAAG,CAAC,UAACO,IAAI,EAAI;QAChC,IAAMb,IAAI,GAAGa,IAAI,CAACM,YAAY,IAAIN,IAAI,CAACb,IAAI;QAC3C,OAAOa,IAAI,CAACO,aAAa,aAAOpB,IAAI,cAAIa,IAAI,CAACO,aAAa,IAAMpB,IAAI;MACtE,CAAC,CAAC;IACJ;EAAC;IAAA;IAAA,KAED,eAAa;MAAA;MACX,OAAOJ,MAAM,CAACC,IAAI,CAAC,IAAI,CAACH,UAAU,CAAC,CAAC2B,MAAM,CAAC,UAACf,GAAG,EAAEC,GAAG,EAAI;QACtDD,GAAG,CAACC,GAAG,CAAC,GAAG,MAAI,CAACb,UAAU,CAACa,GAAG,CAAC,CAACd,SAAS;QACzC,OAAOa,GAAG;MACZ,CAAC,EAAE,EAAoC,CAAC;IAC1C;EAAC;IAAA;IAAA,OAyBO,2BAAkBjB,MAAc,EAAEF,OAAe;MACvD,IAAMmC,YAAY,GAAGjC,MAAM,CAACiB,GAAG,CAAC,cAAI;QAAA,OAAIO,IAAI,CAACb,IAAI;MAAA,EAAC,CAACuB,IAAI,EAAE;MACzD,IAAMC,aAAa,GAAGrC,OAAO,CAACmB,GAAG,CAAC,cAAI;QAAA,OAAIO,IAAI,CAACb,IAAI;MAAA,EAAC,CAACuB,IAAI,EAAE;MAC3D,OAAOD,YAAY,CAACG,IAAI,CAAC,IAAI,CAACC,SAAS,CAAC,GAAG,IAAI,GAC3CF,aAAa,CAACC,IAAI,CAAC,IAAI,CAACC,SAAS,CAAC;IACxC;IAEA;;;;EAAA;IAAA;IAAA,OAIQ,iBAAQrC,MAAsB,EAAEF,OAAe;MACrD,IAAMwC,aAAa,GACfhD,oBAAoB,CAACU,MAAM,EAAEF,OAAO,EAAE,IAAI,CAACiB,SAAS,EAAE,IAAI,CAACd,UAAU,CAAC;MAC1E,IAAOsC,aAAa,GAA6BD,aAAa,CAAvDC,aAAa;QAAEC,WAAW,GAAgBF,aAAa,CAAxCE,WAAW;QAAEC,UAAU,GAAIH,aAAa,CAA3BG,UAAU;MAC7C,IAAID,WAAW,IAAI,IAAI,EAAE;QACvB,MAAM,IAAIE,KAAK,CACX,4CAAqCF,WAAW,CAAC7B,IAAI,+CAClC6B,WAAW,CAACG,EAAE,mBAAgB,+DACW,8CACxBF,UAAU,MAAG,CAAC;;MAGxD,IAAIF,aAAa,CAACK,MAAM,GAAG,CAAC,EAAE;QAC5B,IAAMC,QAAQ,GAAG/C,OAAO,CAACmB,GAAG,CAAC,WAAC;UAAA,OAAI6B,CAAC,CAACnC,IAAI;QAAA,EAAC;QACzC,IAAMoC,OAAO,GAAGxC,MAAM,CAACC,IAAI,CAACR,MAAM,CAAC;QACnC,MAAM,IAAI0C,KAAK,CACX,sCAA+BG,QAAQ,8CACnCE,OAAO,+CAAqCR,aAAa,MAAG,CAAC;;MAGvE,OAAOhD,0BAA0B,CAC7B,IAAI,CAACG,KAAK,EAAE,IAAI,CAACqB,SAAS,EAAEuB,aAAa,CAAC;IAChD;EAAC;IAAA;IAAA,OAEO,4BAAmBnB,MAAc;MACvC,IAAIA,MAAM,IAAI,IAAI,EAAE;QAClB,OAAO,IAAI;;MAEb,IAAM6B,KAAK,GAAG7B,MAAM,CAAC6B,KAAK,EAAE;MAC5B;MACA;MACA;MACApE,IAAI,CAACoE,KAAK,CAAC;MACX,OAAOA,KAAK;IACd;EAAC;IAAA;IAAA,OAEO,yBAAgBC,OAAiB;MAAA;MACvC,IAAI,CAACA,OAAO,EAAE;QACZ,OAAO,IAAI;;MAEb,IAAMC,YAAY,GAAGD,OAAO,CAAChC,GAAG,CAAC,gBAAM,EAAG;QACxC,OAAO,MAAI,CAACkC,kBAAkB,CAAChC,MAAM,CAAC;MACxC,CAAC,CAAC;MACF,OAAO+B,YAAY;IACrB;EAAC;IAAA;IAAA,OAEO,wBAAeE,UAA2B;MAAA;MAChD,OAAO7C,MAAM,CAAC8C,WAAW,CACrB9C,MAAM,CAAC+C,OAAO,CAACF,UAAU,CAAC,CAACnC,GAAG,CAAC,iBAAwB;QAAA;UAAtBN,IAAI;UAAE4C,WAAW;QAChD,OAAO,CAAC5C,IAAI,EAAE,MAAI,CAAC6C,eAAe,CAACD,WAAW,CAAC,CAAC;MAClD,CAAC,CAAC,CAAC;IACT;IAEA;;;;;;;;;EAAA;IAAA;IAAA,OASA,iBAAQvD,MAAsB,EAAEF,OAAkB;MAAA;MAChD;MACA,IAAI,CAAC2D,0BAA0B,EAAE;MACjCzD,MAAM,GAAG,IAAI,CAAC0D,SAAS,CAAC1D,MAAM,CAAC;MAC/B,IAAM2D,KAAK,GAAGpD,MAAM,CAACC,IAAI,CAACR,MAAM,CAAC,CAACkC,IAAI,EAAE;MACxC,IAAI,CAAC0B,WAAW,CAAC5D,MAAM,CAAC;MACxB,IAAI,CAAC6D,sBAAsB,CAAC7D,MAAM,CAAC;MACnCF,OAAO,GAAG,IAAI,CAACgE,UAAU,CAAChE,OAAO,CAAC;MAClC,IAAI,CAACiE,YAAY,CAACjE,OAAO,CAAC;MAC1B,IAAMkE,UAAU,GACZL,KAAK,CAAC1C,GAAG,CAAC,cAAI;QAAA,OAAI,MAAI,CAACvB,KAAK,CAACuE,KAAK,CAAC9E,aAAa,CAACwB,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC;MAAA,EAAC;MAC/D,IAAMuD,eAAe,GAAGpE,OAAO,CAACmB,GAAG,CAAC,cAAI;QAAA,OAAI9B,aAAa,CAACwB,IAAI,CAAC,CAAC,CAAC,CAAC;MAAA,EAAC;MACnE,IAAIwD,WAAW,GAAGD,eAAe,CAACjD,GAAG,CAAC,cAAI;QAAA,OAAI,MAAI,CAACvB,KAAK,CAACuE,KAAK,CAACtD,IAAI,CAAC;MAAA,EAAC;MACrE;MACA,IAAIwD,WAAW,CAACvB,MAAM,KAAK,CAAC,EAAE;QAC5BuB,WAAW,GAAG,IAAI,CAACtE,QAAQ;;MAG7B,IAAMuE,cAAc,GAAG,IAAI,CAACC,iBAAiB,CAACL,UAAU,EAAEG,WAAW,CAAC;MAEtE;MACA,IAAIG,YAAY,GAAG,IAAI,CAACC,WAAW,CAACC,GAAG,CAACJ,cAAc,CAAC;MACvD,IAAIE,YAAY,IAAI,IAAI,EAAE;QACxBA,YAAY,GAAG,IAAI,CAACG,OAAO,CAACzE,MAAM,EAAEmE,WAAW,CAAC;QAChD,IAAI,CAACI,WAAW,CAACG,GAAG,CAACN,cAAc,EAAEE,YAAY,CAAC;;MAGpD;MACA,IAAI;QACF,IAAI,CAACK,uBAAuB,GAAGhG,GAAG,EAAE,CAACiG,OAAO,CAAC,2BAA2B,CAAC;OAC1E,CAAC,OAAOC,CAAC,EAAE;QACV,IAAI,CAACF,uBAAuB,GAAG,KAAK;QACpCG,OAAO,CAACC,IAAI,CAACF,CAAC,CAACG,OAAO,CAAC;;MAEzB,IAAMC,cAAc,GAAmB,EAAE;MACzC,IAAMC,aAAa,GAAkB,EAAE;MAEvC,OAAOrG,IAAI,CAAC,YAAK;QACf,IAAMsG,OAAO,GAAG,IAAI9F,gBAAgB,CAChC,MAAI,CAAC0B,SAAS,EAAEkE,cAAc,EAAEC,aAAa,EAC7C,MAAI,CAACpE,mBAAmB,CAAC;QAC7B,IAAMsC,UAAU,qBAAwB,MAAI,CAACrC,SAAS,CAAC;QACvD,IAAI,MAAI,CAAC4D,uBAAuB,EAAE;UAChC,MAAI,CAACS,gBAAgB,GAAG,MAAI,CAACC,cAAc,CAAC,MAAI,CAACtE,SAAS,CAAC;;QAG7DR,MAAM,CAACC,IAAI,CAACR,MAAM,CAAC,CAACS,OAAO,CAAC,cAAI,EAAG;UACjC,qBAA0BtB,aAAa,CAACwB,IAAI,CAAC;YAAA;YAAtC2E,QAAQ;YAAEC,KAAK;UACtB,IAAMtC,OAAO,GAAa,EAAE;UAC5BA,OAAO,CAACsC,KAAK,CAAC,GAAGvF,MAAM,CAACW,IAAI,CAAC;UAC7ByC,UAAU,CAACkC,QAAQ,CAAC,GAAGrC,OAAO;UAC9B,IAAI,MAAI,CAAC0B,uBAAuB,EAAE;YAChC,MAAI,CAACS,gBAAgB,CAACE,QAAQ,CAAC,GAAG,MAAI,CAAC9B,eAAe,CAACP,OAAO,CAAC;;QAEnE,CAAC,CAAC;QAEF,IAAMuC,aAAa,GAAG,MAAI,CAACC,kBAAkB,CAACrC,UAAU,CAAC;QACzD,IAAMsC,+BAA+B,GAA4B,EAAE;QACnE,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGrB,YAAY,CAAC1B,MAAM,EAAE+C,CAAC,EAAE,EAAE;UAC5C,IAAMnE,IAAI,GAAG8C,YAAY,CAACqB,CAAC,CAAC;UAC5B,IAAI,CAACvC,UAAU,CAAC5B,IAAI,CAACb,IAAI,CAAC,EAAE;YAC1B,IAAMsC,OAAO,GACT7D,SAAS,CAACoC,IAAI,EAAE4B,UAAU,EAAE+B,OAAO,EAAE,MAAI,CAAC5D,gBAAgB,CAClD;YACZ,IAAIzC,IAAI,CAAC8G,SAAS,CAAC3C,OAAO,CAAC,EAAE;cAC3B,MAAM,IAAIP,KAAK,CACX,mCAA4BlB,IAAI,CAACmB,EAAE,wEACO,CAAC;;YAEjDS,UAAU,CAAC5B,IAAI,CAACb,IAAI,CAAC,GAAGsC,OAAO;YAC/B,IAAI,MAAI,CAAC0B,uBAAuB,EAAE;cAChC,MAAI,CAACS,gBAAgB,CAAC5D,IAAI,CAACb,IAAI,CAAC,GAAG,MAAI,CAAC6C,eAAe,CAACP,OAAO,CAAC;;YAElE,MAAI,CAAC4C,sBAAsB,CACvBrE,IAAI,CAACb,IAAI,EAAEa,IAAI,EAAE4B,UAAU,EAAE+B,OAAO,EAAEK,aAAa,EACnDtB,eAAe,EAAEwB,+BAA+B,CAAC;;;QAIzD;QACA,IAAI,MAAI,CAAC/F,MAAM,IAAI,IAAI,EAAE;UACvBwF,OAAO,CAACW,OAAO,CAACN,aAAa,CAAC;;QAGhC,OAAO1F,OAAO,CAACmB,GAAG,CAAC,cAAI;UAAA,OAAIhC,SAAS,CAAC0B,IAAI,EAAEyC,UAAU,EAAE+B,OAAO,CAAC;QAAA,EAAC;MAClE,CAAC,CAAC;IACJ;EAAC;IAAA;IAAA,OAEO,4BAAmBY,SAA0B;MACnD,IAAMC,GAAG,GAAG,EAAE,CAAC3E,MAAM,CAAC4E,KAAK,CACvB,EAAE,EACF1F,MAAM,CAACC,IAAI,CAACuF,SAAS,CAAC,CACjB9E,GAAG,CAAC,aAAG;QAAA,OAAI8E,SAAS,CAAC7E,GAAG,CAAC;MAAA,EAAC,CAC1BD,GAAG,CAAC,iBAAO;QAAA,OAAIgC,OAAO,CAAChC,GAAG,CAAC,gBAAM;UAAA,OAAIE,MAAM,CAACC,EAAE;QAAA,EAAC;MAAA,EAAC,CAAC;MAC1D,OAAO,IAAI8E,GAAG,CAACF,GAAG,CAAC;IACrB;EAAC;IAAA;IAAA,OAEO,gCACJV,QAAgB,EAAE9D,IAAU,EAAEuE,SAA0B,EACxDZ,OAAyB,EAAEK,aAA0B,EACrDW,WAAqB,EACrBT,+BAAwD;MAC1D;MACA;MACA,IAAIlE,IAAI,CAAC4E,QAAQ,KAAK,SAAS,IAAID,WAAW,CAACE,OAAO,CAACf,QAAQ,CAAC,KAAK,CAAC,CAAC,EAAE;QACvE;;MAGFS,SAAS,CAACT,QAAQ,CAAC,CAAC7E,OAAO,CAAC,gBAAM,EAAG;QACnC,IAAIU,MAAM,IAAI,IAAI,EAAE;UAClBuE,+BAA+B,CAACvE,MAAM,CAACC,EAAE,CAAC,GACtC,CAACsE,+BAA+B,CAACvE,MAAM,CAACC,EAAE,CAAC,IAAI,CAAC,IAChDI,IAAI,CAAC8E,QAAQ,CAAC1D,MAAM;;MAE5B,CAAC,CAAC;MACFpB,IAAI,CAACxB,MAAM,CAACS,OAAO,CAAC,eAAK,EAAG;QAC1B;QACA;QACA,IAAI8F,KAAK,CAACH,QAAQ,KAAK,SAAS,EAAE;UAChC,IAAMnD,OAAO,GACT/D,4BAA4B,CAACqH,KAAK,CAAC5F,IAAI,EAAEoF,SAAS,EAAEZ,OAAO,CAAC;UAChE,IAAIlC,OAAO,IAAI,IAAI,EAAE;YACnBA,OAAO,CAACxC,OAAO,CAAC,gBAAM,EAAG;cACvB,IAAIU,MAAM,IAAI,CAACA,MAAM,CAACqF,IAAI,IAAI,CAAChB,aAAa,CAACiB,GAAG,CAACtF,MAAM,CAACC,EAAE,CAAC,EAAE;gBAC3D,IAAMsF,KAAK,GAAGhB,+BAA+B,CAACvE,MAAM,CAACC,EAAE,CAAC;gBACxD,IAAIsF,KAAK,KAAK,CAAC,EAAE;kBACfvF,MAAM,CAAC2E,OAAO,EAAE;kBAChB,OAAOJ,+BAA+B,CAACvE,MAAM,CAACC,EAAE,CAAC;iBAClD,MAAM,IAAIsF,KAAK,IAAI,IAAI,EAAE;kBACxB;kBACA;kBACAhB,+BAA+B,CAACvE,MAAM,CAACC,EAAE,CAAC,EAAE;;;YAGlD,CAAC,CAAC;;;MAGR,CAAC,CAAC;IACJ;IAEA;;;;;;;;;EAAA;IAAA;IAAA;MAAA,gFASA,iBAAmBpB,MAAsB,EAAEF,OAAkB;QAAA;UAAA;YAAA;cAAA,iCAEpD,IAAI,CAAC6G,aAAa,CAAC3G,MAAM,EAAEF,OAAO,CAAC;YAAA;YAAA;cAAA;UAAA;QAAA;MAAA,CAC3C;MAAA;QAAA;MAAA;MAAA;IAAA;EAAA;IAAA;IAAA,OAED,sCAA0B;MACxB,IAAI,CAAC,IAAI,CAACsF,gBAAgB,EAAE;QAC1B;;MAEF7E,MAAM,CAACqG,MAAM,CAAC,IAAI,CAACxB,gBAAgB,CAAC,CAAC3E,OAAO,CAAC,qBAAW,EAAG;QAAA,2CACpC8C,WAAW;UAAA;QAAA;UAAhC,oDAAkC;YAAA,IAAvBpC,MAAM;YACf,IAAIA,MAAM,IAAI,CAACA,MAAM,CAAC0F,UAAU,EAAE;cAChC1F,MAAM,CAAC2E,OAAO,EAAE;;;QAEnB;UAAA;QAAA;UAAA;QAAA;MACH,CAAC,CAAC;MAEF,IAAI,CAACV,gBAAgB,GAAG,IAAI;IAC9B;EAAC;IAAA;IAAA,OAED,kCAAsB;MACpB,OAAO,IAAI,CAACA,gBAAgB;IAC9B;IAEA;;;;;;;;;;;;;;EAAA;IAAA;IAAA;MAAA,gFAcQ,kBACJpF,MAAsB,EAAEF,OAAkB;QAAA;UAAA;UAAA;UAAA;UAAA;UAAA;UAAA;UAAA;UAAA;UAAA;QAAA;UAAA;YAAA;cAAEgH,mBAAmB,8DAAG,KAAK;cACvE7B,4EAAiC,EAAE;cACnCC,2EAA+B,EAAE;cACnC;cACA,IAAI,CAACzB,0BAA0B,EAAE;cACjC,IAAI,CAACqD,mBAAmB,EAAE;gBACxB9G,MAAM,GAAG,IAAI,CAAC0D,SAAS,CAAC1D,MAAM,CAAC;gBAC/B,IAAI,CAAC4D,WAAW,CAAC5D,MAAM,CAAC;gBACxB,IAAI,CAAC6D,sBAAsB,CAAC7D,MAAM,CAAC;gBACnCF,OAAO,GAAG,IAAI,CAACgE,UAAU,CAAChE,OAAO,CAAC;gBAClC,IAAI,CAACiE,YAAY,CAACjE,OAAO,CAAC;;cAG5B;cACA,IAAI;gBACF,IAAI,CAAC6E,uBAAuB,GAAGhG,GAAG,EAAE,CAACiG,OAAO,CAAC,2BAA2B,CAAC;eAC1E,CAAC,OAAOC,CAAC,EAAE;gBACV,IAAI,CAACF,uBAAuB,GAAG,KAAK;gBACpCG,OAAO,CAACC,IAAI,CAACF,CAAC,CAACG,OAAO,CAAC;;cAGnBG,OAAO,GAAG,IAAI9F,gBAAgB,CAChC,IAAI,CAAC0B,SAAS,EAAEkE,cAAc,EAAEC,aAAa,EAC7C,IAAI,CAACpE,mBAAmB,CAAC;cAE7B,IAAI,IAAI,CAAC6D,uBAAuB,EAAE;gBAChC,IAAI,CAACS,gBAAgB,GAAG,IAAI,CAACC,cAAc,CAAC,IAAI,CAACtE,SAAS,CAAC;;cAG7D;cACA;cACA;cAAA;cAAA,OACyB,IAAI,CAACgG,sBAAsB,CAChD/G,MAAM,EAAEmF,OAAO,EAAErF,OAAO,EAAEgH,mBAAmB,CAAC;YAAA;cAD5C1D,UAAU;cAEV4D,OAAO,GAAGlH,OAAO,CAACmB,GAAG,CAAC,cAAI;gBAAA,OAAIhC,SAAS,CAAC0B,IAAI,EAAEyC,UAAU,EAAE+B,OAAO,CAAC;cAAA,EAAC,EAEzE;cACM8B,SAAS,GAAGD,OAAO,CAAC/F,GAAG,CAAC,WAAC;gBAAA,OAAIiG,CAAC,CAAC9F,EAAE;cAAA,EAAC;cAClC+F,QAAQ,GAAG5G,MAAM,CAACC,IAAI,CAACR,MAAM,CAAC,CAACiB,GAAG,CAAC,cAAI;gBAAA,OAAIjB,MAAM,CAACW,IAAI,CAAC,CAACS,EAAE;cAAA,EAAC;cAC3DgG,OAAO,GACT,IAAIlB,GAAG,8BAAae,SAAS,sBAAKE,QAAQ,sBAAK,IAAI,CAACvG,SAAS,GAAE;cAEnEL,MAAM,CAACqG,MAAM,CAACxD,UAAU,CAAC,CAAC3C,OAAO,CAAC,qBAAW,EAAG;gBAC9C8C,WAAW,CAAC9C,OAAO,CAAC,gBAAM,EAAG;kBAC3B,IAAIU,MAAM,IAAI,CAACA,MAAM,CAAC0F,UAAU,IAAI,CAACO,OAAO,CAACX,GAAG,CAACtF,MAAM,CAACC,EAAE,CAAC,EAAE;oBAC3DD,MAAM,CAAC2E,OAAO,EAAE;;gBAEpB,CAAC,CAAC;cACJ,CAAC,CAAC;cAEF;cACA,IAAI,IAAI,CAACnG,MAAM,IAAI,IAAI,EAAE;gBACvBwF,OAAO,CAACW,OAAO,CAACsB,OAAO,CAAC;;cACzB,kCAEMJ,OAAO;YAAA;YAAA;cAAA;UAAA;QAAA;MAAA,CACf;MAAA;QAAA;MAAA;MAAA;IAAA;EAAA;IAAA;IAAA;MAAA,uFAED,kBACIhH,MAAgB,EAAEiF,cAA8B,EAChDC,aAA4B;QAAA;QAAA;QAAA;UAAA;YAAA;cACxBmC,YAAY,GAAGrH,MAAM,CAACgC,MAAM,CAAC,UAACf,GAAG,EAAEE,MAAM,EAAEoE,KAAK,EAAI;gBACxDtE,GAAG,CAAC,MAAI,CAACjB,MAAM,CAACuF,KAAK,CAAC,CAAC5E,IAAI,CAAC,GAAGQ,MAAM;gBACrC,OAAOF,GAAG;cACZ,CAAC,EAAE,EAAoB,CAAC;cAAA,kCAEjB,IAAI,CAAC0F,aAAa,CACrBU,YAAY,EAAE,IAAI,CAAClD,WAAW,EAAE,IAAI,EAAEc,cAAc,EAAEC,aAAa,CAAC;YAAA;YAAA;cAAA;UAAA;QAAA;MAAA,CACzE;MAAA;QAAA;MAAA;MAAA;IAAA;IAED;;;;;;;;;;;EAAA;IAAA;IAAA;MAAA,yFAWQ,kBACJlF,MAAsB,EAAEmF,OAAyB,EAAEgB,WAAsB,EACzEW,mBAA6B;QAAA;QAAA;QAAA;UAAA;YAAA;cACzBnD,KAAK,GAAGpD,MAAM,CAACC,IAAI,CAACR,MAAM,CAAC;cAC3BgE,UAAU,GACZL,KAAK,CAAC1C,GAAG,CAAC,cAAI;gBAAA,OAAI,MAAI,CAACvB,KAAK,CAACuE,KAAK,CAAC9E,aAAa,CAACwB,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC;cAAA,EAAC;cACzDuD,eAAe,GAAGiC,WAAW,CAAClF,GAAG,CAAC,cAAI;gBAAA,OAAI9B,aAAa,CAACwB,IAAI,CAAC,CAAC,CAAC,CAAC;cAAA,EAAC;cACnEwD,WAAW,GAAGD,eAAe,CAACjD,GAAG,CAAC,cAAI;gBAAA,OAAI,MAAI,CAACvB,KAAK,CAACuE,KAAK,CAACtD,IAAI,CAAC;cAAA,EAAC,EAErE;cACA,IAAIwD,WAAW,CAACvB,MAAM,KAAK,CAAC,EAAE;gBAC5BuB,WAAW,GAAG,IAAI,CAACtE,QAAQ;;cAC5B,wBAGGP,oBAAoB,CAChBU,MAAM,EAAEmE,WAAW,EAAE,IAAI,CAACpD,SAAS,EAAE,IAAI,CAACd,UAAU,CAAC,EAFtDqH,SAAS,yBAATA,SAAS,EAAE/E,aAAa,yBAAbA,aAAa,EAAEC,WAAW,yBAAXA,WAAW,EAAEC,UAAU,yBAAVA,UAAU,EAIxD;cACM8E,KAAK,GAAuB,6BAC7BvD,UAAU,sBAAK,IAAI,CAACtE,KAAK,CAAC8H,OAAO,sBAAM,IAAI,CAACvH,UAAU,IAAI,EAAE,GAC/DgB,GAAG,CAAC,cAAI,EAAG;gBACX,OAAO;kBAACO,IAAI,EAAJA,IAAI;kBAAEiG,QAAQ,EAAEtC,OAAO,CAACuC;gBAAc,CAAC;cACjD,CAAC,CAAC;cACItE,UAAU,qBAAwB,IAAI,CAACrC,SAAS,CAAC;cACvDR,MAAM,CAACC,IAAI,CAACR,MAAM,CAAC,CAACS,OAAO,CAAC,cAAI,EAAG;gBACjC,sBAA0BtB,aAAa,CAACwB,IAAI,CAAC;kBAAA;kBAAtC2E,QAAQ;kBAAEC,KAAK;gBACtB,IAAMtC,OAAO,GAAa,EAAE;gBAC5BA,OAAO,CAACsC,KAAK,CAAC,GAAGvF,MAAM,CAACW,IAAI,CAAC;gBAC7ByC,UAAU,CAACkC,QAAQ,CAAC,GAAGrC,OAAO;cAChC,CAAC,CAAC;cACIyC,+BAA+B,GAA4B,EAAE;cAC7DF,aAAa,GAAG,IAAI,CAACC,kBAAkB,CAACrC,UAAU,CAAC;cACnDuE,KAAK,GAA6B,EAAE;YAAA;cAAA,MACnCJ,KAAK,CAAC3E,MAAM,GAAG,CAAC;gBAAA;gBAAA;cAAA;cACfgF,QAAQ,GAAG,IAAI,CAACC,YAAY,CAC9B7D,UAAU,EAAEuD,KAAK,EAAEpC,OAAO,EAAE/B,UAAU,EAAEuE,KAAK,EAAEnC,aAAa,EAC5DtB,eAAe,EAAEwB,+BAA+B,EAAE4B,SAAS,CAAC;cAAA;cAAA,OAC1DQ,OAAO,CAACC,GAAG,CAACH,QAAQ,CAAC;YAAA;cAAA;cAAA;YAAA;cAE7B,IAAIpF,WAAW,IAAI,IAAI,IAAI,CAACsE,mBAAmB,EAAE;gBAC/ChC,OAAO,CAACC,IAAI,CACR,sIACgE,CAAC;;cAEjEiD,cAAc,GAChB7D,WAAW,CACN8D,MAAM,CACH,cAAI;gBAAA,OAAI,CAACzI,aAAa,CAACgC,IAAI,CAAC,IACxB,CAACvC,SAAS,CAACuC,IAAI,CAACb,IAAI,EAAEyC,UAAU,EAAE+B,OAAO,CAAC;cAAA,EAAC,CAClDlE,GAAG,CAAC,cAAI;gBAAA,OAAIO,IAAI,CAACb,IAAI;cAAA,EAAC;cAAA,MAC3BqH,cAAc,CAACpF,MAAM,GAAG,CAAC;gBAAA;gBAAA;cAAA;cACvBsF,cAAc,GAAG,EAAE;cACvB,IAAI1F,WAAW,IAAI,IAAI,EAAE;gBACvB0F,cAAc,GACV,oGAC2BzF,UAAU,MAAG;;cAC7C,MACK,IAAIC,KAAK,CACX,sCAA+BsF,cAAc,8CAClCrE,KAAK,iDAA8C,cAC1DpB,aAAa,gBAAM2F,cAAc,CAAE,CAAC;YAAA;cAAA,kCAEvC9E,UAAU;YAAA;YAAA;cAAA;UAAA;QAAA;MAAA,CAClB;MAAA;QAAA;MAAA;MAAA;IAAA;EAAA;IAAA;IAAA,OAEO,sBACJY,UAAkB,EAAEuD,KAAyB,EAAEpC,OAAyB,EACxEY,SAA0B,EAAE4B,KAA+B,EAC3DnC,aAA0B,EAAEW,WAAqB,EACjDT,+BAAwD,EACxD4B,SAAsB;MAAA;MACxB,IAAMM,QAAQ,GAA6B,EAAE;MAAC,6BACrB;QACvB,IAAMO,IAAI,GAAGZ,KAAK,CAACa,GAAG,EAAE;QACxBjD,OAAO,CAACuC,cAAc,GAAGS,IAAI,CAACV,QAAQ;QACtC,IAAInC,QAAQ,GAAG,EAAE;QACjB;QACA;QACA;QACA,IAAI6C,IAAI,CAAC3G,IAAI,CAACmB,EAAE,KAAK,OAAO,IACxB3D,aAAa,CAAC,YAAY,EAAEmJ,IAAI,CAAC3G,IAAI,EAAEuE,SAAS,EAAEZ,OAAO,CAAC,EAAE;UAAA,2BACjDpG,mBAAmB,CAACoJ,IAAI,CAAC3G,IAAI,CAACb,IAAI,EAAEwE,OAAO,CAAC;UAAA;UAAxDG,QAAQ;;QAGX;QACA;QACA,IAAIS,SAAS,CAACoC,IAAI,CAAC3G,IAAI,CAACb,IAAI,CAAC,IAAI,IAAI,EAAE;UACrC,IAAMsC,OAAO,GACT7D,SAAS,CAAC+I,IAAI,CAAC3G,IAAI,EAAEuE,SAAS,EAAEZ,OAAO,EAAE,MAAI,CAAC5D,gBAAgB,CAAC;UACnE,IAAI,CAAC+D,QAAQ,EAAE;YAAA,4BACAvG,mBAAmB,CAACoJ,IAAI,CAAC3G,IAAI,CAACb,IAAI,EAAEwE,OAAO,CAAC;YAAA;YAAxDG,QAAQ;;UAEX,IAAMoC,cAAc,GAAGvC,OAAO,CAACuC,cAAc;UAC7C,IAAI5I,IAAI,CAAC8G,SAAS,CAAC3C,OAAO,CAAC,EAAE;YAC3B2E,QAAQ,CAACS,IAAI,CAACpF,OAAO,CAACqF,IAAI,CAAC,WAAC,EAAG;cAC7BvC,SAAS,CAACT,QAAQ,CAAC,GAAG4B,CAAC;cACvB,IAAI,MAAI,CAACvC,uBAAuB,EAAE;gBAChC,MAAI,CAACS,gBAAgB,CAACE,QAAQ,CAAC,GAAG,MAAI,CAAC9B,eAAe,CAAC0D,CAAC,CAAC;;cAE3D/B,OAAO,CAACuC,cAAc,GAAGA,cAAc;cACvC,MAAI,CAAC7B,sBAAsB,CACvBP,QAAQ,EAAE6C,IAAI,CAAC3G,IAAI,EAAEuE,SAAS,EAAEZ,OAAO,EAAEK,aAAa,EACtDW,WAAW,EAAET,+BAA+B,CAAC;cACjD,MAAI,CAAC6C,iBAAiB,CAClBJ,IAAI,CAAC3G,IAAI,EAAE+F,KAAK,EAAEpC,OAAO,EAAEY,SAAS,EAAE4B,KAAK,EAAEL,SAAS,CAAC;cAC3D,OAAOJ,CAAC;YACV,CAAC,CAAC,CAAC;WACJ,MAAM;YACLnB,SAAS,CAACT,QAAQ,CAAC,GAAGrC,OAAO;YAC7B,IAAI,MAAI,CAAC0B,uBAAuB,EAAE;cAChC,MAAI,CAACS,gBAAgB,CAACE,QAAQ,CAAC,GAAG,MAAI,CAAC9B,eAAe,CAACP,OAAO,CAAC;;YAEjE,MAAI,CAAC4C,sBAAsB,CACvBP,QAAQ,EAAE6C,IAAI,CAAC3G,IAAI,EAAEuE,SAAS,EAAEZ,OAAO,EAAEK,aAAa,EACtDW,WAAW,EAAET,+BAA+B,CAAC;YACjD,MAAI,CAAC6C,iBAAiB,CAClBJ,IAAI,CAAC3G,IAAI,EAAE+F,KAAK,EAAEpC,OAAO,EAAEY,SAAS,EAAE4B,KAAK,EAAEL,SAAS,CAAC;;SAE9D,MAAM;UACL,MAAI,CAACiB,iBAAiB,CAClBJ,IAAI,CAAC3G,IAAI,EAAE+F,KAAK,EAAEpC,OAAO,EAAEY,SAAS,EAAE4B,KAAK,EAAEL,SAAS,CAAC;;OAE9D;MAlDD,OAAOC,KAAK,CAAC3E,MAAM,GAAG,CAAC;QAAA;MAAA;MAmDvB,OAAOgF,QAAQ;IACjB;EAAC;IAAA;IAAA,OAEO,2BACJpG,IAAU,EAAE+F,KAAyB,EAAEpC,OAAyB,EAChEY,SAA0B,EAAE4B,KAA+B,EAC3DL,SAAsB;MACxB9F,IAAI,CAAC8E,QAAQ,CAAC7F,OAAO,CAAC,UAAC+H,SAAS,EAAI;QAClC,4BAAqBzJ,mBAAmB,CAACyJ,SAAS,CAAC7H,IAAI,EAAEwE,OAAO,CAAC;UAAA;UAA1DG,QAAQ;QACf,IAAIqC,KAAK,CAACrC,QAAQ,CAAC,IAAI,CAACgC,SAAS,CAACb,GAAG,CAAC+B,SAAS,CAAC7H,IAAI,CAAC,EAAE;UACrD;;QAEF;QACA,IAAI6H,SAAS,CAAC7F,EAAE,KAAK,OAAO,EAAE;UAC5B,IAAI6F,SAAS,CAACC,UAAU,CAACC,IAAI,CAAC,cAAI,EAAG;YAC/B,OAAO,CAAC,CAACzJ,SAAS,CAAC0B,IAAI,EAAEoF,SAAS,EAAEZ,OAAO,CAAC;UAC9C,CAAC,CAAC,EAAE;YACNwC,KAAK,CAACrC,QAAQ,CAAC,GAAG,IAAI;YACtBiC,KAAK,CAACc,IAAI,CAAC;cAACZ,QAAQ,EAAEtC,OAAO,CAACuC,cAAc;cAAElG,IAAI,EAAEgH;YAAS,CAAC,CAAC;;SAElE;UAAO;UACJ,IAAIA,SAAS,CAACC,UAAU,CAACE,KAAK,CAAC,cAAI,EAAG;YAChC,OAAO,CAAC,CAAC1J,SAAS,CAAC0B,IAAI,EAAEoF,SAAS,EAAEZ,OAAO,CAAC;UAC9C,CAAC,CAAC,EAAE;YACVwC,KAAK,CAACrC,QAAQ,CAAC,GAAG,IAAI;YACtBiC,KAAK,CAACc,IAAI,CAAC;cAACZ,QAAQ,EAAEtC,OAAO,CAACuC,cAAc;cAAElG,IAAI,EAAEgH;YAAS,CAAC,CAAC;;MAEnE,CAAC,CAAC;IACJ;IAEA;;;EAAA;IAAA;IAAA,OAGA,mBAAO;MAAA;MACLjI,MAAM,CAACC,IAAI,CAAC,IAAI,CAACO,SAAS,CAAC,CACtBN,OAAO,CACJ,aAAG;QAAA,OAAI,MAAI,CAACM,SAAS,CAACG,GAAG,CAAC,CAACT,OAAO,CAAC,gBAAM;UAAA,OAAIU,MAAM,CAAC2E,OAAO,EAAE;QAAA,EAAC;MAAA,EAAC;IACzE;EAAC;IAAA;IAAA,OAEO,gCAAuB9F,MAAsB;MAAA;MACnDO,MAAM,CAACC,IAAI,CAACR,MAAM,CAAC,CAACS,OAAO,CAAC,cAAI,EAAG;QACjC,IAAM8F,KAAK,GAAGvG,MAAM,CAACW,IAAI,CAAC;QAC1B,sBAAqBxB,aAAa,CAACwB,IAAI,CAAC;UAAA;UAAjC2E,QAAQ;QACf,IAAM9D,IAAI,GAAG,OAAI,CAAC9B,KAAK,CAACuE,KAAK,CAACqB,QAAQ,CAAC;QACvC,IAAI9D,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,IAAIF,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,CAACC,KAAK,EAAE;UAC9D,IAAMF,KAAK,GAAGD,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,CAACC,KAAiB;UACxD,IAAMiH,KAAK,GAAGnH,KAAK,CAACmB,MAAM,KAAK2D,KAAK,CAAC9E,KAAK,CAACmB,MAAM,IAC7C2D,KAAK,CAAC9E,KAAK,CAACkH,KAAK,CACb,UAACE,GAAG,EAAEtD,KAAK;YAAA,OAAK9D,KAAK,CAAC8D,KAAK,CAAC,KAAK,CAAC,CAAC,IAAI9D,KAAK,CAAC8D,KAAK,CAAC,KAAKsD,GAAG;UAAA,EAAC;UACpE/J,IAAI,CAACgK,MAAM,CACPF,KAAK,EACL;YAAA,OAAM,6BAAsBpH,IAAI,CAACb,IAAI,8DACDc,KAAK,gBAAa,cAC9C8E,KAAK,CAAC9E,KAAK,MAAG;UAAA,EAAC;;QAE7B,IAAID,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,IAAIF,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,CAACC,KAAK,EAAE;UAC9D7C,IAAI,CAACgK,MAAM,CACPvC,KAAK,CAAC1E,KAAK,KAAKL,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,CAACC,KAAe,EACxD;YAAA,OAAM,6BAAsBH,IAAI,CAACb,IAAI,qDACH,aAC3Ba,IAAI,CAACE,UAAU,CAAC,OAAO,CAAC,CAACC,KAAK,uBAAa4E,KAAK,CAAC1E,KAAK,CAAE;UAAA,EAAC;;MAExE,CAAC,CAAC;IACJ;EAAC;IAAA;IAAA,OAEO,mBAAU7B,MAAsB;;MACtC,IAAM+I,MAAM,GAAmB,EAAE;MACjC,KAAK,IAAMC,SAAS,IAAIhJ,MAAM,EAAE;QAC9B,IAAMmB,MAAM,GAAG,gBAAI,CAAChB,UAAU,0CAAGH,MAAM,0CAAIgJ,SAAS,CAAC;QACrD,IAAI7H,MAAM,IAAI,IAAI,EAAE;UAClB4H,MAAM,CAAC5H,MAAM,CAACR,IAAI,CAAC,GAAGX,MAAM,CAACgJ,SAAS,CAAC;SACxC,MAAM;UACLD,MAAM,CAACC,SAAS,CAAC,GAAGhJ,MAAM,CAACgJ,SAAS,CAAC;;;MAGzC,OAAOD,MAAM;IACf;EAAC;IAAA;IAAA,OAEO,qBAAY/I,MAAsB;MAAA;MACxC,IAAMiJ,UAAU,GAAG1I,MAAM,CAACC,IAAI,CAACR,MAAM,CAAC,CAACiI,MAAM,CAAC,cAAI,EAAG;QACnD,sBAAmB9I,aAAa,CAACwB,IAAI,CAAC;UAAA;UAA/B2E,QAAQ;QACf,OAAO,OAAI,CAAC5F,KAAK,CAACuE,KAAK,CAACqB,QAAQ,CAAC,IAAI,IAAI;MAC3C,CAAC,CAAC;MACF,IAAI2D,UAAU,CAACrG,MAAM,GAAG,CAAC,EAAE;QACzB,MAAM,IAAIF,KAAK,CACX,mEACUuG,UAAU,iCAA8B,CAAC;;IAE3D;EAAC;IAAA;IAAA,OAEO,oBAAWnJ,OAAiB;MAAA;MAClC,OAAOA,OAAO,CAACmB,GAAG,CAAC,cAAI,EAAG;;QACxB,IAAME,MAAM,GAAG,mBAAI,CAAChB,UAAU,0CAAGL,OAAO,0CAAIa,IAAI,CAAC;QACjD,IAAIQ,MAAM,IAAI,IAAI,EAAE;UAClB,OAAOA,MAAM,CAACR,IAAI;;QAEpB,OAAOA,IAAI;MACb,CAAC,EAAE,EAAE,CAAC;IACR;EAAC;IAAA;IAAA,OAEO,sBAAab,OAAiB;MAAA;MACpCA,OAAO,CAACW,OAAO,CAAC,cAAI,EAAG;QACrB,sBAAyBtB,aAAa,CAACwB,IAAI,CAAC;UAAA;UAArCuI,cAAc;QACrB,IAAI,CAAC,OAAI,CAACxJ,KAAK,CAACuE,KAAK,CAACiF,cAAc,CAAC,EAAE;UACrC,MAAM,IAAIxG,KAAK,uBAAgB/B,IAAI,iCAA8B;;MAErE,CAAC,CAAC;IACJ;EAAC;EAAA;AAAA","names":["env","keep","tidy","util","getNodeNameAndIndex","getParamValue","getTensor","getTensorsForCurrentContenxt","parseNodeName","executeOp","ExecutionContext","getExecutionSubgraph","getNodesInTopologicalOrder","isControlFlow","GraphExecutor","graph","parent","Map","_outputs","outputs","_inputs","inputs","_initNodes","initNodes","_signature","signature","_functions","functions","Object","keys","forEach","_functionExecutorMap","name","weightIds","_weightIds","functionExecutorMap","weightMap","_weightMap","map","key","tensor","id","concat","resourceManager","_resourceManager","node","shape","attrParams","value","undefined","dtype","signatureKey","defaultOutput","reduce","sortedInputs","sort","sortedOutputs","join","SEPERATOR","executionInfo","missingInputs","dynamicNode","syncInputs","Error","op","length","outNames","n","inNames","clone","tensors","clonedTensor","cloneAndKeepTensor","tensorsMap","fromEntries","entries","tensorsList","cloneTensorList","disposeIntermediateTensors","mapInputs","names","checkInputs","checkInputShapeAndType","mapOutputs","checkOutputs","inputNodes","nodes","outputNodeNames","outputNodes","compilationKey","getCompilationKey","orderedNodes","compiledMap","get","compile","set","keepIntermediateTensors","getBool","e","console","warn","message","tensorArrayMap","tensorListMap","context","clonedTensorsMap","cloneTensorMap","nodeName","index","tensorsToKeep","getFrozenTensorIds","intermediateTensorConsumerCount","i","isPromise","checkTensorForDisposal","dispose","tensorMap","ids","apply","Set","outputNames","category","indexOf","children","input","kept","has","count","_executeAsync","values","isDisposed","isFunctionExecution","executeWithControlFlow","results","outputIds","t","inputIds","keepIds","mappedInputs","usedNodes","stack","weights","contexts","currentContext","added","promises","processStack","Promise","all","missingOutputs","filter","alternativeMsg","item","pop","push","then","processChildNodes","childNode","inputNames","some","every","match","dim","assert","result","inputName","notInGraph","normalizedName"],"sources":["E:\\react-detect-toxicity-in-a-chat-app-youtube-2\\Toxic-Word-Checker\\node_modules\\@tensorflow\\tfjs-converter\\src\\executor\\graph_executor.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {DataType, env, keep, NamedTensorMap, Tensor, tidy, util} from '@tensorflow/tfjs-core';\n\nimport {ISignatureDef} from '../data/compiled_api';\nimport {NamedTensorsMap, TensorArrayMap, TensorInfo, TensorListMap} from '../data/types';\nimport {getNodeNameAndIndex, getParamValue, getTensor, getTensorsForCurrentContenxt, parseNodeName} from '../operations/executors/utils';\nimport {executeOp} from '../operations/operation_executor';\nimport {Graph, Node} from '../operations/types';\n\nimport {ExecutionContext, ExecutionContextInfo} from './execution_context';\nimport {getExecutionSubgraph, getNodesInTopologicalOrder, isControlFlow} from './model_analysis';\nimport {ResourceManager} from './resource_manager';\nimport {FunctionExecutor} from './types';\n\ninterface NodeWithContexts {\n  contexts: ExecutionContextInfo[];\n  node: Node;\n}\n\nexport class GraphExecutor implements FunctionExecutor {\n  private compiledMap: Map<string, Node[]> = new Map();\n  private _weightMap: NamedTensorsMap = {};\n  private _weightIds: number[];\n  private _signature: ISignatureDef;\n  private _inputs: Node[];\n  private _outputs: Node[];\n  private _initNodes: Node[];  // Internal init nodes to start initialization.\n  private SEPERATOR = ',';\n  private _functions: {[key: string]: Graph} = {};\n  private _functionExecutorMap: {[key: string]: FunctionExecutor} = {};\n  private _resourceManager: ResourceManager;\n  private clonedTensorsMap: NamedTensorsMap;\n  private keepIntermediateTensors = false;\n\n  get weightIds(): number[] {\n    return this.parent ? this.parent.weightIds : this._weightIds;\n  }\n\n  get functionExecutorMap(): {[key: string]: FunctionExecutor} {\n    return this.parent ? this.parent.functionExecutorMap :\n                         this._functionExecutorMap;\n  }\n\n  get weightMap(): NamedTensorsMap {\n    return this.parent ? this.parent.weightMap : this._weightMap;\n  }\n\n  set weightMap(weightMap: NamedTensorsMap) {\n    const weightIds = Object.keys(weightMap).map(\n        key => weightMap[key].map(tensor => tensor.id));\n    this._weightIds = [].concat(...weightIds);\n    this._weightMap = weightMap;\n  }\n\n  /**\n   * Set `ResourceManager` shared by executors of a model.\n   * @param resourceManager: `ResourceManager` of the `GraphModel`.\n   */\n  set resourceManager(resourceManager: ResourceManager) {\n    this._resourceManager = resourceManager;\n  }\n\n  get inputs(): TensorInfo[] {\n    return this._inputs.map(node => {\n      return {\n        name: node.name,\n        shape: node.attrParams['shape'] ?\n            node.attrParams['shape'].value as number[] :\n            undefined,\n        dtype: node.attrParams['dtype'] ?\n            node.attrParams['dtype'].value as DataType :\n            undefined\n      };\n    });\n  }\n\n  get outputs(): TensorInfo[] {\n    return this._outputs.map(node => {\n      return {\n        name: node.name,\n        shape: node.attrParams['shape'] ?\n            node.attrParams['shape'].value as number[] :\n            undefined,\n        dtype: node.attrParams['dtype'] ?\n            node.attrParams['dtype'].value as DataType :\n            undefined\n      };\n    });\n  }\n\n  get inputNodes(): string[] {\n    return this._inputs.map(node => node.signatureKey || node.name);\n  }\n\n  get outputNodes(): string[] {\n    return this._outputs.map((node) => {\n      const name = node.signatureKey || node.name;\n      return node.defaultOutput ? (`${name}:${node.defaultOutput}`) : name;\n    });\n  }\n\n  get functions(): {[key: string]: ISignatureDef} {\n    return Object.keys(this._functions).reduce((map, key) => {\n      map[key] = this._functions[key].signature;\n      return map;\n    }, {} as {[key: string]: ISignatureDef});\n  }\n\n  /**\n   *\n   * @param graph Graph the model or function graph to be executed.\n   * @param parent When building function exector you need to set the parent\n   * executor. Since the weights and function executor maps are set at parant\n   * level, that function executor can access the function maps and weight maps\n   * through the parent.\n   */\n  constructor(private graph: Graph, private parent?: GraphExecutor) {\n    this._outputs = graph.outputs;\n    this._inputs = graph.inputs;\n    this._initNodes = graph.initNodes;\n    this._signature = graph.signature;\n    this._functions = graph.functions;\n    // create sub-graph executors\n    if (graph.functions != null) {\n      Object.keys(graph.functions).forEach(name => {\n        this._functionExecutorMap[name] =\n            new GraphExecutor(graph.functions[name], this);\n      });\n    }\n  }\n\n  private getCompilationKey(inputs: Node[], outputs: Node[]): string {\n    const sortedInputs = inputs.map(node => node.name).sort();\n    const sortedOutputs = outputs.map(node => node.name).sort();\n    return sortedInputs.join(this.SEPERATOR) + '--' +\n        sortedOutputs.join(this.SEPERATOR);\n  }\n\n  /**\n   * Compiles the inference graph and returns the minimal set of nodes that are\n   * required for execution, in the correct execution order.\n   */\n  private compile(inputs: NamedTensorMap, outputs: Node[]): Node[] {\n    const executionInfo =\n        getExecutionSubgraph(inputs, outputs, this.weightMap, this._initNodes);\n    const {missingInputs, dynamicNode, syncInputs} = executionInfo;\n    if (dynamicNode != null) {\n      throw new Error(\n          `This execution contains the node '${dynamicNode.name}', which has ` +\n          `the dynamic op '${dynamicNode.op}'. Please use ` +\n          `model.executeAsync() instead. Alternatively, to avoid the ` +\n          `dynamic ops, specify the inputs [${syncInputs}]`);\n    }\n\n    if (missingInputs.length > 0) {\n      const outNames = outputs.map(n => n.name);\n      const inNames = Object.keys(inputs);\n      throw new Error(\n          `Cannot compute the outputs [${outNames}] from the provided inputs ` +\n          `[${inNames}]. Missing the following inputs: [${missingInputs}]`);\n    }\n\n    return getNodesInTopologicalOrder(\n        this.graph, this.weightMap, executionInfo);\n  }\n\n  private cloneAndKeepTensor(tensor: Tensor) {\n    if (tensor == null) {\n      return null;\n    }\n    const clone = tensor.clone();\n    // Keep the clone because`model.execute()` may be called within\n    // a `tidy()`, but the user may inspect these tensors after the\n    // tidy.\n    keep(clone);\n    return clone;\n  }\n\n  private cloneTensorList(tensors: Tensor[]) {\n    if (!tensors) {\n      return null;\n    }\n    const clonedTensor = tensors.map(tensor => {\n      return this.cloneAndKeepTensor(tensor);\n    });\n    return clonedTensor;\n  }\n\n  private cloneTensorMap(tensorsMap: NamedTensorsMap): NamedTensorsMap {\n    return Object.fromEntries(\n        Object.entries(tensorsMap).map(([name, tensorsList]) => {\n          return [name, this.cloneTensorList(tensorsList)];\n        }));\n  }\n\n  /**\n   * Executes the inference for given input tensors.\n   * @param inputs Tensor map for the model inputs, keyed by the input node\n   * names.\n   * @param outputs Optional. output node name from the Tensorflow model, if\n   * no outputs are specified, the default outputs of the model would be used.\n   * You can inspect intermediate nodes of the model by adding them to the\n   * outputs array.\n   */\n  execute(inputs: NamedTensorMap, outputs?: string[]): Tensor[] {\n    // Dispose any tensors from a prior run to avoid leaking them.\n    this.disposeIntermediateTensors();\n    inputs = this.mapInputs(inputs);\n    const names = Object.keys(inputs).sort();\n    this.checkInputs(inputs);\n    this.checkInputShapeAndType(inputs);\n    outputs = this.mapOutputs(outputs);\n    this.checkOutputs(outputs);\n    const inputNodes =\n        names.map(name => this.graph.nodes[parseNodeName(name)[0]]);\n    const outputNodeNames = outputs.map(name => parseNodeName(name)[0]);\n    let outputNodes = outputNodeNames.map(name => this.graph.nodes[name]);\n    // If no outputs are specified, then use the default outputs of the model.\n    if (outputNodes.length === 0) {\n      outputNodes = this._outputs;\n    }\n\n    const compilationKey = this.getCompilationKey(inputNodes, outputNodes);\n\n    // Do nothing if the compiled graph cache contains the input.\n    let orderedNodes = this.compiledMap.get(compilationKey);\n    if (orderedNodes == null) {\n      orderedNodes = this.compile(inputs, outputNodes);\n      this.compiledMap.set(compilationKey, orderedNodes);\n    }\n\n    // Keep tensors if KEEP_INTERMEDIATE_TENSORS is on.\n    try {\n      this.keepIntermediateTensors = env().getBool('KEEP_INTERMEDIATE_TENSORS');\n    } catch (e) {\n      this.keepIntermediateTensors = false;\n      console.warn(e.message);\n    }\n    const tensorArrayMap: TensorArrayMap = {};\n    const tensorListMap: TensorListMap = {};\n\n    return tidy(() => {\n      const context = new ExecutionContext(\n          this.weightMap, tensorArrayMap, tensorListMap,\n          this.functionExecutorMap);\n      const tensorsMap: NamedTensorsMap = {...this.weightMap};\n      if (this.keepIntermediateTensors) {\n        this.clonedTensorsMap = this.cloneTensorMap(this.weightMap);\n      }\n\n      Object.keys(inputs).forEach(name => {\n        const [nodeName, index] = parseNodeName(name);\n        const tensors: Tensor[] = [];\n        tensors[index] = inputs[name];\n        tensorsMap[nodeName] = tensors;\n        if (this.keepIntermediateTensors) {\n          this.clonedTensorsMap[nodeName] = this.cloneTensorList(tensors);\n        }\n      });\n\n      const tensorsToKeep = this.getFrozenTensorIds(tensorsMap);\n      const intermediateTensorConsumerCount: {[key: number]: number} = {};\n      for (let i = 0; i < orderedNodes.length; i++) {\n        const node = orderedNodes[i];\n        if (!tensorsMap[node.name]) {\n          const tensors =\n              executeOp(node, tensorsMap, context, this._resourceManager) as\n              Tensor[];\n          if (util.isPromise(tensors)) {\n            throw new Error(\n                `The execution of the op '${node.op}' returned a promise. ` +\n                `Please use model.executeAsync() instead.`);\n          }\n          tensorsMap[node.name] = tensors;\n          if (this.keepIntermediateTensors) {\n            this.clonedTensorsMap[node.name] = this.cloneTensorList(tensors);\n          }\n          this.checkTensorForDisposal(\n              node.name, node, tensorsMap, context, tensorsToKeep,\n              outputNodeNames, intermediateTensorConsumerCount);\n        }\n      }\n\n      // dispose the context for the root executor\n      if (this.parent == null) {\n        context.dispose(tensorsToKeep);\n      }\n\n      return outputs.map(name => getTensor(name, tensorsMap, context));\n    });\n  }\n\n  private getFrozenTensorIds(tensorMap: NamedTensorsMap): Set<number> {\n    const ids = [].concat.apply(\n        [],\n        Object.keys(tensorMap)\n            .map(key => tensorMap[key])\n            .map(tensors => tensors.map(tensor => tensor.id)));\n    return new Set(ids);\n  }\n\n  private checkTensorForDisposal(\n      nodeName: string, node: Node, tensorMap: NamedTensorsMap,\n      context: ExecutionContext, tensorsToKeep: Set<number>,\n      outputNames: string[],\n      intermediateTensorConsumerCount: {[key: string]: number}) {\n    // Skip output nodes and any control flow nodes, since its dependency is\n    // tricky to track correctly.\n    if (node.category === 'control' || outputNames.indexOf(nodeName) !== -1) {\n      return;\n    }\n\n    tensorMap[nodeName].forEach(tensor => {\n      if (tensor != null) {\n        intermediateTensorConsumerCount[tensor.id] =\n            (intermediateTensorConsumerCount[tensor.id] || 0) +\n            node.children.length;\n      }\n    });\n    node.inputs.forEach(input => {\n      // Skip any control flow nodes, since its dependency is tricky to track\n      // correctly.\n      if (input.category !== 'control') {\n        const tensors =\n            getTensorsForCurrentContenxt(input.name, tensorMap, context);\n        if (tensors != null) {\n          tensors.forEach(tensor => {\n            if (tensor && !tensor.kept && !tensorsToKeep.has(tensor.id)) {\n              const count = intermediateTensorConsumerCount[tensor.id];\n              if (count === 1) {\n                tensor.dispose();\n                delete intermediateTensorConsumerCount[tensor.id];\n              } else if (count != null) {\n                // only intermediate nodes has count set, inputs and weights\n                // are not.\n                intermediateTensorConsumerCount[tensor.id]--;\n              }\n            }\n          });\n        }\n      }\n    });\n  }\n\n  /**\n   * Executes the inference for given input tensors in Async fashion.\n   * @param inputs Tensor map for the model inputs, keyed by the input node\n   * names.\n   * @param outputs output node name from the Tensorflow model, if no outputs\n   * are specified, the default outputs of the model would be used. You can\n   * inspect intermediate nodes of the model by adding them to the outputs\n   * array.\n   */\n  async executeAsync(inputs: NamedTensorMap, outputs?: string[]):\n      Promise<Tensor[]> {\n    return this._executeAsync(inputs, outputs);\n  }\n\n  disposeIntermediateTensors() {\n    if (!this.clonedTensorsMap) {\n      return;\n    }\n    Object.values(this.clonedTensorsMap).forEach(tensorsList => {\n      for (const tensor of tensorsList) {\n        if (tensor && !tensor.isDisposed) {\n          tensor.dispose();\n        }\n      }\n    });\n\n    this.clonedTensorsMap = null;\n  }\n\n  getIntermediateTensors(): NamedTensorsMap {\n    return this.clonedTensorsMap;\n  }\n\n  /**\n   * Executes the inference for given input tensors in Async fashion.\n   * @param inputs Tensor map for the model inputs, keyed by the input node\n   * names.\n   * @param outputs Optional. output node name from the Tensorflow model,\n   * if no outputs are specified, the default outputs of the model would be\n   * used. You can inspect intermediate nodes of the model by adding them to\n   * the outputs array.\n   * @param isFunctionExecution Optional. Flag for executing a function.\n   * @param tensorArrayMap Optional, global TensorArray map by id. Used for\n   * function execution.\n   * @param tensorArrayMap Optinal global TensorList map by id. Used for\n   * function execution.\n   */\n  private async _executeAsync(\n      inputs: NamedTensorMap, outputs?: string[], isFunctionExecution = false,\n      tensorArrayMap: TensorArrayMap = {},\n      tensorListMap: TensorListMap = {}): Promise<Tensor[]> {\n    // Dispose any tensors from a prior run to avoid leaking them.\n    this.disposeIntermediateTensors();\n    if (!isFunctionExecution) {\n      inputs = this.mapInputs(inputs);\n      this.checkInputs(inputs);\n      this.checkInputShapeAndType(inputs);\n      outputs = this.mapOutputs(outputs);\n      this.checkOutputs(outputs);\n    }\n\n    // Keep tensors if KEEP_INTERMEDIATE_TENSORS is on.\n    try {\n      this.keepIntermediateTensors = env().getBool('KEEP_INTERMEDIATE_TENSORS');\n    } catch (e) {\n      this.keepIntermediateTensors = false;\n      console.warn(e.message);\n    }\n\n    const context = new ExecutionContext(\n        this.weightMap, tensorArrayMap, tensorListMap,\n        this.functionExecutorMap);\n\n    if (this.keepIntermediateTensors) {\n      this.clonedTensorsMap = this.cloneTensorMap(this.weightMap);\n    }\n\n    // Graph with control flow op requires runtime evaluation of the execution\n    // order, while without control flow the execution order is pre-determined\n    // in the compile method.\n    const tensorsMap = await this.executeWithControlFlow(\n        inputs, context, outputs, isFunctionExecution);\n    const results = outputs.map(name => getTensor(name, tensorsMap, context));\n\n    // dispose all the intermediate tensors\n    const outputIds = results.map(t => t.id);\n    const inputIds = Object.keys(inputs).map(name => inputs[name].id);\n    const keepIds =\n        new Set<number>([...outputIds, ...inputIds, ...this.weightIds]);\n\n    Object.values(tensorsMap).forEach(tensorsList => {\n      tensorsList.forEach(tensor => {\n        if (tensor && !tensor.isDisposed && !keepIds.has(tensor.id)) {\n          tensor.dispose();\n        }\n      });\n    });\n\n    // dispose the context for the root executor\n    if (this.parent == null) {\n      context.dispose(keepIds);\n    }\n\n    return results;\n  }\n\n  async executeFunctionAsync(\n      inputs: Tensor[], tensorArrayMap: TensorArrayMap,\n      tensorListMap: TensorListMap): Promise<Tensor[]> {\n    const mappedInputs = inputs.reduce((map, tensor, index) => {\n      map[this.inputs[index].name] = tensor;\n      return map;\n    }, {} as NamedTensorMap);\n\n    return this._executeAsync(\n        mappedInputs, this.outputNodes, true, tensorArrayMap, tensorListMap);\n  }\n\n  /**\n   * When there are control flow nodes in the graph, the graph execution use\n   * ExecutionContext to keep track of the frames and loop iterators.\n   * @param inputs placeholder tensors for the graph.\n   * @param context the execution context object for current execution.\n   * @param outputNames Optional. output node name from the Tensorflow model,\n   * if no outputs are specified, the default outputs of the model would be\n   * used. You can inspect intermediate nodes of the model by adding them to\n   * the outputs array.\n   * @param isFunctionExecution Flag for executing a function.\n   */\n  private async executeWithControlFlow(\n      inputs: NamedTensorMap, context: ExecutionContext, outputNames?: string[],\n      isFunctionExecution?: boolean): Promise<NamedTensorsMap> {\n    const names = Object.keys(inputs);\n    const inputNodes =\n        names.map(name => this.graph.nodes[parseNodeName(name)[0]]);\n    const outputNodeNames = outputNames.map(name => parseNodeName(name)[0]);\n    let outputNodes = outputNodeNames.map(name => this.graph.nodes[name]);\n\n    // If no outputs are specified, then use the default outputs of the model.\n    if (outputNodes.length === 0) {\n      outputNodes = this._outputs;\n    }\n\n    const {usedNodes, missingInputs, dynamicNode, syncInputs} =\n        getExecutionSubgraph(\n            inputs, outputNodes, this.weightMap, this._initNodes);\n\n    // First nodes to execute include inputNodes, weights, and initNodes.\n    const stack: NodeWithContexts[] = [\n      ...inputNodes, ...this.graph.weights, ...(this._initNodes || [])\n    ].map(node => {\n      return {node, contexts: context.currentContext};\n    });\n    const tensorsMap: NamedTensorsMap = {...this.weightMap};\n    Object.keys(inputs).forEach(name => {\n      const [nodeName, index] = parseNodeName(name);\n      const tensors: Tensor[] = [];\n      tensors[index] = inputs[name];\n      tensorsMap[nodeName] = tensors;\n    });\n    const intermediateTensorConsumerCount: {[key: number]: number} = {};\n    const tensorsToKeep = this.getFrozenTensorIds(tensorsMap);\n    const added: {[key: string]: boolean} = {};\n    while (stack.length > 0) {\n      const promises = this.processStack(\n          inputNodes, stack, context, tensorsMap, added, tensorsToKeep,\n          outputNodeNames, intermediateTensorConsumerCount, usedNodes);\n      await Promise.all(promises);\n    }\n    if (dynamicNode == null && !isFunctionExecution) {\n      console.warn(\n          `This model execution did not contain any nodes with control flow ` +\n          `or dynamic output shapes. You can use model.execute() instead.`);\n    }\n    const missingOutputs =\n        outputNodes\n            .filter(\n                node => !isControlFlow(node) &&\n                    !getTensor(node.name, tensorsMap, context))\n            .map(node => node.name);\n    if (missingOutputs.length > 0) {\n      let alternativeMsg = '';\n      if (dynamicNode != null) {\n        alternativeMsg =\n            `Alternatively, to avoid the dynamic ops, use model.execute() ` +\n            `and specify the inputs [${syncInputs}]`;\n      }\n      throw new Error(\n          `Cannot compute the outputs [${missingOutputs}] from the provided ` +\n          `inputs [${names}]. Consider providing the following inputs: ` +\n          `[${missingInputs}]. ${alternativeMsg}`);\n    }\n    return tensorsMap;\n  }\n\n  private processStack(\n      inputNodes: Node[], stack: NodeWithContexts[], context: ExecutionContext,\n      tensorMap: NamedTensorsMap, added: {[key: string]: boolean},\n      tensorsToKeep: Set<number>, outputNames: string[],\n      intermediateTensorConsumerCount: {[key: number]: number},\n      usedNodes: Set<string>) {\n    const promises: Array<Promise<Tensor[]>> = [];\n    while (stack.length > 0) {\n      const item = stack.pop();\n      context.currentContext = item.contexts;\n      let nodeName = '';\n      // The tensor of the Enter op with isConstant set should be set\n      // in the parent scope, so it will be available as constant for the\n      // whole loop.\n      if (item.node.op === 'Enter' &&\n          getParamValue('isConstant', item.node, tensorMap, context)) {\n        [nodeName] = getNodeNameAndIndex(item.node.name, context);\n      }\n\n      // only process nodes that are not in the tensorMap yet, this include\n      // inputNodes and internal initNodes.\n      if (tensorMap[item.node.name] == null) {\n        const tensors =\n            executeOp(item.node, tensorMap, context, this._resourceManager);\n        if (!nodeName) {\n          [nodeName] = getNodeNameAndIndex(item.node.name, context);\n        }\n        const currentContext = context.currentContext;\n        if (util.isPromise(tensors)) {\n          promises.push(tensors.then(t => {\n            tensorMap[nodeName] = t;\n            if (this.keepIntermediateTensors) {\n              this.clonedTensorsMap[nodeName] = this.cloneTensorList(t);\n            }\n            context.currentContext = currentContext;\n            this.checkTensorForDisposal(\n                nodeName, item.node, tensorMap, context, tensorsToKeep,\n                outputNames, intermediateTensorConsumerCount);\n            this.processChildNodes(\n                item.node, stack, context, tensorMap, added, usedNodes);\n            return t;\n          }));\n        } else {\n          tensorMap[nodeName] = tensors;\n          if (this.keepIntermediateTensors) {\n            this.clonedTensorsMap[nodeName] = this.cloneTensorList(tensors);\n          }\n          this.checkTensorForDisposal(\n              nodeName, item.node, tensorMap, context, tensorsToKeep,\n              outputNames, intermediateTensorConsumerCount);\n          this.processChildNodes(\n              item.node, stack, context, tensorMap, added, usedNodes);\n        }\n      } else {\n        this.processChildNodes(\n            item.node, stack, context, tensorMap, added, usedNodes);\n      }\n    }\n    return promises;\n  }\n\n  private processChildNodes(\n      node: Node, stack: NodeWithContexts[], context: ExecutionContext,\n      tensorMap: NamedTensorsMap, added: {[key: string]: boolean},\n      usedNodes: Set<string>) {\n    node.children.forEach((childNode) => {\n      const [nodeName, ] = getNodeNameAndIndex(childNode.name, context);\n      if (added[nodeName] || !usedNodes.has(childNode.name)) {\n        return;\n      }\n      // Merge op can be pushed if any of its inputs has value.\n      if (childNode.op === 'Merge') {\n        if (childNode.inputNames.some(name => {\n              return !!getTensor(name, tensorMap, context);\n            })) {\n          added[nodeName] = true;\n          stack.push({contexts: context.currentContext, node: childNode});\n        }\n      } else  // Otherwise all inputs must to have value.\n          if (childNode.inputNames.every(name => {\n                return !!getTensor(name, tensorMap, context);\n              })) {\n        added[nodeName] = true;\n        stack.push({contexts: context.currentContext, node: childNode});\n      }\n    });\n  }\n\n  /**\n   * Releases the memory used by the weight tensors.\n   */\n  dispose() {\n    Object.keys(this.weightMap)\n        .forEach(\n            key => this.weightMap[key].forEach(tensor => tensor.dispose()));\n  }\n\n  private checkInputShapeAndType(inputs: NamedTensorMap) {\n    Object.keys(inputs).forEach(name => {\n      const input = inputs[name];\n      const [nodeName, ] = parseNodeName(name);\n      const node = this.graph.nodes[nodeName];\n      if (node.attrParams['shape'] && node.attrParams['shape'].value) {\n        const shape = node.attrParams['shape'].value as number[];\n        const match = shape.length === input.shape.length &&\n            input.shape.every(\n                (dim, index) => shape[index] === -1 || shape[index] === dim);\n        util.assert(\n            match,\n            () => `The shape of dict['${node.name}'] provided in ` +\n                `model.execute(dict) must be [${shape}], but was ` +\n                `[${input.shape}]`);\n      }\n      if (node.attrParams['dtype'] && node.attrParams['dtype'].value) {\n        util.assert(\n            input.dtype === node.attrParams['dtype'].value as string,\n            () => `The dtype of dict['${node.name}'] provided in ` +\n                `model.execute(dict) must be ` +\n                `${node.attrParams['dtype'].value}, but was ${input.dtype}`);\n      }\n    });\n  }\n\n  private mapInputs(inputs: NamedTensorMap) {\n    const result: NamedTensorMap = {};\n    for (const inputName in inputs) {\n      const tensor = this._signature ?.inputs ?.[inputName];\n      if (tensor != null) {\n        result[tensor.name] = inputs[inputName];\n      } else {\n        result[inputName] = inputs[inputName];\n      }\n    }\n    return result;\n  }\n\n  private checkInputs(inputs: NamedTensorMap) {\n    const notInGraph = Object.keys(inputs).filter(name => {\n      const [nodeName] = parseNodeName(name);\n      return this.graph.nodes[nodeName] == null;\n    });\n    if (notInGraph.length > 0) {\n      throw new Error(\n          `The dict provided in model.execute(dict) has ` +\n          `keys: [${notInGraph}] that are not part of graph`);\n    }\n  }\n\n  private mapOutputs(outputs: string[]) {\n    return outputs.map(name => {\n      const tensor = this._signature ?.outputs ?.[name];\n      if (tensor != null) {\n        return tensor.name;\n      }\n      return name;\n    }, {});\n  }\n\n  private checkOutputs(outputs: string[]): void {\n    outputs.forEach(name => {\n      const [normalizedName] = parseNodeName(name);\n      if (!this.graph.nodes[normalizedName]) {\n        throw new Error(`The output '${name}' is not found in the graph`);\n      }\n    });\n  }\n}\n"]},"metadata":{},"sourceType":"module","externalDependencies":[]}