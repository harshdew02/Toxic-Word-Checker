{"ast":null,"code":"/**\n * @license\n * Copyright 2017 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport * as util from './util';\n/**\n * Computes a list of TapeNodes that connect x to y, filtering everything else\n * out and preserving the order of the original tape elements.\n *\n * @param tape The tape elements to filter.\n * @param xs The input Tensors.\n * @param y The output Tensor.\n */\nexport function getFilteredNodesXToY(tape, xs, y) {\n  // Forward pass to compute all the nodes and Tensors that are transitively a\n  // function of x.\n  var tensorsFromX = {};\n  var nodesFromX = {};\n  for (var i = 0; i < xs.length; i++) {\n    tensorsFromX[xs[i].id] = true;\n  }\n  for (var _i = 0; _i < tape.length; _i++) {\n    var node = tape[_i];\n    var nodeInputs = node.inputs;\n    for (var inputName in nodeInputs) {\n      var input = nodeInputs[inputName];\n      var anyInputFromX = false;\n      for (var j = 0; j < xs.length; j++) {\n        if (tensorsFromX[input.id]) {\n          node.outputs.forEach(function (output) {\n            return tensorsFromX[output.id] = true;\n          });\n          anyInputFromX = true;\n          nodesFromX[node.id] = true;\n          break;\n        }\n      }\n      if (anyInputFromX) {\n        break;\n      }\n    }\n  }\n  // Backward pass to find all of the nodes and Tensors that lead to y.\n  var tensorsLeadToY = {};\n  tensorsLeadToY[y.id] = true;\n  var nodesToY = {};\n  for (var _i2 = tape.length - 1; _i2 >= 0; _i2--) {\n    var _node = tape[_i2];\n    var _nodeInputs = _node.inputs;\n    // If any of the outputs lead to y, mark all of the inputs as leading to y.\n    for (var _j = 0; _j < _node.outputs.length; _j++) {\n      if (tensorsLeadToY[_node.outputs[_j].id]) {\n        for (var _inputName in _nodeInputs) {\n          tensorsLeadToY[_nodeInputs[_inputName].id] = true;\n          nodesToY[_node.id] = true;\n        }\n        break;\n      }\n    }\n  }\n  // Return the paths that come from x and lead to y.\n  var filteredTape = [];\n  for (var _i3 = 0; _i3 < tape.length; _i3++) {\n    var _node2 = tape[_i3];\n    if (nodesFromX[_node2.id] && nodesToY[_node2.id]) {\n      // Prune the inputs from the node that aren't a function of x.\n      var prunedInputs = {};\n      for (var _inputName2 in _node2.inputs) {\n        var nodeInput = _node2.inputs[_inputName2];\n        if (tensorsFromX[nodeInput.id]) {\n          prunedInputs[_inputName2] = nodeInput;\n        }\n      }\n      // Copy the node and overwrite inputsAndArgs to the pruned version.\n      var prunedNode = Object.assign({}, _node2);\n      prunedNode.inputs = prunedInputs;\n      prunedNode.outputs = _node2.outputs;\n      filteredTape.push(prunedNode);\n    }\n  }\n  return filteredTape;\n}\n/**\n * Backpropagate gradients through the filtered TapeNodes.\n *\n * @param tensorAccumulatedGradientMap A map of Tensor to its gradient. This map\n * is mutated by this method.\n * @param filteredTape The filtered TapeNodes to backprop through.\n */\nexport function backpropagateGradients(tensorAccumulatedGradientMap, filteredTape, tidy, add) {\n  var _loop = function _loop() {\n    var node = filteredTape[i];\n    var dys = [];\n    node.outputs.forEach(function (o) {\n      var gradTensor = tensorAccumulatedGradientMap[o.id];\n      if (gradTensor != null) {\n        dys.push(gradTensor);\n      } else {\n        // This particular output is not in the back-propagation subgraph, so it\n        // does not affect the final output, thus we put null for its dy.\n        dys.push(null);\n      }\n    });\n    if (node.gradient == null) {\n      throw new Error(\"Cannot compute gradient: gradient function not found \" + \"for \".concat(node.kernelName, \".\"));\n    }\n    // Backprop dy through this node and accumulate gradients over the inputs.\n    var inputGradients = node.gradient(dys);\n    var _loop2 = function _loop2(inputName) {\n      if (!(inputName in inputGradients)) {\n        throw new Error(\"Cannot backprop through input \".concat(inputName, \". \") + \"Available gradients found: \".concat(Object.keys(inputGradients), \".\"));\n      }\n      // Call the gradient function.\n      var dx = tidy(function () {\n        return inputGradients[inputName]();\n      });\n      if (dx.dtype !== 'float32') {\n        throw new Error(\"Error in gradient for op \".concat(node.kernelName, \". The gradient of input \") + \"\".concat(inputName, \" must have 'float32' dtype, but has '\").concat(dx.dtype, \"'\"));\n      }\n      var x = node.inputs[inputName];\n      if (!util.arraysEqual(dx.shape, x.shape)) {\n        throw new Error(\"Error in gradient for op \".concat(node.kernelName, \". The gradient of input \") + \"'\".concat(inputName, \"' has shape '\").concat(dx.shape, \"', which does not match \") + \"the shape of the input '\".concat(x.shape, \"'\"));\n      }\n      if (tensorAccumulatedGradientMap[x.id] == null) {\n        tensorAccumulatedGradientMap[x.id] = dx;\n      } else {\n        var curGradient = tensorAccumulatedGradientMap[x.id];\n        tensorAccumulatedGradientMap[x.id] = add(curGradient, dx);\n        curGradient.dispose();\n      }\n    };\n    for (var inputName in node.inputs) {\n      _loop2(inputName);\n    }\n  };\n  // Walk the tape backward and keep a map of Tensor to its gradient.\n  for (var i = filteredTape.length - 1; i >= 0; i--) {\n    _loop();\n  }\n}","map":{"version":3,"mappings":"AAAA;;;;;;;;;;;;;;;;AAmBA,OAAO,KAAKA,IAAI,MAAM,QAAQ;AAgB9B;;;;;;;;AAQA,OAAM,SAAUC,oBAAoB,CAChCC,IAAgB,EAAEC,EAAY,EAAEC,CAAS;EAC3C;EACA;EACA,IAAMC,YAAY,GAAkC,EAAE;EACtD,IAAMC,UAAU,GAAgC,EAAE;EAClD,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGJ,EAAE,CAACK,MAAM,EAAED,CAAC,EAAE,EAAE;IAClCF,YAAY,CAACF,EAAE,CAACI,CAAC,CAAC,CAACE,EAAE,CAAC,GAAG,IAAI;;EAG/B,KAAK,IAAIF,EAAC,GAAG,CAAC,EAAEA,EAAC,GAAGL,IAAI,CAACM,MAAM,EAAED,EAAC,EAAE,EAAE;IACpC,IAAMG,IAAI,GAAGR,IAAI,CAACK,EAAC,CAAC;IACpB,IAAMI,UAAU,GAAGD,IAAI,CAACE,MAAM;IAC9B,KAAK,IAAMC,SAAS,IAAIF,UAAU,EAAE;MAClC,IAAMG,KAAK,GAAGH,UAAU,CAACE,SAAS,CAAC;MAEnC,IAAIE,aAAa,GAAG,KAAK;MACzB,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGb,EAAE,CAACK,MAAM,EAAEQ,CAAC,EAAE,EAAE;QAClC,IAAIX,YAAY,CAACS,KAAK,CAACL,EAAE,CAAC,EAAE;UAC1BC,IAAI,CAACO,OAAO,CAACC,OAAO,CAAC,gBAAM;YAAA,OAAIb,YAAY,CAACc,MAAM,CAACV,EAAE,CAAC,GAAG,IAAI;UAAA,EAAC;UAC9DM,aAAa,GAAG,IAAI;UACpBT,UAAU,CAACI,IAAI,CAACD,EAAE,CAAC,GAAG,IAAI;UAC1B;;;MAIJ,IAAIM,aAAa,EAAE;QACjB;;;;EAKN;EACA,IAAMK,cAAc,GAAkC,EAAE;EACxDA,cAAc,CAAChB,CAAC,CAACK,EAAE,CAAC,GAAG,IAAI;EAC3B,IAAMY,QAAQ,GAAgC,EAAE;EAEhD,KAAK,IAAId,GAAC,GAAGL,IAAI,CAACM,MAAM,GAAG,CAAC,EAAED,GAAC,IAAI,CAAC,EAAEA,GAAC,EAAE,EAAE;IACzC,IAAMG,KAAI,GAAGR,IAAI,CAACK,GAAC,CAAC;IACpB,IAAMI,WAAU,GAAGD,KAAI,CAACE,MAAM;IAE9B;IACA,KAAK,IAAII,EAAC,GAAG,CAAC,EAAEA,EAAC,GAAGN,KAAI,CAACO,OAAO,CAACT,MAAM,EAAEQ,EAAC,EAAE,EAAE;MAC5C,IAAII,cAAc,CAACV,KAAI,CAACO,OAAO,CAACD,EAAC,CAAC,CAACP,EAAE,CAAC,EAAE;QACtC,KAAK,IAAMI,UAAS,IAAIF,WAAU,EAAE;UAClCS,cAAc,CAACT,WAAU,CAACE,UAAS,CAAC,CAACJ,EAAE,CAAC,GAAG,IAAI;UAC/CY,QAAQ,CAACX,KAAI,CAACD,EAAE,CAAC,GAAG,IAAI;;QAE1B;;;;EAKN;EACA,IAAMa,YAAY,GAAe,EAAE;EACnC,KAAK,IAAIf,GAAC,GAAG,CAAC,EAAEA,GAAC,GAAGL,IAAI,CAACM,MAAM,EAAED,GAAC,EAAE,EAAE;IACpC,IAAMG,MAAI,GAAGR,IAAI,CAACK,GAAC,CAAC;IAEpB,IAAID,UAAU,CAACI,MAAI,CAACD,EAAE,CAAC,IAAIY,QAAQ,CAACX,MAAI,CAACD,EAAE,CAAC,EAAE;MAC5C;MACA,IAAMc,YAAY,GAAkC,EAAE;MACtD,KAAK,IAAMV,WAAS,IAAIH,MAAI,CAACE,MAAM,EAAE;QACnC,IAAMY,SAAS,GAAGd,MAAI,CAACE,MAAM,CAACC,WAAS,CAAC;QACxC,IAAIR,YAAY,CAACmB,SAAS,CAACf,EAAE,CAAC,EAAE;UAC9Bc,YAAY,CAACV,WAAS,CAAC,GAAGW,SAAS;;;MAIvC;MACA,IAAMC,UAAU,GAAGC,MAAM,CAACC,MAAM,CAAC,EAAE,EAAEjB,MAAI,CAAC;MAC1Ce,UAAU,CAACb,MAAM,GAAGW,YAAY;MAChCE,UAAU,CAACR,OAAO,GAAGP,MAAI,CAACO,OAAO;MAEjCK,YAAY,CAACM,IAAI,CAACH,UAAU,CAAC;;;EAIjC,OAAOH,YAAY;AACrB;AAEA;;;;;;;AAOA,OAAM,SAAUO,sBAAsB,CAClCC,4BAA0D,EAC1DR,YAAwB,EAAES,IAA6B,EACvDC,GAAqC;EAAA,6BAEY;IACjD,IAAMtB,IAAI,GAAGY,YAAY,CAACf,CAAC,CAAC;IAE5B,IAAM0B,GAAG,GAAa,EAAE;IACxBvB,IAAI,CAACO,OAAO,CAACC,OAAO,CAAC,WAAC,EAAG;MACvB,IAAMgB,UAAU,GAAGJ,4BAA4B,CAACK,CAAC,CAAC1B,EAAE,CAAC;MACrD,IAAIyB,UAAU,IAAI,IAAI,EAAE;QACtBD,GAAG,CAACL,IAAI,CAACM,UAAU,CAAC;OACrB,MAAM;QACL;QACA;QACAD,GAAG,CAACL,IAAI,CAAC,IAAI,CAAC;;IAElB,CAAC,CAAC;IAEF,IAAIlB,IAAI,CAAC0B,QAAQ,IAAI,IAAI,EAAE;MACzB,MAAM,IAAIC,KAAK,CACX,wEACO3B,IAAI,CAAC4B,UAAU,MAAG,CAAC;;IAGhC;IACA,IAAMC,cAAc,GAAG7B,IAAI,CAAC0B,QAAQ,CAACH,GAAG,CAAC;IAAC,wCAEL;MACnC,IAAI,EAAEpB,SAAS,IAAI0B,cAAc,CAAC,EAAE;QAClC,MAAM,IAAIF,KAAK,CACX,wCAAiCxB,SAAS,+CACZa,MAAM,CAACc,IAAI,CAACD,cAAc,CAAC,MAAG,CAAC;;MAGnE;MACA,IAAME,EAAE,GAAGV,IAAI,CAAC;QAAA,OAAMQ,cAAc,CAAC1B,SAAS,CAAC,EAAE;MAAA,EAAC;MAClD,IAAI4B,EAAE,CAACC,KAAK,KAAK,SAAS,EAAE;QAC1B,MAAM,IAAIL,KAAK,CACX,mCACI3B,IAAI,CAAC4B,UAAU,0CAChBzB,SAAS,kDAAwC4B,EAAE,CAACC,KAAK,MAAG,CAAC;;MAEtE,IAAMC,CAAC,GAAGjC,IAAI,CAACE,MAAM,CAACC,SAAS,CAAC;MAChC,IAAI,CAACb,IAAI,CAAC4C,WAAW,CAACH,EAAE,CAACI,KAAK,EAAEF,CAAC,CAACE,KAAK,CAAC,EAAE;QACxC,MAAM,IAAIR,KAAK,CACX,mCACI3B,IAAI,CAAC4B,UAAU,2CACfzB,SAAS,0BAAgB4B,EAAE,CAACI,KAAK,6BAA0B,qCACpCF,CAAC,CAACE,KAAK,MAAG,CAAC;;MAG5C,IAAIf,4BAA4B,CAACa,CAAC,CAAClC,EAAE,CAAC,IAAI,IAAI,EAAE;QAC9CqB,4BAA4B,CAACa,CAAC,CAAClC,EAAE,CAAC,GAAGgC,EAAE;OACxC,MAAM;QACL,IAAMK,WAAW,GAAGhB,4BAA4B,CAACa,CAAC,CAAClC,EAAE,CAAC;QACtDqB,4BAA4B,CAACa,CAAC,CAAClC,EAAE,CAAC,GAAGuB,GAAG,CAACc,WAAW,EAAEL,EAAE,CAAC;QACzDK,WAAW,CAACC,OAAO,EAAE;;KAExB;IA/BD,KAAK,IAAMlC,SAAS,IAAIH,IAAI,CAACE,MAAM;MAAA;IAAA;GAgCpC;EAzDD;EACA,KAAK,IAAIL,CAAC,GAAGe,YAAY,CAACd,MAAM,GAAG,CAAC,EAAED,CAAC,IAAI,CAAC,EAAEA,CAAC,EAAE;IAAA;EAAA;AAyDnD","names":["util","getFilteredNodesXToY","tape","xs","y","tensorsFromX","nodesFromX","i","length","id","node","nodeInputs","inputs","inputName","input","anyInputFromX","j","outputs","forEach","output","tensorsLeadToY","nodesToY","filteredTape","prunedInputs","nodeInput","prunedNode","Object","assign","push","backpropagateGradients","tensorAccumulatedGradientMap","tidy","add","dys","gradTensor","o","gradient","Error","kernelName","inputGradients","keys","dx","dtype","x","arraysEqual","shape","curGradient","dispose"],"sources":["E:\\react-detect-toxicity-in-a-chat-app-youtube-2\\Toxic-Word-Checker\\node_modules\\@tensorflow\\tfjs-core\\src\\tape.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2017 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {Tensor} from './tensor';\nimport {NamedTensorMap} from './tensor_types';\nimport * as util from './util';\n\nexport interface TapeNode {\n  id: number;\n  kernelName: string;\n  outputs: Tensor[];\n  inputs: NamedTensorMap;\n  // Optional params, defined only for ops with gradient impl.\n  gradient?: (dys: Tensor[]) => NamedGradientMap;\n  saved?: Tensor[];\n}\n\nexport type NamedGradientMap = {\n  [inputName: string]: () => Tensor;\n};\n\n/**\n * Computes a list of TapeNodes that connect x to y, filtering everything else\n * out and preserving the order of the original tape elements.\n *\n * @param tape The tape elements to filter.\n * @param xs The input Tensors.\n * @param y The output Tensor.\n */\nexport function getFilteredNodesXToY(\n    tape: TapeNode[], xs: Tensor[], y: Tensor): TapeNode[] {\n  // Forward pass to compute all the nodes and Tensors that are transitively a\n  // function of x.\n  const tensorsFromX: {[tensorId: number]: boolean} = {};\n  const nodesFromX: {[nodeId: number]: boolean} = {};\n  for (let i = 0; i < xs.length; i++) {\n    tensorsFromX[xs[i].id] = true;\n  }\n\n  for (let i = 0; i < tape.length; i++) {\n    const node = tape[i];\n    const nodeInputs = node.inputs;\n    for (const inputName in nodeInputs) {\n      const input = nodeInputs[inputName];\n\n      let anyInputFromX = false;\n      for (let j = 0; j < xs.length; j++) {\n        if (tensorsFromX[input.id]) {\n          node.outputs.forEach(output => tensorsFromX[output.id] = true);\n          anyInputFromX = true;\n          nodesFromX[node.id] = true;\n          break;\n        }\n      }\n\n      if (anyInputFromX) {\n        break;\n      }\n    }\n  }\n\n  // Backward pass to find all of the nodes and Tensors that lead to y.\n  const tensorsLeadToY: {[tensorId: number]: boolean} = {};\n  tensorsLeadToY[y.id] = true;\n  const nodesToY: {[nodeId: number]: boolean} = {};\n\n  for (let i = tape.length - 1; i >= 0; i--) {\n    const node = tape[i];\n    const nodeInputs = node.inputs;\n\n    // If any of the outputs lead to y, mark all of the inputs as leading to y.\n    for (let j = 0; j < node.outputs.length; j++) {\n      if (tensorsLeadToY[node.outputs[j].id]) {\n        for (const inputName in nodeInputs) {\n          tensorsLeadToY[nodeInputs[inputName].id] = true;\n          nodesToY[node.id] = true;\n        }\n        break;\n      }\n    }\n  }\n\n  // Return the paths that come from x and lead to y.\n  const filteredTape: TapeNode[] = [];\n  for (let i = 0; i < tape.length; i++) {\n    const node = tape[i];\n\n    if (nodesFromX[node.id] && nodesToY[node.id]) {\n      // Prune the inputs from the node that aren't a function of x.\n      const prunedInputs: {[inputName: string]: Tensor} = {};\n      for (const inputName in node.inputs) {\n        const nodeInput = node.inputs[inputName];\n        if (tensorsFromX[nodeInput.id]) {\n          prunedInputs[inputName] = nodeInput;\n        }\n      }\n\n      // Copy the node and overwrite inputsAndArgs to the pruned version.\n      const prunedNode = Object.assign({}, node);\n      prunedNode.inputs = prunedInputs;\n      prunedNode.outputs = node.outputs;\n\n      filteredTape.push(prunedNode);\n    }\n  }\n\n  return filteredTape;\n}\n\n/**\n * Backpropagate gradients through the filtered TapeNodes.\n *\n * @param tensorAccumulatedGradientMap A map of Tensor to its gradient. This map\n * is mutated by this method.\n * @param filteredTape The filtered TapeNodes to backprop through.\n */\nexport function backpropagateGradients(\n    tensorAccumulatedGradientMap: {[tensorId: number]: Tensor},\n    filteredTape: TapeNode[], tidy: (f: Function) => Tensor,\n    add: (a: Tensor, b: Tensor) => Tensor) {\n  // Walk the tape backward and keep a map of Tensor to its gradient.\n  for (let i = filteredTape.length - 1; i >= 0; i--) {\n    const node = filteredTape[i];\n\n    const dys: Tensor[] = [];\n    node.outputs.forEach(o => {\n      const gradTensor = tensorAccumulatedGradientMap[o.id];\n      if (gradTensor != null) {\n        dys.push(gradTensor);\n      } else {\n        // This particular output is not in the back-propagation subgraph, so it\n        // does not affect the final output, thus we put null for its dy.\n        dys.push(null);\n      }\n    });\n\n    if (node.gradient == null) {\n      throw new Error(\n          `Cannot compute gradient: gradient function not found ` +\n          `for ${node.kernelName}.`);\n    }\n\n    // Backprop dy through this node and accumulate gradients over the inputs.\n    const inputGradients = node.gradient(dys);\n\n    for (const inputName in node.inputs) {\n      if (!(inputName in inputGradients)) {\n        throw new Error(\n            `Cannot backprop through input ${inputName}. ` +\n            `Available gradients found: ${Object.keys(inputGradients)}.`);\n      }\n\n      // Call the gradient function.\n      const dx = tidy(() => inputGradients[inputName]());\n      if (dx.dtype !== 'float32') {\n        throw new Error(\n            `Error in gradient for op ${\n                node.kernelName}. The gradient of input ` +\n            `${inputName} must have 'float32' dtype, but has '${dx.dtype}'`);\n      }\n      const x = node.inputs[inputName];\n      if (!util.arraysEqual(dx.shape, x.shape)) {\n        throw new Error(\n            `Error in gradient for op ${\n                node.kernelName}. The gradient of input ` +\n            `'${inputName}' has shape '${dx.shape}', which does not match ` +\n            `the shape of the input '${x.shape}'`);\n      }\n\n      if (tensorAccumulatedGradientMap[x.id] == null) {\n        tensorAccumulatedGradientMap[x.id] = dx;\n      } else {\n        const curGradient = tensorAccumulatedGradientMap[x.id];\n        tensorAccumulatedGradientMap[x.id] = add(curGradient, dx);\n        curGradient.dispose();\n      }\n    }\n  }\n}\n"]},"metadata":{},"sourceType":"module","externalDependencies":[]}