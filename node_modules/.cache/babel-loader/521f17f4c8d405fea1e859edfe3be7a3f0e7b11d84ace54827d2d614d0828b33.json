{"ast":null,"code":"import _regeneratorRuntime from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/regeneratorRuntime.js\";\nimport _toConsumableArray from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/toConsumableArray.js\";\nimport _asyncToGenerator from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\nimport _classCallCheck from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/classCallCheck.js\";\nimport _createClass from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createClass.js\";\nimport _inherits from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/inherits.js\";\nimport _createSuper from \"E:/react-detect-toxicity-in-a-chat-app-youtube-2/node_modules/babel-preset-react-app/node_modules/@babel/runtime/helpers/esm/createSuper.js\";\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { dispose as _dispose, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { div } from '../ops/div';\nimport { mul } from '../ops/mul';\nimport { sqrt } from '../ops/ops';\nimport { square } from '../ops/square';\nimport { zerosLike } from '../ops/zeros_like';\nimport { Optimizer } from './optimizer';\n/** @doclink Optimizer */\nexport var AdadeltaOptimizer = /*#__PURE__*/function (_Optimizer) {\n  _inherits(AdadeltaOptimizer, _Optimizer);\n  var _super = _createSuper(AdadeltaOptimizer);\n  function AdadeltaOptimizer(learningRate, rho) {\n    var _this;\n    var epsilon = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : null;\n    _classCallCheck(this, AdadeltaOptimizer);\n    _this = _super.call(this);\n    _this.learningRate = learningRate;\n    _this.rho = rho;\n    _this.epsilon = epsilon;\n    _this.accumulatedGrads = [];\n    _this.accumulatedUpdates = [];\n    if (epsilon == null) {\n      _this.epsilon = ENGINE.backend.epsilon();\n    }\n    return _this;\n  }\n  /** @nocollapse */\n  _createClass(AdadeltaOptimizer, [{\n    key: \"applyGradients\",\n    value: function applyGradients(variableGradients) {\n      var _this2 = this;\n      var variableNames = Array.isArray(variableGradients) ? variableGradients.map(function (item) {\n        return item.name;\n      }) : Object.keys(variableGradients);\n      variableNames.forEach(function (name, i) {\n        var value = ENGINE.registeredVariables[name];\n        var trainable = false;\n        if (_this2.accumulatedGrads[i] == null) {\n          _this2.accumulatedGrads[i] = {\n            originalName: \"\".concat(name, \"/accum_grad\"),\n            variable: tidy(function () {\n              return zerosLike(value).variable(trainable);\n            })\n          };\n        }\n        if (_this2.accumulatedUpdates[i] == null) {\n          _this2.accumulatedUpdates[i] = {\n            originalName: \"\".concat(name, \"/accum_var\"),\n            variable: tidy(function () {\n              return zerosLike(value).variable(trainable);\n            })\n          };\n        }\n        var gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];\n        if (gradient == null) {\n          return;\n        }\n        var accumulatedGrad = _this2.accumulatedGrads[i].variable;\n        var accumulatedUpdate = _this2.accumulatedUpdates[i].variable;\n        tidy(function () {\n          var newAccumulatedGrad = add(mul(accumulatedGrad, _this2.rho), mul(square(gradient), 1 - _this2.rho));\n          var updates = mul(div(sqrt(add(accumulatedUpdate, _this2.epsilon)), sqrt(add(accumulatedGrad, _this2.epsilon))), gradient);\n          var newAccumulatedUpdate = add(mul(accumulatedUpdate, _this2.rho), mul(square(updates), 1 - _this2.rho));\n          accumulatedGrad.assign(newAccumulatedGrad);\n          accumulatedUpdate.assign(newAccumulatedUpdate);\n          var newValue = add(mul(updates, -_this2.learningRate), value);\n          value.assign(newValue);\n        });\n      });\n      this.incrementIterations();\n    }\n  }, {\n    key: \"dispose\",\n    value: function dispose() {\n      if (this.accumulatedUpdates != null) {\n        _dispose(this.accumulatedGrads.map(function (v) {\n          return v.variable;\n        }));\n        _dispose(this.accumulatedUpdates.map(function (v) {\n          return v.variable;\n        }));\n      }\n    }\n  }, {\n    key: \"getWeights\",\n    value: function () {\n      var _getWeights = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee() {\n        var variables;\n        return _regeneratorRuntime().wrap(function _callee$(_context) {\n          while (1) switch (_context.prev = _context.next) {\n            case 0:\n              // Order matters for Python compatibility.\n              variables = [].concat(_toConsumableArray(this.accumulatedGrads), _toConsumableArray(this.accumulatedUpdates));\n              _context.next = 3;\n              return this.saveIterations();\n            case 3:\n              _context.t0 = _context.sent;\n              return _context.abrupt(\"return\", [_context.t0].concat(variables.map(function (v) {\n                return {\n                  name: v.originalName,\n                  tensor: v.variable\n                };\n              })));\n            case 5:\n            case \"end\":\n              return _context.stop();\n          }\n        }, _callee, this);\n      }));\n      function getWeights() {\n        return _getWeights.apply(this, arguments);\n      }\n      return getWeights;\n    }()\n  }, {\n    key: \"setWeights\",\n    value: function () {\n      var _setWeights = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee2(weightValues) {\n        var variableCount, trainable;\n        return _regeneratorRuntime().wrap(function _callee2$(_context2) {\n          while (1) switch (_context2.prev = _context2.next) {\n            case 0:\n              _context2.next = 2;\n              return this.extractIterations(weightValues);\n            case 2:\n              weightValues = _context2.sent;\n              variableCount = weightValues.length / 2;\n              trainable = false;\n              this.accumulatedGrads = weightValues.slice(0, variableCount).map(function (v) {\n                return {\n                  originalName: v.name,\n                  variable: v.tensor.variable(trainable)\n                };\n              });\n              this.accumulatedUpdates = weightValues.slice(variableCount, variableCount * 2).map(function (v) {\n                return {\n                  originalName: v.name,\n                  variable: v.tensor.variable(trainable)\n                };\n              });\n            case 7:\n            case \"end\":\n              return _context2.stop();\n          }\n        }, _callee2, this);\n      }));\n      function setWeights(_x) {\n        return _setWeights.apply(this, arguments);\n      }\n      return setWeights;\n    }()\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      return {\n        'learningRate': this.learningRate,\n        'rho': this.rho,\n        'epsilon': this.epsilon\n      };\n    }\n    /** @nocollapse */\n  }], [{\n    key: \"className\",\n    get: function get() {\n      // Name matters for Python compatibility.\n      // This is a getter instead of a property because when it's a property, it\n      // prevents the entire class from being tree-shaken.\n      return 'Adadelta';\n    }\n  }, {\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      return new cls(config['learningRate'], config['rho'], config['epsilon']);\n    }\n  }]);\n  return AdadeltaOptimizer;\n}(Optimizer);","map":{"version":3,"mappings":";;;;;;;AAAA;;;;;;;;;;;;;;;;AAiBA,SAAQA,MAAM,QAAO,WAAW;AAChC,SAAQC,OAAO,IAAPA,QAAO,EAAEC,IAAI,QAAO,YAAY;AACxC,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,GAAG,QAAO,YAAY;AAC9B,SAAQC,IAAI,QAAO,YAAY;AAC/B,SAAQC,MAAM,QAAO,eAAe;AACpC,SAAQC,SAAS,QAAO,mBAAmB;AAI3C,SAAQC,SAAS,QAA0B,aAAa;AAExD;AACA,WAAaC,iBAAkB;EAAA;EAAA;EAW7B,2BACcC,YAAoB,EAAYC,GAAW,EACrB;IAAA;IAAA,IAAtBC,8EAAkB,IAAI;IAAA;IAClC;IAFY,kBAAY,GAAZF,YAAY;IAAoB,SAAG,GAAHC,GAAG;IACnC,aAAO,GAAPC,OAAO;IALb,sBAAgB,GAAwB,EAAE;IAC1C,wBAAkB,GAAwB,EAAE;IAOlD,IAAIA,OAAO,IAAI,IAAI,EAAE;MACnB,MAAKA,OAAO,GAAGb,MAAM,CAACc,OAAO,CAACD,OAAO,EAAE;;IACxC;EACH;EAlBA;EAAA;IAAA;IAAA,OAoBA,wBAAeE,iBAAiD;MAAA;MAC9D,IAAMC,aAAa,GAAGC,KAAK,CAACC,OAAO,CAACH,iBAAiB,CAAC,GAClDA,iBAAiB,CAACI,GAAG,CAAC,cAAI;QAAA,OAAIC,IAAI,CAACC,IAAI;MAAA,EAAC,GACxCC,MAAM,CAACC,IAAI,CAACR,iBAAiB,CAAC;MAElCC,aAAa,CAACQ,OAAO,CAAC,UAACH,IAAI,EAAEI,CAAC,EAAI;QAChC,IAAMC,KAAK,GAAG1B,MAAM,CAAC2B,mBAAmB,CAACN,IAAI,CAAC;QAC9C,IAAMO,SAAS,GAAG,KAAK;QACvB,IAAI,MAAI,CAACC,gBAAgB,CAACJ,CAAC,CAAC,IAAI,IAAI,EAAE;UACpC,MAAI,CAACI,gBAAgB,CAACJ,CAAC,CAAC,GAAG;YACzBK,YAAY,YAAKT,IAAI,gBAAa;YAClCU,QAAQ,EAAE7B,IAAI,CAAC;cAAA,OAAMM,SAAS,CAACkB,KAAK,CAAC,CAACK,QAAQ,CAACH,SAAS,CAAC;YAAA;WAC1D;;QAEH,IAAI,MAAI,CAACI,kBAAkB,CAACP,CAAC,CAAC,IAAI,IAAI,EAAE;UACtC,MAAI,CAACO,kBAAkB,CAACP,CAAC,CAAC,GAAG;YAC3BK,YAAY,YAAKT,IAAI,eAAY;YACjCU,QAAQ,EAAE7B,IAAI,CAAC;cAAA,OAAMM,SAAS,CAACkB,KAAK,CAAC,CAACK,QAAQ,CAACH,SAAS,CAAC;YAAA;WAC1D;;QAGH,IAAMK,QAAQ,GAAGhB,KAAK,CAACC,OAAO,CAACH,iBAAiB,CAAC,GAC7CA,iBAAiB,CAACU,CAAC,CAAC,CAACS,MAAM,GAC3BnB,iBAAiB,CAACM,IAAI,CAAC;QAC3B,IAAIY,QAAQ,IAAI,IAAI,EAAE;UACpB;;QAGF,IAAME,eAAe,GAAG,MAAI,CAACN,gBAAgB,CAACJ,CAAC,CAAC,CAACM,QAAQ;QACzD,IAAMK,iBAAiB,GAAG,MAAI,CAACJ,kBAAkB,CAACP,CAAC,CAAC,CAACM,QAAQ;QAE7D7B,IAAI,CAAC,YAAK;UACR,IAAMmC,kBAAkB,GACpBlC,GAAG,CAACE,GAAG,CAAC8B,eAAe,EAAE,MAAI,CAACvB,GAAG,CAAC,EAC9BP,GAAG,CAACE,MAAM,CAAC0B,QAAQ,CAAC,EAAE,CAAC,GAAG,MAAI,CAACrB,GAAG,CAAC,CAAC;UAE5C,IAAM0B,OAAO,GACTjC,GAAG,CAACD,GAAG,CAACE,IAAI,CAACH,GAAG,CAACiC,iBAAiB,EAAE,MAAI,CAACvB,OAAO,CAAC,CAAC,EAC1CP,IAAI,CAACH,GAAG,CAACgC,eAAe,EAAE,MAAI,CAACtB,OAAO,CAAC,CAAC,CAAC,EAC7CoB,QAAQ,CAAC;UAEjB,IAAMM,oBAAoB,GACtBpC,GAAG,CAACE,GAAG,CAAC+B,iBAAiB,EAAE,MAAI,CAACxB,GAAG,CAAC,EAChCP,GAAG,CAACE,MAAM,CAAC+B,OAAO,CAAC,EAAE,CAAC,GAAG,MAAI,CAAC1B,GAAG,CAAC,CAAC;UAE3CuB,eAAe,CAACK,MAAM,CAACH,kBAAkB,CAAC;UAC1CD,iBAAiB,CAACI,MAAM,CAACD,oBAAoB,CAAC;UAE9C,IAAME,QAAQ,GAAGtC,GAAG,CAACE,GAAG,CAACiC,OAAO,EAAE,CAAC,MAAI,CAAC3B,YAAY,CAAC,EAAEe,KAAK,CAAC;UAC7DA,KAAK,CAACc,MAAM,CAACC,QAAQ,CAAC;QACxB,CAAC,CAAC;MACJ,CAAC,CAAC;MACF,IAAI,CAACC,mBAAmB,EAAE;IAC5B;EAAC;IAAA;IAAA,OAEQ,mBAAO;MACd,IAAI,IAAI,CAACV,kBAAkB,IAAI,IAAI,EAAE;QACnC/B,QAAO,CAAC,IAAI,CAAC4B,gBAAgB,CAACV,GAAG,CAAC,WAAC;UAAA,OAAIwB,CAAC,CAACZ,QAAQ;QAAA,EAAC,CAAC;QACnD9B,QAAO,CAAC,IAAI,CAAC+B,kBAAkB,CAACb,GAAG,CAAC,WAAC;UAAA,OAAIwB,CAAC,CAACZ,QAAQ;QAAA,EAAC,CAAC;;IAEzD;EAAC;IAAA;IAAA;MAAA,6EAEQ;QAAA;QAAA;UAAA;YAAA;cACP;cACMa,SAAS,gCACP,IAAI,CAACf,gBAAgB,sBAAK,IAAI,CAACG,kBAAkB;cAAA;cAAA,OAC3C,IAAI,CAACa,cAAc,EAAE;YAAA;cAAA;cAAA,+CAAEC,MAAM,CACvCF,SAAS,CAACzB,GAAG,CAAC,WAAC;gBAAA,OAAK;kBAACE,IAAI,EAAEsB,CAAC,CAACb,YAAY;kBAAEI,MAAM,EAAES,CAAC,CAACZ;gBAAQ,CAAC;cAAA,CAAC,CAAC;YAAA;YAAA;cAAA;UAAA;QAAA;MAAA,CACrE;MAAA;QAAA;MAAA;MAAA;IAAA;EAAA;IAAA;IAAA;MAAA,6EAEQ,kBAAiBgB,YAA2B;QAAA;QAAA;UAAA;YAAA;cAAA;cAAA,OAC9B,IAAI,CAACC,iBAAiB,CAACD,YAAY,CAAC;YAAA;cAAzDA,YAAY;cACNE,aAAa,GAAGF,YAAY,CAACG,MAAM,GAAG,CAAC;cACvCtB,SAAS,GAAG,KAAK;cACvB,IAAI,CAACC,gBAAgB,GACjBkB,YAAY,CAACI,KAAK,CAAC,CAAC,EAAEF,aAAa,CAAC,CAAC9B,GAAG,CAAC,WAAC;gBAAA,OAAK;kBACJW,YAAY,EAAEa,CAAC,CAACtB,IAAI;kBACpBU,QAAQ,EAAEY,CAAC,CAACT,MAAM,CAACH,QAAQ,CACvBH,SAAS;iBACd;cAAA,CAAC,CAAC;cAChD,IAAI,CAACI,kBAAkB,GACnBe,YAAY,CAACI,KAAK,CAACF,aAAa,EAAEA,aAAa,GAAG,CAAC,CAAC,CAC/C9B,GAAG,CAAC,WAAC;gBAAA,OAAK;kBACJW,YAAY,EAAEa,CAAC,CAACtB,IAAI;kBACpBU,QAAQ,EAAEY,CAAC,CAACT,MAAM,CAACH,QAAQ,CAACH,SAAS;iBACtC;cAAA,CAAC,CAAC;YAAC;YAAA;cAAA;UAAA;QAAA;MAAA,CAClB;MAAA;QAAA;MAAA;MAAA;IAAA;EAAA;IAAA;IAAA,OAED,qBAAS;MACP,OAAO;QACL,cAAc,EAAE,IAAI,CAACjB,YAAY;QACjC,KAAK,EAAE,IAAI,CAACC,GAAG;QACf,SAAS,EAAE,IAAI,CAACC;OACjB;IACH;IAEA;EAAA;IAAA;IAAA,KAnHA,eAAoB;MAClB;MACA;MACA;MACA,OAAO,UAAU;IACnB;EAAC;IAAA;IAAA,OA+GD,oBACIuC,GAA+B,EAAEC,MAAkB;MACrD,OAAO,IAAID,GAAG,CAACC,MAAM,CAAC,cAAc,CAAC,EAAEA,MAAM,CAAC,KAAK,CAAC,EAAEA,MAAM,CAAC,SAAS,CAAC,CAAC;IAC1E;EAAC;EAAA;AAAA,EAzHoC5C,SAAS","names":["ENGINE","dispose","tidy","add","div","mul","sqrt","square","zerosLike","Optimizer","AdadeltaOptimizer","learningRate","rho","epsilon","backend","variableGradients","variableNames","Array","isArray","map","item","name","Object","keys","forEach","i","value","registeredVariables","trainable","accumulatedGrads","originalName","variable","accumulatedUpdates","gradient","tensor","accumulatedGrad","accumulatedUpdate","newAccumulatedGrad","updates","newAccumulatedUpdate","assign","newValue","incrementIterations","v","variables","saveIterations","concat","weightValues","extractIterations","variableCount","length","slice","cls","config"],"sources":["E:\\react-detect-toxicity-in-a-chat-app-youtube-2\\node_modules\\@tensorflow\\tfjs-core\\src\\optimizers\\adadelta_optimizer.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {ENGINE} from '../engine';\nimport {dispose, tidy} from '../globals';\nimport {add} from '../ops/add';\nimport {div} from '../ops/div';\nimport {mul} from '../ops/mul';\nimport {sqrt} from '../ops/ops';\nimport {square} from '../ops/square';\nimport {zerosLike} from '../ops/zeros_like';\nimport {ConfigDict, Serializable, SerializableConstructor} from '../serialization';\nimport {NamedTensor, NamedVariableMap} from '../tensor_types';\n\nimport {Optimizer, OptimizerVariable} from './optimizer';\n\n/** @doclink Optimizer */\nexport class AdadeltaOptimizer extends Optimizer {\n  /** @nocollapse */\n  static get className() {\n    // Name matters for Python compatibility.\n    // This is a getter instead of a property because when it's a property, it\n    // prevents the entire class from being tree-shaken.\n    return 'Adadelta';\n  }\n  private accumulatedGrads: OptimizerVariable[] = [];\n  private accumulatedUpdates: OptimizerVariable[] = [];\n\n  constructor(\n      protected learningRate: number, protected rho: number,\n      protected epsilon: number = null) {\n    super();\n\n    if (epsilon == null) {\n      this.epsilon = ENGINE.backend.epsilon();\n    }\n  }\n\n  applyGradients(variableGradients: NamedVariableMap|NamedTensor[]) {\n    const variableNames = Array.isArray(variableGradients) ?\n        variableGradients.map(item => item.name) :\n        Object.keys(variableGradients);\n\n    variableNames.forEach((name, i) => {\n      const value = ENGINE.registeredVariables[name];\n      const trainable = false;\n      if (this.accumulatedGrads[i] == null) {\n        this.accumulatedGrads[i] = {\n          originalName: `${name}/accum_grad`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n      if (this.accumulatedUpdates[i] == null) {\n        this.accumulatedUpdates[i] = {\n          originalName: `${name}/accum_var`,\n          variable: tidy(() => zerosLike(value).variable(trainable))\n        };\n      }\n\n      const gradient = Array.isArray(variableGradients) ?\n          variableGradients[i].tensor :\n          variableGradients[name];\n      if (gradient == null) {\n        return;\n      }\n\n      const accumulatedGrad = this.accumulatedGrads[i].variable;\n      const accumulatedUpdate = this.accumulatedUpdates[i].variable;\n\n      tidy(() => {\n        const newAccumulatedGrad =\n            add(mul(accumulatedGrad, this.rho),\n                mul(square(gradient), 1 - this.rho));\n\n        const updates =\n            mul(div(sqrt(add(accumulatedUpdate, this.epsilon)),\n                    sqrt(add(accumulatedGrad, this.epsilon))),\n                gradient);\n\n        const newAccumulatedUpdate =\n            add(mul(accumulatedUpdate, this.rho),\n                mul(square(updates), 1 - this.rho));\n\n        accumulatedGrad.assign(newAccumulatedGrad);\n        accumulatedUpdate.assign(newAccumulatedUpdate);\n\n        const newValue = add(mul(updates, -this.learningRate), value);\n        value.assign(newValue);\n      });\n    });\n    this.incrementIterations();\n  }\n\n  override dispose(): void {\n    if (this.accumulatedUpdates != null) {\n      dispose(this.accumulatedGrads.map(v => v.variable));\n      dispose(this.accumulatedUpdates.map(v => v.variable));\n    }\n  }\n\n  override async getWeights(): Promise<NamedTensor[]> {\n    // Order matters for Python compatibility.\n    const variables: OptimizerVariable[] =\n        [...this.accumulatedGrads, ...this.accumulatedUpdates];\n    return [await this.saveIterations()].concat(\n        variables.map(v => ({name: v.originalName, tensor: v.variable})));\n  }\n\n  override async setWeights(weightValues: NamedTensor[]): Promise<void> {\n    weightValues = await this.extractIterations(weightValues);\n    const variableCount = weightValues.length / 2;\n    const trainable = false;\n    this.accumulatedGrads =\n        weightValues.slice(0, variableCount).map(v => ({\n                                                   originalName: v.name,\n                                                   variable: v.tensor.variable(\n                                                       trainable)\n                                                 }));\n    this.accumulatedUpdates =\n        weightValues.slice(variableCount, variableCount * 2)\n            .map(v => ({\n                   originalName: v.name,\n                   variable: v.tensor.variable(trainable)\n                 }));\n  }\n\n  getConfig(): ConfigDict {\n    return {\n      'learningRate': this.learningRate,\n      'rho': this.rho,\n      'epsilon': this.epsilon\n    };\n  }\n\n  /** @nocollapse */\n  static override fromConfig<T extends Serializable>(\n      cls: SerializableConstructor<T>, config: ConfigDict): T {\n    return new cls(config['learningRate'], config['rho'], config['epsilon']);\n  }\n}\n"]},"metadata":{},"sourceType":"module","externalDependencies":[]}